arXiv:2201.08802v3 [cs.LG] 1 Feb 2022

Preprint
DECONFOUNDING TO EXPLANATION EVALUATION IN GRAPH NEURAL NETWORKS
Ying-Xin Wuâ€ , Xiang Wangâˆ—â€ , An ZhangÂ§, Xia Huâ€¡, Fuli Fengâ€ , Xiangnan Heâ€ , Tat-Seng ChuaÂ§ â€  University of Science and Technology of China Â§ National University of Singapore, â€¡ Rice University {wuyxinsh, xiangwang1223}@gmail.com, an_zhang@nus.edu.sg, xia.hu@rice.edu, {fulifeng93,xiangnanhe}@gmail.com, dcscts@nus.edu.sg
ABSTRACT
Explainability of graph neural networks (GNNs) aims to answer â€œWhy the GNN made a certain prediction?â€, which is crucial to interpret the model prediction. The feature attribution framework distributes a GNNâ€™s prediction to its input features (e.g., edges), identifying an inï¬‚uential subgraph as the explanation. When evaluating the explanation (i.e., subgraph importance), a standard way is to audit the model prediction based on the subgraph solely. However, we argue that a distribution shift exists between the full graph and the subgraph, causing the out-ofdistribution problem. Furthermore, with an in-depth causal analysis, we ï¬nd the OOD effect acts as the confounder, which brings spurious associations between the subgraph importance and model prediction, making the evaluation less reliable. In this work, we propose Deconfounded Subgraph Evaluation (DSE) which assesses the causal effect of an explanatory subgraph on the model prediction. While the distribution shift is generally intractable, we employ the front-door adjustment and introduce a surrogate variable of the subgraphs. Speciï¬cally, we devise a generative model to generate the plausible surrogates that conform to the data distribution, thus approaching the unbiased estimation of subgraph importance. Empirical results demonstrate the effectiveness of DSE in terms of explanation ï¬delity.
1 INTRODUCTION
Explainability of graph neural networks (GNNs) (Hamilton et al., 2017; Dwivedi et al., 2020) is crucial to model understanding and reliability in real-world applications, especially when about fairness and privacy (Ying et al., 2019; Luo et al., 2020). It aims to provide insight into how predictor models work, answering â€œWhy the target GNN made a certain prediction?â€. Towards this end, a variety of explainer models are proposed for feature attribution (Selvaraju et al., 2017; Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020), which decomposes the predictorâ€™s prediction as contributions (i.e., importance) of its input features (e.g., edges, nodes). While feature attribution assigns the features with importance scores, it redistributes the graph features and creates a new distribution different from that of the original full graphs, from which a subgraph is sampled as the explanation. Such sampling process is referred to as feature removal (Covert et al., 2020).
Then, to assess the explanatory subgraph, the current evaluation frameworks use the feature removal principle â€” (1) only feed the subgraph into the target predictor, discarding the other features; (2) measure the importance of the subgraph based on its information amount to recover the modelâ€™s prediction. Such subgraph-prediction correlations uncovered by the removal-based evaluator should offer a faithful inspection of the predictorâ€™s decision-making process and assess the ï¬delity of the explainers reliably.
However, feature removal brings the out-of-distribution (OOD) problem (Frye et al., 2020; Chang et al., 2019; Lukas Faber, 2021): the distribution shift from full graphs to subgraphs likely violates
âˆ—Corresponding author.
1

ğ“–
ğ‘®: Full Graph

Preprint

ğ“–ğ’”ğŸ

ğ“–ğ’”ğŸ

ğ‘®ğ’”: Subgraph

ğ“–

ğ“–ğ’”ğŸ

House

Cycle

ğ“– ğ“–ğ’”ğŸ

ğ“–ğ’”ğŸ

ğ‘«: Distribution Shift

Target Predictor

0.70 0.21

Crane House CHycoluese Crane
Cyğ’€cle: Predğ“–iğ’”cğŸğ“–atğ“–eğ’”ğŸLogits
ğ‘«: Distribution Shift

ğ“–ğ’”ğŸ

Target Predictor

0.70 0.21

ğ‘®: Full Graph

ğ‘®ğ’”: Subgraph

House Cycle Crane
ğ’€: Predicate Logits

âˆ—

ğ¬

ğ’”

Front-door Adjustment

âˆ—

ğ¬

ğ’”

Front-door Adjustment

ğ¬

(a) Feature Removal to Evaluate Explanatory Subgraph Gs

(b) SCM I

Figure 1: (a) A real example in TR3. The GNN predictor classiï¬es the full graph as â€˜Houseâ€. On subgraphs Gs1 and Gs2, the prediction probabilities of being â€œHouseâ€ are respeğ¬ctively 0.21 and 0.70. (b) The structural causal model represents the causalities among variables: G as the input graph, D

as the unobserved distribution shift, Gs as the explanatory subgraph, and Y as the model prediction.

underlying properties, including node degree distribution (Leskovec et al., 2005) and domain-speciï¬c

constraints (Liu et al., 2018) of the full graphs. For example, graph properties of chemical molecules,

such as the valency rules, impose some constraints on syntactically valid molecules (Liu et al., 2018);

hence, simply removing some bonds (edges) or atoms (nodes) creates invalid molecular subgraphs

that never appear in the training dataset. Such OOD subgraphs could manipulate the predictorâ€™s

outcome arbitrarily (Dai et al., 2018; ZÃ¼gner et al., 2018), generates erroneous predictions, and limits

the reliability of the evaluation process.

Here we demonstrate the OOD effect by a real example in Figure 1a, where the trained ASAP (Ranjan
et al., 2020) predictor has classiï¬ed the input graph as â€œHouseâ€ for its attached motif (see Section 4 for more details). On the ground-truth explanation Gs1, the output probability of the â€œHouseâ€ class is surprisingly low (0.21). While for Gs2 with less discriminative information, the outputs probability of the â€œHouseâ€ class (0.70) is higher. Clearly, the removal-based evaluator assigns the OOD subgraphs
with unreliable importance scores, which are unfaithful to the predictorâ€™s decision.

The OOD effect has not been explored in evaluating GNN explanations, to the best of our knowledge.
We rigorously investigate it from a causal view (Pearl et al., 2016; Pearl, 2000; Pearl & Mackenzie,
2018). Figure 1b represents our causal assumption via a structural causal model (SCM) (Pearl et al., 2016; Pearl, 2000), where we target the causal effect of Gs on Y . Nonetheless, as a confounder between Gs and Y , distribution shift D opens the spurious path Gs â† D â†’ Y . By â€œspuriousâ€, we mean that the path lies outside the direct causal path from Gs to Y , making Gs and Y spuriously correlated and yielding an erroneous effect. And one can hardly distinguish between the spurious correlation and causative relations (Pearl et al., 2016). Hence, auditing Y on Gs suffers from the OOD effect and wrongly evaluates the importance of Gs.

ğ“– ğ‘®: Full Graph

Motivated by our causal insight, we propose a novel evaluation paradigm, Deconfounded Subgraph

Evaluator (DSE), to faithfully measure the causal effect of explanatory subgraphs on the prediction.

Based on Figure 1b, as the distribution shift D is hardly mea-

surable, we cannot block thCerabneackdoor path from Gs to Y by

ğ‘«

the backdoor adjustment. Thanks toHthouesefront-door adjustment

â€œw(iPmheeaargrelinweetesâ€ailnw.,thr2oa0dt1uth6ce)e,ftuwhlelegsirunarCsprytoheclgsae adisteclioGknğ“–eâˆ—sğ’”sğŸgğ“–ibdiveğ“–eetğ’”rwğŸntehtheeenSsGuCbsMgarnaidpnhYFs.i,gWwuerheiocb2h-, tain the causal effect of Gsğ‘«o: DnisYtribbuytioidneSnhitfitfying the causal effects carried by Gs â†’ Gâˆ—s and Gâˆ—s â†’ Y , which requires Gâˆ—s to respect the data distribution. Hence we design a gener0.a70tive model, Conditional Variational Graph Auto-Encoder (C0V.21GAE), to generate the possible sğ“–uğ’”rğŸrogates.ğ“–Iğ’”tğŸis wToarrgtehtwPrheidliectmorentioning that our DSE is explainer-agnostic, which can assist the exHpoulsaenaCytcileonCreanvealuation

ğ‘®

ğ‘®ğ¬ ğ‘®ğ’”âˆ—

ğ’€

Front-door Adjustment

Figure 2: SCM II with a mediating variable Gâˆ—s.

reliably anğ‘®dğ’”:fSuurbthgrearpghuide explainers to geneğ’€r:aPtreedfaiciatthefLuolgeitxs planations.

ğ‘«

In a nutshell, our contributions are:

â€¢ From a causal perspective, we argue that the OOD effect is the confounder that causes spurious

correlations between subgraph importance and model prediction. ğ‘®

ğ‘®ğ¬

ğ’€

â€¢ We propose a deconfounding paradigm, DSE, which exploits the front-door adjustment to mitigate

the out-of-distribution effect and evaluate the explanatory subgraphs unbiasedly.

2

Preprint
â€¢ We validate the effectiveness of our framework over various explainers, target GNN models, and datasets. Signiï¬cant boosts are achieved over the conventional feature removal techniques. Code and datasets are available at: https://anonymous.4open.science/r/DSE-24BC/.
2 A CAUSAL VIEW OF EXPLANATION EVALUATION
Here we begin with the causality-based view of feature removal in Section 2.1 and present our causal assumption to inspect the OOD effect in Section 2.2.
2.1 PROBLEM FORMULATION
Without loss of generality, we focus on the graph classiï¬cation task: a well-trained GNN predictor f takes the graph variable G as input and predicts the class Y âˆˆ {1, Â· Â· Â· , K}, i.e., Y = f (G). Generation of Explanatory Subgraphs. Post-hoc explainability typically considers the question â€œWhy the GNN predictor f made certain prediction?â€. A prevalent solution is building an explainer model to conduct feature attribution (Ying et al., 2019; Luo et al., 2020; Pope et al., 2019). It decomposes the prediction into the contributions of the input features, which redistributes the probability of features according to their importance and sample the salient features as an explanatory subgraph Gs. Speciï¬cally, Gs can be a structure-wise (Ying et al., 2019; Luo et al., 2020) or featurewise (Ying et al., 2019) subgraph of G. In this paper, we focus on the structural features. That is, for graph G = (N , E) with the edge set E and the node set N , the explanatory subgraph Gs = (Ns, Es) consists of a subset of edges Es âŠ‚ E and their endpoints Ns = {u, v|(u, v) âˆˆ Es}. Evaluation of Explanatory Subgraphs. Insertion-based evaluation by feature removal (Covert et al., 2020; Dabkowski & Gal, 2017) aims to check whether the subgraph is the supporting substructure 1 that alone allows a conï¬dent classiï¬cation. We systematize this paradigm as three steps: (1) divide the full graph G into two parts, the subgraph Gs and the complement Gs; (2) feed Gs into the target GNN f , while discarding Gs; and (3) obtain the model prediction on Gs, to assess its discriminative information to recover the prediction on G. Brieï¬‚y, at the core of the evaluator is the subgraphprediction correlation. However, as discussed in Section 1, the OOD effect is inherent in the removal-based evaluator, hindering the subgraph-prediction correlation from accurately estimating the subgraph importance.
2.2 STRUCTURAL CAUSAL MODEL
To inspect the OOD effect rigorously, we take a causal look at the evaluation process with a Structural Causal Model (SCM I) in Figure 1b. We denote the abstract data variables by the nodes, where the directed links represent the causality. The SCM indicates how the variables interact with each other through the graphical deï¬nition of causation:
â€¢ G â†’ Gs â† D. We introduce an abstract distribution shift variable D to sample a subgraph Gs from the edge distributions of the full graph G.
â€¢ Gs â†’ Y â† D. We denote Y as the prediction variable (e.g., logits output), which is determined by (1) the direct effect from Gs, and (2) the confounding effect caused by D. In particular, the former causation that led to the result is the focus of this work.
We suggest readers to refer to Appendix A where we offer an elaboration of D. With our SCM assumption, directly measuring the importance of explanatory subgraphs is distracted by the backdoor path (Pearl, 2000), Gs â† D â†’ Y . This path introduces the confounding associations between Gs and Y , which makes Gs and Y spuriously correlated, i.e., biases the subgraph-prediction correlations, thus making the evaluator invalid. How to mitigate the OOD effect and quantify Gsâ€™s genuine causal effect on Y remains largely unexplored in the literature and is the focus of our work.
1We focus on insertion-based evaluation here while we discuss deletion-based evaluation in Appendix C.
3

Preprint

3 DECONFOUNDED EVALUATION OF EXPLANATORY SUBGRAPHS

In this section, we propose a novel deconfounding framework to evaluate the explanatory subgraphs in a trustworthy way. Speciï¬cally, we ï¬rst leverage the front-door adjustment (Pearl, 2000) to formulate a causal objective in Section 3.1. We then devise a conditional variational graph auto-encoders (CVGAE) as the effective implementation of our objective in Section 3.2.

3.1 FRONT-DOOR ADJUSTMENT

To the best of our knowledge, our work is the ï¬rst to adopt the causal theory to solve the OOD problem in the explanation evaluation of GNNs. To pursue the causal effect of Gs on Y , we perform the calculus of the causal intervention P (Y = y|do(Gs = Gs)). Speciï¬cally, the do-calculus (Pearl, 2000; Pearl et al., 2016) is to intervene the subgraph variable Gs by cutting off its coming links and assigning it with the certain value Gs, making it unaffected from its causal parents G and D. From inspection of the SCM in Figure 1b, the distribution effect D acts as the confounder between Gs and Y , and opens the backdoor path Gs â† D â†’ Y . However, as D is hardly measurable, we can not use the backdoor adjustment (Pearl, 2000; Pearl et al., 2016) to block the backdoor path from Gs to Y . Hence, the causal effect of Gs on Y is not identiï¬able from SCM I.
However, we can go much further by considering SCM II in Figure 2 instead, where a mediating variable Gâˆ—s is introduced between Gs and Y :
â€¢ Gs â†’ Gâˆ—s. Gâˆ—s is the surrogate variable of Gs, which completes Gs to make them in the data distribution. First, it originates from and contains Gs. Speciï¬cally, it imagines how the possible full graphs should be when observing the subgraph Gs. Second, Gâˆ—s should follow the data distribution and respect the inherent knowledge of graph properties, thus no link exists between D and Gâˆ—s.
â€¢ Gâˆ—s â†’ Y . This is based on our causal assumption that the causality-related information of Gs on Y , i.e., the discriminative information for Gs to make prediction, is well-preserved by Gâˆ—s. Thus, with the core of Gs, Gâˆ—s is qualiï¬ed to serve as the mediator which further results in the model prediction.

With SCM II, we can exploit the front-door adjustment (Pearl, 2000; Pearl et al., 2016) instead to quantify the causal effect of Gs on Y . Speciï¬cally, by summing over possible surrogate graphs Gsâˆ— of Gâˆ—s, we chain two identiï¬able partial effects of Gs on Gâˆ—s and Gâˆ—s on Y together:

P (Y |do(Gs = Gs)) = P (Y |do(Gâˆ—s = Gsâˆ—))P (Gâˆ—s = Gsâˆ—|do(Gs = Gs))
Gsâˆ—

=

P (Y |Gâˆ—s = Gsâˆ—, Gs = Gs)P (Gs = Gs)P (Gâˆ—s = Gsâˆ—|do(Gs = Gs))

Gsâˆ— Gs

=

P (Y |Gâˆ—s = Gsâˆ—, Gs = Gs)P (Gs = Gs)P (Gâˆ—s = Gsâˆ—|Gs = Gs), (1)

Gsâˆ— Gs

Speciï¬cally, we have P (Gâˆ—s|do(Gs = Gs)) = P (Gâˆ—s|Gs = Gs) as Gs is the only parent of Gâˆ—s. And we distinguish the Gs in our target expression P (Y |do(Gs = Gs)) between Gs, the latter of which is adjusted to pursue P (Y |do(Gâˆ—s = Gsâˆ—)). With the data of (Gs, Gsâˆ—) pairs, we can obtain P (Y |Gâˆ—s = Gsâˆ—, Gs = Gs) by feeding the surrogate graph Gsâˆ— into the GNN predictor, conditional on the subgraph Gs; similarly, we can estimate P (Gs = Gs) statistically; P (Gâˆ—s = Gsâˆ—|Gs = Gs) is
the conditional distribution of the surrogate variable, after observing the subgraphs. As a result, this

front-door adjustment yields a consistent estimation of Gsâ€™s effect on Y and avoids the confounding associations from the OOD effect.

3.2 DEEP GENERATIVE MODEL
However, it is non-trivial to instantiate Gsâˆ— and collect the (Gs, Gsâˆ—) pairs. We get inspiration from the great success of generative models and devise a novel probabilistic model, conditional variational graph auto-encoder (CVGAE), and an adversarial training framework, to generate Gsâˆ—.
Conditional Generation. Inspired by previous works (Thomas N. Kipf, 2016; Liu et al., 2018), we model the data distribution via a generative model gÎ¸ parameterized by Î¸. It is composed of an

4

Preprint

Node Encoder ğ

ğ‘(ğ’|ğ“–, ğ“–ğ’”) Edge emb.

ğ‘(ğ“”ğ’”âˆ—|ğ’)

â€¦

ğ¬

Sample

AGG

Sample

ğ“–ğ’”âˆ—ğŸ

ğ‘·dse = ğŸ. ğŸ—ğŸ—

â€¦

â€¦ â€¦ â€¦
â€¦

â€¦

ğˆ

ğ“–ğ’”âˆ—ï¿½

Figure 3: Model structure of CVGAE. Pdse is the average probability of Gsâˆ— on the target prediction. AGG indicates the representations of the end nodes are aggregated as the edge embeddings.

encoder q(Z|G, Gs) and a decoder p(Gsâˆ—|Z). Speciï¬cally, the encoder q(Z|G, Gs) embeds each node i in G with a stochastic representation zi, and summarize all node representations in Z:

N
q(Z|G, Gs) = q(zi|G, Gs),
i=1

with

q(zi|G, Gs) = N (zi | [Âµ1i, Âµ2i],

Ïƒ12i 0

0 Ïƒ22i

)

(2)

where zi is sampled from a diagonal normal distribution by mean vector [Âµ1i, Âµ2i] and standard deviation vector diag(Ïƒ12i, Ïƒ22i); Âµ1 = fÂµ(G) and log Ïƒ1 = fÏƒ(G) denote the matrices of mean vectors Âµ1i and standard deviation vectors log Ïƒ1i respectively, which are derived from two GNN
models fÂµ and fÏƒ on the top of the full graph G; similarly, Âµ2 = fÂµ(Gs) and log Ïƒ2 = fÏƒ(Gs) are on the top of the subgraph Gs. Then, the decoder p(Gsâˆ—|Z) generates the valid surrogates:

NN

p(Gsâˆ—|Z) =

p(Aij|zi, zj), with p(Aij = 1|zi, zj) = fA([zi, zj]),

(3)

ij

where Aij = 1 indicates the existence of an edge between nodes i and j; fA is a MLP, which takes the concatenation of node representations zi and zj as the input and outputs the probability of Aij = 1.

Leveraging the variational graph auto-encoder, we are able to generate some counterfactual edges that never appear in G and sample Gsâˆ— from the conditional distribution p(Gsâˆ—|Z), formally, Gsâˆ— âˆ¼ p(Gâˆ—s|Z). As a result, P (Gâˆ—s = Gsâˆ—|Gs = Gs) in Equation 1 is identiï¬ed by p(Gsâˆ—|Z). The quality of the generator directly affects the quality of the surrogate graphs, further determines how well the front-
door adjustment is conducted. Next, we will detail an adversarial training framework to optimize the
generator, which is distinct from the standard training of VAE.

Adversarial Training. To achieve high-quality generation, we get inspiration from the adversarial training (Goodfellow et al., 2020; Yue et al., 2021) and devise the following training objective:

min
Î¸

LVAE

+

Î³LC

+

max
Âµ

Ï‰LD,

(4)

where Î³, Ï‰ are trade-off hyper-parameters. These losses are carefully designed to assure the generation follows the data distribution. Next, we will elaborate on each of them.

LVAE = âˆ’EG[Eq(Z|G,Gs)[log p(GË†s|Z)]] + Î²EG[DKL(q(Z|G, Gs)||p(Z))],

(5)

We ï¬rst minimize the Î²-VAE loss(Higgins et al., 2017), and the ï¬rst term is the reconstruction loss responsible to predict the probability of edgesâ€™ existence; the second term is the KL-divergence between the variational and prior distributions. Here we resort to the isotropic Gaussian distribution p(Z) = i p(zi) = i N (zi|0, I) as the prior. Î² reweighs the KL-divergence, which promises to learn the disentangled factors in Z (Higgins et al., 2017; Yue et al., 2021; Suter et al., 2019).

Moreover, we highlight the class-discriminative information in Z, by encouraging the agreement between graph representations with the same class compared to that with different classes. Technically, the contrastive loss is adopted:

LC = âˆ’EG[log

G âˆˆB+ exp (s(zG, zG )/Ï„ ) ], G âˆˆB+âˆªBâˆ’ exp (s(zG , zG )/Ï„ )

(6)

where zG is the representation of G that aggregates all node representations Z together; s is the similarity function, which is given by an inner product here; Ï„ is the temperature hyper-parameter; B+ is the graph set having the same class to G, while the graphs involved in Bâˆ’ have different classes from G. Minimizing this loss enables the generator to go beyond the generic knowledge and uncover
the class-wise patterns of graph data.

5

Preprint

Besides, we introduce a discriminative model dÂµ to distinguish the generated graphs. Speciï¬cally, we set it as a probability-conditional GNN (Fey & Lenssen, 2019) parameterized by Âµ. It takes a graph as
input and outputs a score between 0 to 1, which indicates the conï¬dence of the graph being realistic. Hence, given a real graph G with the ground-truth label y, we can use the generator gÎ¸ to generate Gsâˆ—. Then the discriminator learns to assign G with a large score while labeling Gsâˆ— with a small score. To optimize the discriminator, we adopt the Wasserstein GAN (WGAN) (Martin Arjovsky, 2017) loss:

LD = EG [Ep(Gsâˆ—|Z)[d(G, y) âˆ’ d(Gsâˆ—, y) âˆ’ Î»(||âˆ‡Gsâˆ— d(Gsâˆ—, y)||2 âˆ’ 1)2]],

(7)

where d(Gsâˆ—, y) is the probability of generating Gsâˆ— from the generator; Î» is the hyper-parameter. By playing the min-max game between the generator and the discriminator in Equation 4, the generator
can create the surrogate graphs from the data distribution plausibly.

Subgraph Evaluation. With the well-trained generator gÎ¸âˆ— whose parameters are ï¬xed, we now

approximate sample a set

the causal effect of Gs on Y . of plausible surrogate graphs

Here we conduct Monte-Carlo simulation based on {Gsâˆ—} from p(Gsâˆ—|Z). Having collected the (Gs, Gsâˆ—)

gÎ¸âˆ— to data,

we can arrive the estimation of Equation 1.

4 EXPERIMENTS
We aim to answer the following research questions:
â€¢ Study of Explanation Evaluation. How effective is our DSE in mitigating the OOD effect and evaluating the explanatory subgraph more reliably? (Section 4.2)
â€¢ Study of Generator. How effective is our CVGAE in generating the surrogates for the explanatory subgraphs and making them conform to the data distribution? (Section 4.3)
4.1 EXPERIMENTAL SETTINGS
Datasets & Target GNNs. We ï¬rst train various target GNN classiï¬ers on the three datasets:
â€¢ TR3 is a synthetic dataset involving 3000 graphs, each of which is constructed by connecting a random tree-shape base with one motif (house, cycle, crane). The motif type is the ground-truth label, while we treat the motifs as the ground-truth explanations following Ying et al. (2019); Yuan et al. (2020a). A Local Extremum GNN (Ranjan et al., 2019) is trained for classiï¬cation.
â€¢ MNIST superpixels (MNISTsup) (Monti et al., 2017) converts the MNIST images into 70,000 superpixel graphs. Every graph with 75 nodes is labeled as one of 10 classes. We train a Splinebased GNN (Fey et al., 2018) as the classiï¬er model. The subgraphs representing digits can be viewed as human explanations.
â€¢ Graph-SST2 (Yuan et al., 2020b) is based on text sentiment dataset SST2 (Socher et al., 2013) and converts the text sentences to graphs where nodes represent tokens and edges indicate relations between nodes. Each graph is labeled by its sentence sentiment. The node embeddings are initialized by the pre-trained BERT word embeddings (Devlin et al., 2018). Graph Attention Network (VelicË‡kovicÂ´ et al., 2018) is trained as the classiï¬er.
Ground-Truth Explanations. By â€œground-truthâ€, we follow the prior studies (Ying et al., 2019; Yuan et al., 2020a; Luo et al., 2020) and treat the subgraphs coherent to the model knowledge (e.g., the motif subgraphs in TR3) or human knowledge (e.g., the digit subgraphs in MNISTsup) as the ground-truth explanations. Although such ground-truth explanations might not ï¬t the decision-making process of the model exactly, they contain sufï¬cient discriminative information to help justify the explanations. Note that no ground-truth explanation is available in Graph-SST2.
Explainers. To explain the decisions made by these GNNs, we adopt several state-of-the-art explainers, including SA (Baldassarre & Azizpour, 2019), Grad-CAM (Selvaraju et al., 2017), GNNExplainer (Ying et al., 2019), CXPlain (Schwab & Karlen, 2019), PGM-Explainer (Vu & Thai, 2020), Screener (Anonymous, 2021), to generate the explanatory subgraphs. Speciï¬cally, top-15%, 20%, 20% of edges on the full graph instance construct the explanatory subgraphs in TR3, MNIST, and Graph-SST2, respectively. We refer readers to Appendix D for more experimental details.

6

Preprint

(a) In TR3

(b) In MNISTsup

Figure 4: Validation of different frameworks for explanation evaluation.

Table 1: Evaluation of explainers under different evaluation frameworks. Rs is Spearman rank correlation function. Best explainers are underlined. Symbol (Â·) indicates the rank of explainers.

SA Grad-CAM GNNExplainer
CXPlain PGM-Explainer
Screener
Rs â†‘

Impre(%) 33.07
33.07 0.011

TR3
Impdse(%)
43.23(1) 43.18(2) 41.73(3)
38.61(6) 39.58(5) 40.31(4) 0.943âˆ—

Prec
86.53(1) 75.07(2) 56.34(4)
34.38(6) 48.47(5) 66.49(3)
-

MNISTsup

Impre(%) Impdse(%)

17.60(3) 16.90(5) 17.00(4)

10.98(3) 11.51(2) 12.27(1)

14.30(6) 22.20(2) 32.20(1)

10.78(5) 10.77(6) 10.96(4)

âˆ’0.142 0.943âˆ—

Prec
32.98(2) 31.42(3) 57.75(1)
11.14(5) 2.31(6)
19.51(4)
-

Graph-SST2

Impre(%) Impdse(%)

91.93(4) 91.94(3) 89.40(5)

95.67(4) 96.21(2) 95.20(6)

92.40(2) 89.16(6) 96.04(1)
0.657

95.98(3) 95.45(5) 96.39(1)
0.714âˆ—

Score
4.48(3) 6.21(2) 4.26(4)
3.93(5) 1.68(6) 6.42(1)
-

4.2 STUDY OF EXPLANATION EVALUATION (RQ1)

Deconfounded Evaluation Performance. For an explanation Gs, the conventional removal-based evaluation framework quantiï¬es its importance as the subgraph-prediction correlation, termed

Impre(Gs) = f (Gs); whereas, our DSE framework focuses on the causal effect caused by Gs on Y which is computed based on Equation 1, and we denote it as Impdse(Gs) for short. These importance scores broadly aim to reï¬‚ect the discriminative information carried by Gs. Thanks to the
ground-truth knowledge available in TR3 and MNISTsup, we are able to get a faithful and principled metric to measure the discriminative information amount â€” the precision Prec(Gs, Gs+) between the ground-truth explanation Gs+ and the explanatory subgraph Gs. This precision metric allows us to perform a fair comparison between Impre(Gs) and Impdse(Gs) via:

Ïre = Ï([Prec(Gs, Gs+)], [Impre(Gs)]), Ïdse = Ï([Prec(Gs, Gs+)], [Impdse(Gs)]),

(8)

where Ï is the correlation coefï¬cient between the lists of precision and importance scores. We present

the results in Figure 4 and have some interesting insights:

â€¢ Insight 1: Removal-based evaluation hardly reï¬‚ects the importance of explanations. In most cases, Prec(Gs, Gs+) is negatively correlated with the importance. This again shows that simply discarding a part of a graph could violate some underlying properties of graphs and mislead the target GNN, which is consistent with the adversarial attack works (Dai et al., 2018; ZÃ¼gner et al., 2018). Moreover, the explainers that target high prediction accuracy, such as GNNExplainer, are easily distracted by the OOD effect and thus miss the important subgraphs.
â€¢ Insight 2: Deconfounded evaluation quantiï¬es the explanation importance more faithfully. Substantially, Ïdse greatly improves after the frontdoor adjustments via the surrogate variable. The most notable case is GNNExplainer in MNISTsup, where Ïdse = 0.17 achieves a tremendous increase from Ïdse = âˆ’0.11. Although our DSE alleviates the OOD problem signiï¬cantly, weak positive or negative correlations still exist, which indicates the limitation of the current CVGAE. We leave the exploration of higher-quality generation in future work.

Revisiting & Reranking Explainers. Here we investigate the rankings of explainers generated from different evaluation frameworks, and further compute the Spearman rank correlations between these evaluation rankings and the reference rankings of explainers. Speciï¬cally, for TR3 and MNISTsup with ground-truth explanations, we regard the ranks w.r.t. precision as the references, while obtaining the reference of Graph-SST2 by a user study2. Such a reference offers the human knowledge for explanations and benchmarks the comparison. We show the results in Table 1 and conclude:
270 volunteers are engaged, where each was asked to answer 10 questions randomly sampled from 32 movie reviews and choose the best explanations generated by the explainers. See Appendix E for more details.

7

Preprint

Table 2: Importance scores or probabilities of subgraphs before and after feature removal.

TR3

MNISTsup Graph-SST2

Imp(G) or GMM(G)

0.958âˆ’0.520 0.982âˆ’0.574 35.3âˆ’11.3

Imp(G+s ) or GMM(Gs) 0.438

0.408

24.0

Table 3: Performances of Generators in terms of Validity and Fidelity.

Random VGAE ARGVA CVGAE

Imp(Gâˆ—s )
0.451 0.469 0.392 0.603

TR3
VALâ†‘
0.013 0.031 0.061 0.165

FIDâ†“
0.794 0.754 0.726 0.598

MNISTsup

Imp(Gâˆ—s) VALâ†‘

0.448 0.205 0.466 0.552

0.040 -0.203 0.058 0.144

FIDâ†“
1.325 1.501 1.306 0.910

Graph-SST2

GMM(Gâˆ—s) VALâ†‘

38.8

14.8

37.6

13.6

31.0

7.0

45.8

21.8

FIDâ†“
0.060 0.078 0.079 0.057

â€¢ Insight 3: DSE presents a more fair and reliable comparison among explainers. The DSEbased rankings are highly consistent with the references, while the removal-based rankings struggle to pass the check. In particular, we observe that for TR3, the unrealistic splicing inputs cause a plain ranking w.r.t. Impre. We ï¬nd that various input subgraphs are predicted as cycle class. That is, the target GNN model is a deterministic gambler with serious OOD subgraphs. In contrast, DSE outputs a more informative ranking; For MNISTsup, GNNExplainer with the highest precision is overly underrated by the removal-based evaluation framework, but DSE justiï¬es its position faithfully; For Graph-SST2, although the OOD problem seems to be minor, DSE can still achieve signiï¬cant improvement.

Case Study. We present a case study in Graph-SST2 to illustrate how DSE mitigates the potential OOD problem. ğ“–ğ¬

Impre = ğŸ. ğŸ‘ğŸ–ğŸ“

See Appendix F for another case study on TR3. In Figure 5, G is a graph predicted as â€œnegative" sentiment. The explanatory subgraph Gs emphasizes tokens like â€œweakâ€ and relations like â€œnâ€™tâ†’funnyâ€, which is cogent according

ğ‘·(ğ’š|ğ“–ğ¬) = ğŸ. ğŸ‘ğŸ–ğŸ“ Explanation: a little weakâ€”and it isnâ€™t that funny Sentiment: Negative

to human knowledge. However, its removal-based impor- ğ“– tance is highly underestimated as 0.385, possibly due to its disconnectivity or sparsity after feature removal. To miti-

ğ‘«ğ‘ºğ‘¬
Surrogate Samples

gate the OOD problem, DSE samples 50 surrogate graphs

from the generator, performs the frontdoor adjustment, and Impğ“–ğ’”âˆ—ğŸ=0.984

â€¦

justiï¬es the subgraph importance as 0.913, which shows

the effectiveness of our DSE framework.
We also observe some limitations of the generator (1) Due to the limited training data, the generators only reï¬‚ect the distribution of the observed graphs, thus making some

Impğ“–ğ’”âˆ—ğ’ =0.827

average

Impdse = ğŸ. ğŸ—ğŸğŸ‘

Figure 5: A Case Example.

generations grammatically wrong. (2) The generations is constrained within the complete graph

determined by the node set of the explanatory subgraph, thereby limits the quality of deconfounding.

As we mainly focus on the OOD problem, we will leave the ability of the generator as future work.

4.3 STUDY OF GENERATORS (RQ2)
The generator plays an important role in our DSE framework, which aims to generate the valid surrogates conform to the data distribution. To evaluate the generatorâ€™s quality, we compare it with three baselines: a random generator, a variational graph auto-encoder (VGAE) (Thomas N. Kipf, 2016), and an adversarially regularized variational graph auto-encoder (ARGVA) (Pan et al., 2018). We perform the evaluation based on two metrics: (1) Validity. For the ground-truth explanations Gs+ that contains all discriminative information of the full graph G, the importance of its surrogate graph Gsâˆ— should be higher than itself. The difference between the two importance scores indicates the validity of the generator, thus we deï¬ne VAL = EG[Imp(Gsâˆ—) âˆ’ Imp(Gs+)]. For Graph-SST2 where the class-wise features are intractable, we leverage the embeddings of training graphs and additionally train a Gaussian Mixture Model (GMM) as our distribution prior. Then, we compute the average loglikelihood of random subgraphs after in-ï¬lling, thus we have VAL = EGEGsâˆ¼Random(G)[GMM(Gsâˆ—) âˆ’ GMM(Gs)]. (2) Fidelity. Towards a ï¬ner-grained assessment w.r.t. prediction probability of any

8

Preprint
random subgraphs, we adopt the metric following (Frye et al., 2021): FID = EGEGs Ey|fy(G) âˆ’ EGsâˆ— [fy(Gsâˆ—)]|2. This measures how well the surrogates cover the target prediction distribution. Before comparing different generators, we ï¬rst compute the importance or probabilities of the graphs before and after feature removal, which are summarized in Table 2. When inspecting the Removalâ€™s results without any in-ï¬lls, the OOD problem is severe: in TR3 and MNISTsup, the importance of ground-truth subgraphs only reaches 43.8% and 40.8%, respectively, which are far away from the target importance of full graphs. Analogously in Graph-SST2. For the performance of the generators w.r.t. the two metrics, we summarize the average results over 5 runs in Table 3:
â€¢ The performance of the baselines are poor. This suggests that they can hardly ï¬t the target conditional distribution.
â€¢ CVGAE outperforms other generators consistently across all cases, thus justifying the rationale and effectiveness of our proposed generator and adversarial training paradigm. For example, in TR3, CVGAE signiï¬cantly increases the VAL scores and mitigates the OOD effect effectively.
Moreover, we conduct ablation studies and sensitivity analysis in Appendix G to better understand the model components and validate the effectiveness of the designed objective.
5 RELATED WORK
Post-hoc Explainability of GNNs. Inspired by the explainability in computer vision, Baldassarre & Azizpour (2019); Pope et al. (2019); Schnake et al. (2020) obtain the gradient-like scores of the modelâ€™s outcome or loss w.r.t. the input features. Another line (Luo et al., 2020; Ying et al., 2019; Yuan et al., 2020a; Yue Zhang, 2020; Michael Sejr Schlichtkrull, 2021) learns the masks on graph features. Typically, GNN-Explainer (Ying et al., 2019) applies the instance-wise masks on the messages carried by graph structures, and maximizes the mutual information between the masked graph and the prediction. Going beyond the instance-wise explanation, PGExplainer (Luo et al., 2020) generates masks for multiple instances inductively. Recently, researchers adopt the causal explainability (Pearl & Mackenzie, 2018) to uncover the causation of the model predictions.For instance, CXPlain (Schwab & Karlen, 2019) quantiï¬es a featureâ€™s importance by leaving it out. PGM-Explainer (Vu & Thai, 2020) performs perturbations on graph structures and builds an Bayesian network upon the perturbation-prediction pairs. Causal Screening (Screener) (Anonymous, 2021) measures the importance of an edge as its causal effect, conditional on the previously selected structures. Lately, SubgraphX (Yuan et al., 2021) explores different subgraphs with Monte-Carlo tree search and evaluates subgraphs with the Shapley value (Kuhn & Tucker, 1953).
Counterfactual Generation for the OOD Problem. The OOD effect of feature removal has been investigated in some other domains. There are generally two classes of generation (i) Static generation. For example, Fong & Vedaldi. (2017); Dabkowski & Gal (2017) adopted blurred input and random colors for the image reference, respectively. Due to the unnatural in-ï¬lling, the generated images are distributional irrespective and can still introduce confounding bias. (ii) Adaptive generation: Chang et al. (2019); Frye et al. (2021); Agarwal et al. (2019); Kim et al. (2020). The generators of these methods, like DSE, overcomes the defects aforementioned, which generates data that conforms to the training distribution. For example, in computer vision, FIDO (Chang et al., 2019) generates imagespeciï¬c explanations that respect the data distribution, answering â€œWhich region, when replaced by plausible alternative values, would maximally change classiï¬er output?â€.
For the difference, ï¬rstly, DSEâ€™s formulated importance involves additional adjustment on Gs and guarantees the unbiasedness of introducing the surrogate variable Gâˆ—s, which is commonly discarded by the prior works with in-ï¬llings only. Speciï¬cally, we offer a comparison with FIDO in Appendix B. Secondly, the distribution of graph data is more complicated to model than other domains. And the proposed CVGAE is carefully designed for graph data, where the contrastive loss and the adversarial training framework are shown to be effective for learning the data distribution of graphs.
6 CONCLUSION
In this work, we investigate the OOD effect on the explanation evaluation of GNNs. With a causal view, we uncover the OOD effect â€” the distribution shift between full graphs and subgraphs, as the
9

Preprint confounder between the explanatory subgraphs and the model prediction, making the evaluation less reliable. To mitigate it, we propose a deconfounding evaluation framework that exploits the front-door adjustment to measure the causal effect of the explanatory subgraphs on the model prediction. And a deep generative model is devised to achieve the front-door adjustment by generating in-distribution surrogates of the subgraphs. In-so-doing, we can reliably evaluate the explanatory subgraphs. As the evaluation for explanations fundamentally guides the objective in GNNs explainability, this work offers in-depth insights into the future interpretability systems.
10

Preprint
ETHICS STATEMENT
This work raises concerns about the removal-based evaluation in the explainability literature and proposed Deconfounded Subgraph Evaluator. For the user study that involves human subjects, we have detailed the fair evaluation procedure for each explanation generated by the explainers in Appendix E. For real-world applications, we admitted that the modeling of the distribution shift could be a barrier to fulï¬ll their evaluation faithfulness. However, as shown in the paper, improper evaluation under the OOD setting largely biases the inspection of the modelâ€™s decision-making process and the quality of explainers. Therefore, we argue that explainability should exhibit faithful explanation evaluation before auditing deep modelsâ€™ actual decision-making process. And a wrongly evaluated explanation might do more signiï¬cant harm than an incorrect prediction, as the former could affect the general adjustment (e.g., structure construction) and human perspective (e.g., fairness check) of the model.
REPRODUCIBILITY STATEMENT
We have made great efforts to ensure reproducibility in this paper. Firstly, we make all causal assumptions clear in Section 2.2, Section 3.1 and Appendix A. For datasets, we have released the synthetic dataset, which can be referred to the link in Section 1, while the other two datasets are publicly available. We also include our code for model construction in the link. In Appendix D, we have reported the settings of hyper-parameters used in our implementation for model training.
REFERENCES
Chirag Agarwal, Dan Schonfeld, and Anh Nguyen. Removing input features via a generative model to explain their attributions to classiï¬erâ€™s decisions. CoRR, 2019.
Anonymous. Causal screening to interpret graph neural networks. in Submitted to ICLR. https: //openreview.net/forum?id=nzKv5vxZfge, 2021.
Federico Baldassarre and Hossein Azizpour. Explainability techniques for graph convolutional networks. CoRR, abs/1905.13686, 2019.
Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining image classiï¬ers by counterfactual generation. In ICLR, 2019.
Ian Covert, Scott Lundberg, and Su-In Lee. Feature removal is a unifying principle for model explanation methods. In NeurIPS, 2020.
Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classiï¬ers. In NeurIPS, pp. 6967â€“6976, 2017.
Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on graph structured data. In ICML, pp. 1123â€“1132, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Vijay Prakash Dwivedi, Chaitanya K. Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. CoRR, abs/2003.00982, 2020.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.
Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich MÃ¼ller. Splinecnn: Fast geometric deep learning with continuous b-spline kernels. In CVPR, pp. 869â€“877, 2018.
R. Fong and A. Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In ICCV, 2017.
Christopher Frye, Damien de Mijolla, Laurence Cowton, Megan Stanley, and Ilya Feige. Shapleybased explainability on the data manifold. CoRR, abs/2006.01272, 2020.
11

Preprint
Christopher Frye, Damien de Mijolla, Laurence Cowton, Megan Stanley, and Ilya Feige. Shapleybased explainability on the data manifold. In ICLR, 2021.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. Commun. ACM, 63(11): 139â€“144, 2020.
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NeurIPS, pp. 1024â€“1034, 2017.
Irina Higgins, LoÃ¯c Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In ICLR, 2017.
P.W. Holland. Causal inference, path analysis, and recursive structural equations models. C. Clogg, editor, Sociological Methodology, pages 449â€“484. American Sociological Association, Washington, D.C., 1988.
Siwon Kim, Jihun Yi, Eunji Kim, and Sungroh Yoon. Interpretation of NLP models through input marginalization. In EMNLP, pp. 3154â€“3167, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
H. W. Kuhn and A. W. Tucker. Contributions to the theory of games, volume 2. Princeton University Press, 1953.
Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graphs over time: densiï¬cation laws, shrinking diameters and possible explanations. In KDD, pp. 177â€“187, 2005.
Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L. Gaunt. Constrained graph variational autoencoders for molecule design. In NeurIPS, pp. 7806â€“7815, 2018.
Roger Wattenhofer Lukas Faber, Amin K. Moghaddam. Contrastive graph neural network explanation. In ICLR Workshop on Representation Learning, 2021.
Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. Parameterized explainer for graph neural network. In NeurIPS, 2020.
LÃ©on Bottou Martin Arjovsky, Soumith Chintala. Wasserstein generative adversarial networks. In ICML, 2017.
Ivan Titov Michael Sejr Schlichtkrull, Nicola De Cao. Interpreting graph neural networks for nlp with differentiable edge masking. In ICLR, 2021.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele RodolÃ , Jan Svoboda, and Michael M. Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In CVPR, pp. 5425â€“5434, 2017.
Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adversarially regularized graph autoencoder for graph embedding. In IJCAI, 2018.
Judea Pearl. Causality: Models, Reasoning, and Inference. 2000.
Judea Pearl and Dana Mackenzie. The book of why: the new science of cause and effect. Basic Books, 2018.
Judea Pearl, Madelyn Glymour, and Nicholas P Jewell. Causal inference in statistics: A primer. John Wiley & Sons, 2016.
Phillip E. Pope, Soheil Kolouri, Mohammad Rostami, Charles E. Martin, and Heiko Hoffmann. Explainability methods for graph convolutional neural networks. In CVPR, pp. 10772â€“10781, 2019.
12

Preprint
Ekagra Ranjan, Soumya Sanyal, and Partha Pratim Talukdar. ASAP: Adaptive structure aware pooling for learning hierarchical graph representations. arXiv preprint arXiv:1911.07979, 2019.
Ekagra Ranjan, Soumya Sanyal, and Partha P. Talukdar. ASAP: adaptive structure aware pooling for learning hierarchical graph representations. In AAAI, pp. 5470â€“5477, 2020.
Thomas Schnake, Oliver Eberle, Jonas Lederer, Shinichi Nakajima, K. T. Schutt, Klaus-Robert Muller, and GrÃ©goire Montavon. Higher-order explanations of graph neural networks via relevant walks. arXiv, 2020.
Patrick Schwab and Walter Karlen. Cxplain: Causal explanations for model interpretation under uncertainty. In NeurIPS, pp. 10220â€“10230, 2019.
Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, pp. 618â€“626, 2017.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, pp. 1631â€“1642, 2013.
Raphael Suter, ÃorÃ°e Miladinovic, Bernhard SchÃ¶lkopf, and Stefan Bauer. Robustly disentangled causal mechanisms: Validating deep representations for interventional robustness. In ICML, volume 97, pp. 6056â€“6065, 2019.
Max Welling Thomas N. Kipf. Variational graph auto-encoders. In NeurIPS Workshops, 2016. Petar VelicË‡kovicÂ´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua
Bengio. Graph attention networks. ICLR, 2018. accepted as poster. Minh N. Vu and My T. Thai. Pgm-explainer: Probabilistic graphical model explanations for graph
neural networks. In NeurIPS, 2020. Tian Xie and Jeffrey C. Grossman. Crystal graph convolutional neural networks for an accurate and
interpretable prediction of material properties. Phys. Rev. Lett., 120:145301, Apr 2018. Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer:
Generating explanations for graph neural networks. In NeurIPS, pp. 9240â€“9251, 2019. Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. XGNN: towards model-level explanations of
graph neural networks. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD, pp. 430â€“438, 2020a. Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic survey. CoRR, 2020b. Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks via subgraph explorations. ArXiv, 2021. Zhongqi Yue, Tan Wang, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. Counterfactual zero-shot and open-set visual recognition. In CVPR, 2021. Arti Ramesh Yue Zhang, David Defazio. Relex: A model-agnostic relational model explainer. arXiv preprint arXiv:2006.00305, 2020. Daniel ZÃ¼gner, Amir Akbarnejad, and Stephan GÃ¼nnemann. Adversarial attacks on neural networks for graph data. In KDD, pp. 2847â€“2856, 2018.
13

Preprint

A ELABORATION FOR OOD VARIABLE

Training Distribution
ğ‘«ğ’•ğ’“

Model Weight
ğ‘¾

ğ‘¨

Attributor

ğ‘®

ğ‘®ğ¬

ğ’€

ğ‘®

Input Graph Explanatory Subgraph

Model Prediction

ğ‘«ğ’•ğ’“

ğ‘¾

ğ‘¨

ğ‘®ğ¬

ğ’€

Collective Representation
ğ‘®

Distribution Shift
ğ‘«

ğ‘®ğ¬

ğ’€

Figure 6: Elaboration for variable D in Figure 1b, where the deï¬nitions of the variables are annotated. The blue, green, red arrows represent the data-generation process of model training, explaining, and removal-based evaluation, respectively; the dashed arrow denotes optional causal relation.

We offer a more ï¬ne-grained introduction of variable D here. We ï¬rst detail the data-generation process of model training, feature attribution and explanation evaluation of feature removal in Figure 6. Next, we justify each process:
â€¢ Model Training: Dtr â†’ W , which represents that the weights of the trained GNN f is a random variable set by the optimizer, e.g., a SGD, from a ï¬nite training dataset starting from random weights.
â€¢ Feature Attribution: Taking a view of certain explainers (Baldassarre & Azizpour, 2019; Selvaraju et al., 2017), the logits or probability vector from the last layer of trained GNN is redistributed using weights of the network onto the input and it will highlight the relevant edges. Thus, the causal relation W â†’ A holds. Moreover, for parameterized explainers (Luo et al., 2020) which optimizes on the masks of the input features, the attributors (explainers) also results from the training distribution, forming the causal relation Dtr â†’ A. Finally, with input graphs G, we have G â†’ Gs â† A where Gs is the attribution from Aâ€™s algorithmic functions on the input graphs.
â€¢ Explanation Evaluation of Feature Removal: traditional evaluation methods simply evaluate the faithfulness of explanation via model forward, i.e., YË† = fW (Gs) and regard the softmax readout on target prediction as the subgraph importance.
For simplicity, we combine variables Dtr, W and A collectively as an abstract distribution shift variable D which is unobservable from real data. Thus, this justiï¬es the existence of D and our proposed SCM I.

B COMPARISON OF IMPORTANCE ESTIMATIONS

In this section, we compare our proposed estimation via front-door adjustment with the estimation in FIDO (Chang et al., 2019). We rephrased each estimation as

Impdse(Gs) = P (Gâˆ—s = Gsâˆ— | Gs = Gs) P (Y | Gâˆ—s = Gsâˆ—)

Gsâˆ—
= P (Gâˆ—s = Gsâˆ— | Gs = Gs)

(9) P (Y | Gâˆ—s = Gsâˆ—, Gs = Gs) P (Gs = Gs)

Gsâˆ—

Gs

and

ImpFIDO(Gs) = P (Gâˆ—s = Gsâˆ— | Gs = Gs) P (Y | Gâˆ—s = Gsâˆ—)

(10)

Gsâˆ—

14

Preprint

where DSE has alternatively adjusted on Gs (represented as Gs). To make it clear, we consider the underlined part of each equation. For Equation 9, we have

P (Y | Gâˆ—s = Gsâˆ—, Gs = Gs) P (Gs = Gs)

Gs

=

Gs

P (Y

| Gâˆ—s

= Gsâˆ—, Gs

= Gs) P

(Gs

= Gs

| Gâˆ—s

=

Gsâˆ—)

P

P (Gs (Gs = Gs

= Gs) | Gâˆ—s =

Gsâˆ—)

(11)

=

Gs

P (Y, Gs

= Gs

| Gâˆ—s

= Gsâˆ—) P

P (Gs = Gs) (Gs = Gs | Gâˆ—s = Gsâˆ—)

While for the formulation of Equation 10, we have

P (Y | Gâˆ—s = Gsâˆ—) = P (Y, Gs = Gs | Gâˆ—s = Gsâˆ—)

(12)

Gs

In the comparison of these two parts, we can see that Equation 12 is biased under our causal assumption. Intuitively, each contribution of the importance of Gsâˆ— on Y should be inversely proportional to the posterior probability, i.e., the probability of Gs given the observation Gsâˆ—. However, FIDO fails to consider the causal relation between Gs â†’ Gâˆ—s, which biases tha approximation of the genuine causal effect under our causal assumption.
Back to our proposed estimation, as we have collected (Gs, Gsâˆ—)-pairs via Monte-Carlo simulation, thus additional adjustment on Gs (Gs) can be achieved via Equation 11.

C DSE FOR DELETION-BASED EVALUATION

Based on the idea of deletion-based evaluation, we can instead use the average causal effect (Holland.,
1988) (ACE) to look for a smallest deletion graph by conducting two interventions do(Gs = G) (i.e., , no feature removal) and do(Gs = G/s) where G/s denotes the complement of the explanatory graph Gs, meaning that the GNN input receives treatment and control, respectively. Formally, we have

f id

Impdse(Gs = Gs) = P (Y | do (Gs = G)) âˆ’ P Y | do Gs = G/s

(13)

Then, we can similarly adjust for the individual terms as Equation 1, obtaining the unbiased importance value as the result of deletion-based evaluation.

D EXPERIMENTAL DETAILS

In this paper, all experiments are done on a single Tesla V100 SXM2 GPU (32 GB). The well-trained GNNs used in our experiments achieve high classiï¬cation accuracies of 0.958 in TR3, 0.982 in MNISTsup, 0.909 in Graph-SST2.
Now We introduce the model construction of the proposed generator. The encoder used is Crystal Graph Convolutional Neural Networks (Xie & Grossman, 2018), which contains three Convolutional layers. The encode dimensions in Tr3, MNISTsup, Graph-SST2 datasets are respectively 256, 64, 256. For decoder, we adopt two fully connected layers with ReLU as activation layers, where the numbers of neurons are the same with the encode dimensions. Next, we summarize the pseudocodes for the Adversarial Training in Algorithm 1.
15

Preprint
Algorithm 1 Generative Adversarial Training. All experiments in the paper used the default values m = 256, Î± = 2 Ã— 10âˆ’4, Î² = 1 Ã— 10âˆ’4, Ï‰ = Î» = 5, Ï„ = 0.1 Require: Pr, real graphsâ€™ distribution. r, masking ratio. Require: m, batch size. Î±, learning rate. Î², Î³, Î», Ï‰, Ï„ , hyper-parameters. 1: Âµ â† Âµ0; Î¸ â† Î¸0 2: while loss in Equation (4) is not converged do 3: # Discriminatorâ€™s training 4: Sample {G(i)}m i=1 âˆ¼ Pr a batch from the real graphs. 5: Randomly generate broken graphs {Gs(i)}m i=1 from {G(i)}m i=1 with masking ratio r. 6: Embed the nodes through encoder q(Z|{Gs(i), G(i)}m i=1) 7: Decode the edge probabilities and sample in-ï¬ll graphs {GË†sÂ¯}m i=1 âˆ¼ p(GË†sÂ¯ | Z) 8: Compute Discriminatorâ€™s loss from Equation 7. 9: Update parameter Âµ with back-propagation. 10: # Generatorâ€™s training 11: Repeat the operations from line 4 to 7. 12: Compute Generatorâ€™s loss from Equation 4, 5, 6. 13: Update parameter Î¸ with back-propagation. 14: end while
For other hyper-parameters, we set r = 0.3, Î³ = 3 in Tr3 dataset. In MNISTsup and Graph-SST2 datasets, we set r = 0.6, Î³ = 1. We use Adam (Kingma & Ba, 2014) with weight decay rate 1e-5 for optimization. The maximum number of epochs is 100.
E DETAILED USER STUDY
The User Study starts by instructions to participants, where they will see a sentence (movie reviews) in each question and its sentiment (Positive of Negative), e.g.,
Sentence: â€œis more of an ordeal than an amusementâ€ Sentiment: Negative
then several explanations are presented for the answers of â€œWhy the sentiment of this sentence is negative (positive)?â€. The explanations (see Figure 7) are shown in graph form (edges indicate relations between words), and colors of more important features are darker.
Figure 7: Instruction Example for conducting the user study. Then they were asked to choose the best explanation(s). A good explanation should be concise, informative, and the rational cause of sentenceâ€™s sentiment. In this case, (B) could be the best explanation since â€œordealâ€ mostly decides the negative sentiment, while (A) only identiï¬es plain words like â€œmore thanâ€ and (C) is quite the opposite. Note that the participants can choose multiple answers and some choices are the same. Thereafter, 10 questions out of 32 questions in total are presented for each participant and we compute the average scores for the explainers.
16

Preprint

F EXTRA CASE STUDY
In this section, we further present a case study for TR3 dataset. In Figure 8, the OOD probabilities for the ground truth explanatory subgraphs in each row remain the same as the edge selection ratios vary, which are 100%, 0%, 0% respectively. In contrast, the evaluation results generated from our DSE have shown strong rationality. Speciï¬cally, the importance score compute by our DSE increases with the increasing number of selected ground truth edges. This well validates our DSE framework, where we mitigate the OOD effect by generating the plausible surrogates, making the graphs to be evaluated conforms to the graph distribution in the training data. In this way, the effect of D â†’ Y could hardly affect our assessment for the explanatory subgraph. Thereafter, as the explanatory graph becomes more informative and discriminative, it offers more evidence for the GNN to classify it as the target class which we want to explain, yielding faithful evaluation results.

Impdse

Cycle
House
Crane
Figure 8: Three cases in TR3 datasets. Each graph in the left represents the ground truth explanatory subgraphs (red) for explaining a given graph. One of the complement graphs (light blue) generated from CVGAE is also shown with each explanatory subgraph. As the edge selection ratio increases in each row, the importance scores output by our DSE are shown in the right.

G ABLATION STUDY & SENSITIVITY ANALYSIS
We ï¬rst conduct ablation studies to investigate the contribution of the contrastive parameter Î³ and the penalty parameter Î» in CVGAE. The ablation models are proposed by I. removing the contrastive loss, i.e., setting Î³ = 0 and II. removing the penalty term in the Wasserstein GAN (WGAN) (Martin Arjovsky, 2017) loss, i.e., setting Î» = 0. The performance of the ablation models is reported in Table 4. We observe that the superiority of CVGAE compared with the ablation model supports our model design by (i) smoothing the model optimization which yields a more performant generator (ii) highlighting the class-discriminative information in the graph embeddings, which implicitly encodes the class information.
Table 4: Ablation study on proposed CVGAE.

Ablation Models
I. Remove LC II. Remove the penalty in LD CVGAE

TR3 VALâ†‘ FIDâ†“

0.068 0.035
0.165âˆ—

0.643 0.739
0.598âˆ—

MNISTsup VALâ†‘ FIDâ†“

0.038 0.078
0.144âˆ—

1.314 1.139
0.910âˆ—

Graph-SST2 VALâ†‘ FIDâ†“

21.4 11.3
21.8âˆ—

0.065 0.083
0.057âˆ—

Also, we conduct sensitivity analysis for CVGAE w.r.t. the hyper-parameters. Speciï¬cally, we select Î», the penalty in the WGAN loss (cf. Euqation 7) and Î³, the strength of the contrastive loss (cf. Equation 4). While we empirically found the performance is relatively indifferent to other parameters in a wide range. The results are shown in Figure 9. We observe that the best performance is achieved with Î» taking values from 1 to 10, and Î³ taking values from 1 to 10 in TR3 dataset and 0.1 to 5 in MNISTsup and Graph-SST2 datasets. And we found a large Î» generally causes an increase in the FID metric, as it may alleviate the penalty on the reconstruction errors, which further makes a larger difference between fy(G) and E[fy(Gsâˆ—)].
17

Preprint

1 1
ï¿½5
10

ï¿½
3 10

ï¿½

0.1 1

5

1

5

10

0.1 1

ï¿½

1

5

5

10

1

1

1

ï¿½5

5

5

10

10

10

TR3

MNIST sup

Graph-SST2

Figure 9: The performance of CVGAE using different Î» and Î³ values.

18

