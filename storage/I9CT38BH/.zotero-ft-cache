arXiv:2110.01167v2 [cs.AI] 26 May 2022

Trustworthy AI: From Principles to Practices
BO LI, PENG QI, BO LIU, SHUAI DI, JINGEN LIU, JIQUAN PEI, JINFENG YI, and BOWEN ZHOU
The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people’s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.
Additional Key Words and Phrases: trustworthy AI, robustness, generalization, explainability, transparency, reproducibility, fairness, privacy protection, accountability
1 Introduction
The rapid development of Artificial Intelligence (AI) continues to provide significant economic and social benefits to society. With the widespread application of AI in areas such as transportation, finance, medicine, security, and entertainment, there is rising societal awareness that we need these systems to be trustworthy. This is because the breach of stakeholders’ trust can lead to severe societal consequences given the pervasiveness of these AI systems. Such breaches can range from biased treatment by automated systems in hiring and loan decisions [48, 146] to the loss of human life [51]. By contrast, AI practitioners, including researchers, developers, and decision-makers, have traditionally considered system performance (i.e., accuracy) to be the main metric in their workflows. This metric is far from sufficient to reflect the trustworthiness of AI systems. Various aspects of AI systems beyond system performance should be considered to improve their trustworthiness, including but not limited to their robustness, algorithmic fairness, explainability, and transparency.
While most active academic research on AI trustworthiness has focused on the algorithmic properties of models, advancements in algorithmic research alone is insufficient for building trustworthy AI products. From an industrial perspective, the lifecycle of an AI product consists of multiple stages, including data preparation, algorithmic design, development, and deployment as well as operation, monitoring, and governance. Improving trustworthiness in any single aspect (e.g., robustness) involves efforts at multiple stages in this lifecycle, e.g., data sanitization, robust algorithms, anomaly monitoring, and risk auditing. On the contrary, the breach of trust in any single link or aspect can undermine the trustworthiness of the entire system. Therefore, AI trustworthiness should be established and assessed systematically throughout the lifecycle of an AI system.
Contacts: Bo Li, libo427@jd.com; Peng Qi, peng.qi@jd.com; Bo Liu, bo.liu2@jd.com; Shuai Di, dishuai@jd.com; Jingen Liu, jingen.liu@jd.com; Jiquan Pei, peijiquan@jd.com; Jinfeng Yi, yijinfeng@jd.com; Bowen Zhou (*corresponding author), zhoubowen@jd.com, zhoubw@gmail.com.
© 2022 Association for Computing Machinery. This is the author’s version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in , https://doi.org/10.1145/nnnnnnn.nnnnnnn.
, Vol. 1, No. 1, Article . Publication date: May 2022.

2

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

In addition to taking a holistic view of the trustworthiness of AI systems over all stages of their lifecycle, it is important to understand the big picture of different aspects of AI trustworthiness. In addition to pursuing AI trustworthiness by establishing requirements for each specific aspect, we call attention to the combination of and interaction between these aspects, which are important and underexplored topics for trustworthy real-world AI systems. For instance, the need for data privacy might interfere with the desire to explain the system output in detail, and the pursuit of algorithmic fairness may be detrimental to the accuracy and robustness experienced by some groups [361, 284]. As a result, trivially combining systems to separately improve each aspect of trustworthiness does not guarantee a more trustworthy and effective end result. Instead, elaborated joint optimization and trade-offs between multiple aspects of trustworthiness are necessary [361, 158, 331, 378, 46].
These facts suggest that a systematic approach is necessary to shift the current AI paradigm toward trustworthiness. This requires awareness and cooperation from multi-disciplinary stakeholders who work on different aspects of trustworthiness and different stages of the system’s lifecycle. We have recently witnessed important developments in multi-disciplinary research on trustworthy AI. From the perspective of technology, trustworthy AI has promoted the development of adversarial learning, private learning, and the fairness and explainability of machine learning (ML) . Some recent studies have organized these developments from the perspective of either research [218, 357, 182] or engineering [61, 56, 338, 353, 199]. Developments in non-technical areas have also been reviewed in a few studies, and involve guidelines [178, 294, 145], standardization [210], and management processes [301, 30, 274]. We have conducted a detailed analysis of the various reviews, including algorithmic research, engineering practices, and institutionalization, in Section A.2 in the appendix. These fragmented reviews have mostly focused on specific views of trustworthy AI. To synchronize these diverse developments in a systematic view, we organize multi-disciplinary knowledge in an accessible manner for AI practitioners, and provide actionable and systematic guidance in the context of the lifecycle of an industrial system to build trustworthy AI systems. Our main contributions are as follows:
• We dissect the entire lifecycle of the development and deployment of AI systems in industrial applications, and discuss how AI trustworthiness can be enhanced at each stage—from data to AI models, and from system deployment to its operation. We propose a systematic framework to organize the multi-disciplinary and fragmented approaches toward trustworthy AI, and propose pursuing it as a continuous workflow to incorporate feedback at each stage of the lifecycle of the AI system.
• We dissect the entire development and deployment lifecycle of AI systems in industrial applications and discuss how AI trustworthiness can be enhanced at each stage – from data to AI models, from system deployment to its operation. We propose a systematic framework to organize the multi-disciplinary and fragmented approaches towards trustworthy AI, and further propose to pursue AI trustworthiness as a continuous workflow to incorporate feedback at each stage of the AI system lifecycle. We also analyze the relationship between different aspects of trustworthiness in practice (mutual enhancement and, sometimes, trade-offs). The aim is to provide stakeholders of AI systems, such as researchers, developers, operators, and legal experts, with an accessible and comprehensive guide to quickly understand the approaches toward AI trustworthiness (Section 3).
• We discuss outstanding challenges facing trustworthy AI on which the research community and industrial practitioners should focus in the near future. We identify several key issues, including the need for a deeper and fundamental understanding of several aspects of AI trustworthiness (e.g., robustness, fairness, and explainability), the importance of user awareness, and the promotion of inter-disciplinary and international collaboration (Section 4).

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

Security
Fairness

Performance

Robustness

Generalization
Explainability

Privacy

Accountability

Transparency

Safety
Autonomy

Human Value
Sustainability

Auditability

Traceability

Reproducibility

3

Trade-off with

Contribute to

Manifest in
Holistically evaluate all the aspects
Technical requirements
Ethical requirements
Other representative requirements

Fig. 1. The relation between different aspects of AI trustworthiness discussed in this survey. Note that implicit interaction widely exists between aspects, and we cover only representative explicit interactions.

With these contributions, we aim to provide the practitioners and stakeholders of AI systems not only with a comprehensive introduction to the foundations and future of AI trustworthiness, but also with an operational guidebook for how to construct AI systems that are trustworthy.
2 AI Trustworthiness: Beyond Predictive Accuracy
The success of ML technology in the last decades has largely benefited from the accuracy-based performance measurements. By assessing task performance based on quantitative accuracy or loss, training AI models becomes tractable in the sense of optimization. Meanwhile, predictive accuracy is widely adopted to indicate the superiority of an AI product over others. However, with the recent widespread applications of AI, the limitation of an accuracy-only measurement has been exposed to a number of new challenges, ranging from malicious attacks against AI systems to misuses of AI that violate human values. To solve these problems, the AI community has realized in the last decade that factors beyond accuracy should be considered and improved when building an AI system. A number of enterprises [136, 254, 166, 61, 56, 338], academia [199, 322, 122, 301, 218], public sectors, and organizations [9, 334, 210] have recently identified these factors and summarized them as principles of AI trustworthiness. They include robustness, security, transparency, fairness, and safety [178]. Comprehensive statistics relating to and comparisons between these principles have been provided in [178, 145]. In this paper, we study the representative principles that have recently garnered wide interest and are closely related to practical applications. These principles can be categorized as follows:
• We consider representative requirements that pertain to technical challenges faced by current AI systems. We review aspects that have sparked wide interest in recent technical studies, including robustness, explainability, transparency1, reproducibility, and generalization.
• We consider ethical requirements with broad concerns in recent literature [178, 9, 121, 56, 218, 199, 301, 334, 145], including fairness, privacy, and accountability.
In this section, we illustrate the motivation for and definition of each requirement. We also survey approaches to the evaluation of each requirement. It should also be noted that the selected
1We follow [333] to exclude transparency as an ethical requirement.
, Vol. 1, No. 1, Article . Publication date: May 2022.

4

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

requirements are not orthogonal, and some of them are closely correlated. We explain the relationships with the corresponding requirements in this section. We also use Figure 1 to visualize the relationships between aspects, including trade-offs, contributions, and manifestation.
2.1 Robustness In general, robustness refers to the ability of an algorithm or system to deal with execution errors, erroneous inputs, or unseen data. Robustness is an important factor affecting the performance of AI systems in empirical environments. The lack of robustness might also cause unintended or harmful behavior by the system, thus diminishing its safety and trustworthiness. In the context of ML systems, the term robustness is applicable to a diversity of situations. In this review, we non-exhaustively summarize the robustness of an AI system by categorizing vulnerabilities at the levels of data, algorithms, and systems, respectively.
Data. With the widespread application of AI systems, the environment in which an AI model is deployed becomes more complicated and diverse. If an AI model is trained without considering the diverse distributions of data in different scenarios, its performance might be significantly affected. Robustness against distributional shifts has been a common problem in various applications of AI [18]. In high-stake applications, this problem is even more critical owing to its negative effect on safety and security. For example, in the field of autonomous driving, besides developing a perceptual system working in sunny scenes, academia and the industry are using numerous development and testing strategies to enhance the perceptual performance of the vehicles in nighttime/rainy scenes to guarantee the system’s reliability under a variety of weather conditions [382, 318].
Algorithms. It is widely recognized that AI models might be vulnerable to attacks by adversaries with malicious intentions. Among the various forms of attacks, the adversarial attack and defenses against it have raised concerns in both academia and the industry in recent years. Literature has categorized the threat of adversarial attacks in several typical aspects and proposed various defense approaches [68, 304, 11, 373, 213]. For example, in [340], adversarial attacks were categorized with respect to the attack timing. Decision-time attack perturbs input samples to mislead the prediction of a given model so that adversary could evade security checks or impersonate victims. Training-time attack injects carefully designed samples into the training data to change the system’s response to specific patterns, and is also known as poisoning attack. Considering the practicality of attacks, it is also useful to note the differences of attacks in terms of the spaces in which they are carried out. Conventional studies have mainly focused on feature space attacks, which are generated directly as the input features of a model. In many practical scenarios, the adversaries can modify only the input entity to indirectly produce attack-related features. For example, it is easy for someone to wear adversarial pattern glasses to evade a face verification system but difficult to modify the image data in memory. Studies on producing realizable entity-based attacks (problem space attacks) have recently garnered increasing interest [325, 358]. Algorithm-level threats might exist in various forms, in addition to directly misleading AI models. Model stealing (a.k.a. the exploratory attack) attempts to steal knowledge about models. Although it does not directly change model behavior, the stolen knowledge has significant value for generating adversarial samples [329].
Systems. System-level robustness against illegal inputs should also be carefully considered in realistic AI products. The cases of illegal inputs can be extremely diverse in practical situations. For example, an image with a very high resolution might cause an imperfect image recognition system to hang. A lidar perception system for an autonomous vehicle might perceive laser beams emitted by lidars in other vehicles and produce corrupted inputs. The presentation attack [275] (a.k.a. spoof attack) is another example that has generated wide concerns in recent years. It fakes inputs by, for example, photos or masks to fool biometric systems.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

5

Various approaches have been explored to prevent vulnerabilities in AI systems. The objective of defense can be either proactive or reactive [227]. A proactive defense attempts to optimize the AI system to be more robust to various inputs while a reactive defense aims at detecting potential security issues, such as changing distributions or adversarial samples. Representative approaches to improve the robustness of an AI system are introduced in Section 3.
Evaluation Evaluating the robustness of an AI system serves as an important means of avoiding vulnerabilities and controlling risks. We briefly describe two groups of evaluations: robustness test and mathematical verification.
Robustness test. Testing has served as an essential approach to evaluate and enhance the robustness not only of conventional software, but also of AI systems. Conventional functional test methodologies, such as the monkey test [115], provide effective approaches to evaluating the systemlevel robustness. Moreover, as will be introduced in Section 3.3.1, software testing methodologies have recently been extended to evaluate robustness against adversarial attacks [260, 226].
In comparison with functional test, performance test, i.e., benchmarking, are more widely adopted approach in the area of ML to evaluate system performance along various dimensions. Test datasets with various distributions are used to evaluate the robustness of data in ML research. In the context of adversarial attacks, the minimal adversarial perturbation is a core metric of robustness, and its empirical upper bound, a.k.a. empirical robustness, on a test dataset has been widely used [312, 64]. From the attacker’s perspective, the rate of success of an attack also intuitively measures the robustness of the system [312].
Mathematical verification. Inherited from the theory of formal method, certified verification of the adversarial robustness of an AI model has led to growing interest in research on ML. For example, adversarial robustness can be reflected by deriving a non-trivial and certified lower bound of the minimum distortion to an attack on an AI model [379, 50]. We introduce this direction in Section 3.2.1.
2.2 Generalization Generalization has long been a source of concern in ML models. It represents the capability to distill knowledge from limited training data to make accurate predictions regarding unseen data [133]. Although generalization is not a frequently mentioned direction in the context of trustworthy AI, we find that its impact on AI trustworthiness should not be neglected and deserves specific discussion. On one hand, generalization requires that AI systems make predictions on realistic data, even on domains or distributions on which they are not trained [133]. This significantly affects the reliability and risk of practical systems. On the other hand, AI models should be able to generalize without the need to exhaustively collect and annotate large amounts of data for various domains [391, 343], so that the deployment of AI systems in a wide range of applications is more affordable and sustainable.
In the field of ML, the canonical research on generalization theory has focused on the prediction of unseen data, which typically share the same distribution as the training data [133]. Although AI models can achieve a reasonable accuracy on training datasets, it is known that a gap (a.k.a generalization gap) exists between their training and testing accuracies. Approaches in different areas, ranging from statistic learning to deep learning, have been studied to analyze this problem and enhance the model generalization. Typical representatives like cross-validation, regularization, and data augmentation can be found in many ML textbooks [133].
The creation of a modern data-driven AI model requires a large amount of data and annotations in the training stage. This leads to a high cost for manufacturers and users for re-collecting and re-annotating data to train the model for each task. The cost highlights the need to generalize the knowledge of a model to different tasks, which not only reduces data cost, but also improves

, Vol. 1, No. 1, Article . Publication date: May 2022.

6

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

model performance in many cases. Various directions of research have been explored to address knowledge generalization under different scenarios and configurations within the paradigm of transfer learning [255, 350]. We review the representative approaches in Section 3.2.2.
The inclusive concept of generalization is closely related to other aspects of AI trustworthiness, especially robustness. In the context of ML, the robustness against distributional shifts mentioned in Section 2.1 is also considered a problem of generalization. This implies that the requirements of robustness and generalization have some overlapping aspects. The relationship between adversarial robustness and generalization is more complicated. As demonstrated in [362], an algorithm that is robust against small perturbations has better generalization. Recent research [331, 270], however, has noted that improving adversarial robustness by adversarial training may reduce the testing accuracy and leads to worse generalization. To explain this phenomenon, [116] has argued that the adversarial robustness corresponds to different data distributions that may hurt a model’s generalization.
Evaluation Benchmarking on test datasets with various distributions is a widely used approach to evaluate the generalization of an AI model in realistic scenarios. A summary of commonly used datasets and benchmarks for domain generalization can be found in [391], and covers the tasks of object recognition, action recognition, segmentation, and face recognition.
In terms of theoretical evaluation, past ML studies have developed rich approaches to measure the bounds of a model’s generalization error. For example, Rademacher complexity [34] is commonly used to determine how well a model can fit a random assignment of class labels. In addition, the Vapnik-Chervonenkis (VC) dimension [337] is a measure of the capacity/complexity of a learnable function set. A larger number of VC dimensions indicates a higher capacity.
Advances in the DNN has led to new developments in the theory of generalization. [377] observed that modern deep learning models can achieve a generalization gap despite their massive capacity. This phenomenon has led to academic discussions on the generalization of the DNN [22, 38]. For example, [38] examined generalization from the perspective of the bias–variance trade-off to explain and evaluate the generalization of the DNN.
2.3 Explainability and Transparency The opaqueness of complex AI systems has led to widespread concerns in academia, the industry, and society at large. The problem of how DNNs outperform other conventional ML approaches has been puzzling researchers [23]. From the perspective of practical systems, there is a demand among users for the right to know the intention, business model, and technological mechanism of AI products [9, 135]. A variety of studies have addressed these problems in terms of nomenclature including interpretability, explainability, and transparency [23, 140, 46, 216, 250, 4], and have delved into different definitions. To make our discussion more concise and targeted, we narrow the coverage of explainability and transparency to address the above concerns in theoretical research and practical systems, respectively.
• Explainability addresses to understand how an AI model makes decision [23]. • Transparency considers AI as a software system, and seeks to disclose information regarding
its entire lifecycle (c.f., “operate transparently” in [9]).
2.3.1 Explainability Explainability, i.e., understanding how an AI model makes its decision, stays at the core place of modern AI research and serves as a fundamental factor that determines the trust in AI technology. The motivation for the explainability of AI comes from various aspects [24, 23]. From the perspective of scientific research, it is meaningful to understand all intrinsic mechanisms of the data, parameters, procedures, and outcomes in an AI system. The mechanisms also fundamentally determine AI trustworthiness. From the perspective of building AI products, there exist various practical requirements on explainability. For operators like bank executives, explainability helps

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

7

understand the AI credit system to prevent potential defects in it [24, 185]. Users like loan applicants are interested to know why they are rejected by the model, and what they can do to qualify [24]. See [24] for a detailed analysis of the various motivations of explainability.
Explaining ML models has been an active topic not only in ML research, but also in psychological research in the past five years [23, 140, 46, 216, 250, 4]. Although the definition of the explainability of an AI model is still an open question, research has sought to address this problem from the perspectives of AI [285, 140] and psychology [144, 245]. In summary, the relevant studies divide explainability into two levels to explain it.
• Model explainability by design. A series of fully or partially explainable ML models have been designed in the past half-century of ML research. Representatives include linear regression, trees, the KNN, rule-based learners, generalized additive model (GAM), and Bayesian models [23]. The design of explainable models remains an active area in ML.
• Post-hoc model explainability. Despite the good explainability of the above conventional models, more complex models such as the DNN or GDBT have exhibited better performance in recent industrial AI systems. Because the relevant approaches still cannot holistically explain these complex models, researchers have turned to post-hoc explanation. It addresses a model’s behavior by analyzing its input, intermediate result, and output. A representative category in this vein approximates the decision surface either globally or locally by using an explainable ML model, i.e., explainer, such as a linear model [279, 225] and rules [280, 141]. For deep learning models like the CNN or the transformer, the inspection of intermediate features is a widely used means of explaining model behavior [332, 366].
Approaches to explainability are an active area of work in ML and have been comprehensively surveyed in a variety of studies [23, 140, 46, 250]. Representative algorithms to achieve the above two levels of explainability are reviewed in Section 3.2.3.
Evaluation In addition to the problem of explaining AI models, a unified evaluation of explainability has been recognized as a challenge. A major reason for this lies in the ambiguity of the psychological outlining of explainability. To sidestep this problem, a variety of studies have used qualitative metrics to evaluate explainability with human participation. Representative approaches include the following:
• Subjective human evaluation. The methods of evaluation in this context include interviews, self-reports, questionnaires, and case studies that measure, e.g., user satisfaction, mental models, and trust [144, 155, 267].
• Human–AI task performance. In tasks performed with human–AI collaboration, the collaborative performance is significantly affected by the human understanding of the AI collaborator, and can be viewed as a reflection of the quality of explanation [249]. This evaluation has been used for the development of, for instance, recommendation systems [198] and data analysis [132].
In addition, if explainability can be achieved by an explainer, the performance of the latter, such as in terms of the precision of approximation (fidelity [141, 280, 279]), can be used to indirectly and quantitatively evaluate explainability [15].
Despite the above evaluations, a direct quantitative measurement of explainability remains a problem. Some naive measurements of model complexity, like tree depth [45] and the size of the rule set [202], have been studied as surrogate explainability metrics in previous work. We believe that a unified quantitative metric lies at the very heart of fundamental AI research. Recent research on the complexity of ML models [162] and their cognitive functional complexity [347] may inspire future research on a unified quantitative evaluation metric.

, Vol. 1, No. 1, Article . Publication date: May 2022.

8

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

2.3.2 Transparency Transparency requires the disclosure of the information on a system, and has long been a recognized requirement in software engineering [207, 88]. In the AI industry, this requirement naturally covers the lifecycle of an AI system and helps stakeholders confirm that appropriate design principles are reflected in it. Consider a biometric system for identification as an example. Users are generally concerned with the purpose for which their biometric information is collected and how it is used. Business operators are concerned with the accuracy and robustness against attacks so that they can control risks. Government sectors are concerned with whether the AI system follows guidelines and regulations. On the whole, transparency serves as a basic requirement to build the public’s trust in AI systems [178, 21, 189].
To render the lifecycle of an AI system transparent, a variety of information regarding its creation needs to be disclosed, including the design purposes, data sources, hardware requirements, configurations, working conditions, expected usage, and system performance. A series of studies have examined disclosing this information through appropriate documentation [21, 129, 156, 246, 265]. This is discussed in Section 3.5.1. The recent trend of open-source systems also significantly contributes to the algorithmic transparency of AI systems.
Owing to the complex and dynamic internal procedure of an AI system, facts regarding its creation are not sufficient in many cases to fully reveal its mechanism. Hence, the transparency of the runtime process and decision-making should also be considered in various scenarios. For an interactive AI system, an appropriately designed user interface serves as an important means to disclose the underlying decision procedure [8]. In many safety-critical systems, such as autonomous driving vehicles, logging systems [28, 261, 369] are widely adopted to trace and analyze the system execution.
Evaluation Although a unified quantitative evaluation is not yet available, the qualitative evaluation of transparency has undergone recent advances in the AI industry. Assessment checklists [8, 292] have been regarded as an effective means to evaluate and enhance the transparency of a system. In the context of the psychology of users or the public, user studies or A/B test can provide a useful evaluation based on user satisfaction [249].
Quality evaluations of AI documentation have also been explored in recent years. Some studies [272, 21, 129, 156, 246] have proposed standard practices to guide and evaluate the documentation of an AI system. [265] summarized the general qualitative dimensions for more specified evaluations.
2.4 Reproducibility Modern AI research involves both mathematical derivation and computational experiments. The reproducibility of these computational procedures serves as an essential step to verify AI research. In terms of AI trustworthiness, this verification facilitates the detection, analysis, and mitigation of potential risks in an AI system, such as a vulnerability on specific inputs or unintended bias. With the gradual establishment of the open cooperative ecosystem in the AI research community, reproducibility is emerging as a concern among researchers and developers. In addition to enabling the effective verification of research, reproducibility allows the community to quickly convert the latest approaches into practice or conduct follow-up research.
There has been a new trend in the AI research community to regard reproducibility as a requirement when publicizing research [142]. We have seen major conferences, such as NeurIPS, ICML, and ACMMM, introduce reproducibility-related policies or programs [263] to encourage the reproducibility of works. To obtain a clear assessment, degrees of reproducibility have been studied in such works as the ACM Artifact Review and [143, 106]. For example, in [143], the lowest degree of reproducibility requires the exact replication of an experiment with the same implementation and data, while a higher degree requires using different implementations or data. Beyond the basic

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

9

verification of research, a higher degree of reproducibility promotes a better understanding of the research by distinguishing among the key factors influencing effectiveness.
Some recently developed large scale pre-trained AI models, such as GPT-3 and BERT, are representative of the challenges to the reproducibility of AI research. The creation of these models involves specifically designed data collection strategies, efficient storage of big data, communication and scheduling between distributed clusters, algorithm implementation, appropriate software and hardware environments, and other kinds of knowhow. The reproducibility of such a model should be considered over its entire lifecycle. In recent studies on the reproducibility of ML, this requirement has been decomposed into the reproducibility of data, methods, and experiments [143, 142, 169], where the latter range over a series of lifecycle artifacts such as code, documentation, software, hardware, and deployment configuration. Based on this methodology, an increasing number of ML platforms are being developed that assist researchers and developers better track the lifecycle in a reproducible manner [169, 374].
Evaluation Reproducibility checklists have been recently widely adopted in ML conferences to assess the reproducibility of submissions [263]. Beyond the replication of experiments in publication, [142, 143] also specified checklists to evaluate reproducibility at varying degrees. In addition to checklists, mechanisms such as challenges to reproducibility and paper tracks of reproducibility have been adopted to evaluate the reproducibility of publications [263, 118]. To quantitatively evaluate reproducibility in the context of challenges, a series of quantitative metrics have been studied. For example, [118, 52] designed metrics to quantify how closely an information retrieval system can be reproduced to its origins.
2.5 Fairness When AI systems help us in areas such as hiring, financial risk assessment, and face identification, systematic unfairness in their decisions might have negative social ramifications (e.g., underprivileged groups might experience systematic disadvantage in hiring decisions [48], or be disproportionally impacted in criminal risk profiling [104, 161, 146]). This not only damages the trust that various stakeholders have in AI, but also hampers the development and application of AI technology for the greater good. Therefore, it is important that practitioners keep in mind the fairness of AI systems to avoid instilling or exacerbating social bias [242, 65, 105].
A common objective of fairness in AI systems is to eliminate or mitigate the effects of biases The mitigation is non-trivial because the biases can take various forms, such as data bias, model bias, and procedural bias, in the process of developing and applying AI systems [242]. Bias often manifests in the form of unfair treatment of different groups of people based on their protected information (e.g., gender, race, and ethnicity). Therefore, group identity (sometimes also called sensitive variables) and system response (prediction) are two factors influencing bias. Some cases also involve objective ground truths of a given task that one should consider when evaluating system fairness, e.g., whether a person’s speech is correctly recognized or their face correctly identified.
Fairness can be applicable at multiple granularities of system behavior [242, 65, 339]. At each granularity, we might be concerned with distributive fairness or fairness of the outcome, or procedural fairness or fairness of the process (we refer the reader to [137] for a more detailed discussion). In each case, we are commonly concerned with the aggregated behavior and the bias therein of an AI system, which is referred to as statistical fairness or group fairness. In certain applications, it is also helpful to consider individual fairness or counterfactual fairness, especially when the sensitive variable can be more easily decoupled from the other features that should justifiably determine the system’s prediction [242]. While the former is more widely applicable to various ML tasks, e.g.,

, Vol. 1, No. 1, Article . Publication date: May 2022.

10

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

speech recognition and face identification, the latter can be critical in cases like resume reviewing for candidate screening [43].
At the group level, researchers have identified three abstract principles to categorize different types of fairness [65]. We illustrate them with a simple example of hiring applicants from a population consisting of 50% male and 50% female applicants, where gender is the sensitive variable (examples adapted from [339, 388]):
• Independence. This requires for the system outcome to be statistically independent of sensitive variables. In our example, this requires that the rate of admission of male and female candidates be equal (known as demographic parity [376]; see also disparate impact [117]).
• Separation. Independence does not account for a justifiable correlation between the ground truth and the sensitive variable (e.g., fewer female candidates might be able to lift 100-lb goods more easily than male candidates). Separation therefore requires that the independence principle hold, conditioned on the underlying ground truth. That is, if the job requires strength qualifications, the rate of admission for qualified male and female candidates should be equal (known as equal opportunity [147]; see also equal odds [42] and accuracy equity [95]).
• Sufficiency. Sufficiency similarly considers the ground truth but requires that the true outcome and the sensitive variable be independent when conditioned on the same system prediction. That is, given the same hiring decision predicted by the model, we want the same ratio of qualified candidates among male and female candidates (known as test fairness [79, 147]). This is closely related to model calibration [266].
Note that these principles are mutually exclusive under certain circumstances (e.g., independence and separation cannot both hold when the sensitive variable is correlated with the ground truth). [187] has discussed the trade-off between various fairness metrics. Furthermore, [83] advocated an extended view of these principles, where the utility of the predicted and true outcomes is factored into consideration (e.g., the risk and cost of recidivism in violent crimes compared with the cost of detention), and can be correlated with the sensitive variables. We refer the reader to this work for a more detailed discussion.
Evaluation Despite the simplicity of the abstract criteria outlined in the previous section, fairness can manifest in many different forms following these principles (see [356, 65] for comprehensive surveys, and [228] for AI ethics’ checklists). We categorize metrics of fairness according to the properties of models and tasks to help the reader choose appropriate ones for their application:
Discrete vs. continuous variables. The task output, model prediction, and sensitive variables can all be discrete (e.g., classification and nationality), ranked (e.g., search engines, recommendation systems), or continuous (e.g., regression, classifier scores, age, etc.) in nature. An empirical correlation of discrete variables can be evaluated with standard statistical tools, such as correlation coefficients (Pearson/Kendall/Spearman) and ANOVA), while continuous variables often further require binning, quantization, or loss functions to evaluate fairness [65].
Loss function. The criteria of fairness often cannot be exactly satisfied given the limitations of empirical data (e.g., demographic parity between groups when hiring only three candidates). Loss functions are useful in this case to gauge how far we are from empirical fairness. The choice of loss function can be informed by the nature of the variables of concern: If the variables represent probabilities, likelihood ratios are more meaningful (e.g., disparate impact [117]); for real-valued regression, the difference between mean distances to the true value aggregated over each group might be used instead to indicate whether we model one group significantly better than another [58].
Multiple sensitive variables. In many applications, the desirable AI system should be fair to more than one sensitive variables (e.g., the prediction of risk posed by a loan should be fair in terms of both gender and ethnicity; inter alia, a recommendation system should ideally be fair to both users and recommendees). One can either form a trade-off of “marginal fairness” between these variables

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

11

when they are considered one at a time, i.e., evaluate the fairness of each variable separately and combine the loss functions for final evaluation, or explore the full Cartesian product [307] of all variables to achieve joint fairness, which typically requires more empirical observations but tends to satisfy stronger ethical requirements.
2.6 Privacy protection Privacy protection mainly refers to protecting against unauthorized use of the data that can directly or indirectly identify a person or household. These data cover a wide range of information, including name, age, gender, face image, fingerprints, etc. Commitment to privacy protection is regarded as an important factor determining the trustworthiness of an AI system. The recently released AI ethics guidelines also highlight privacy as one of the key concerns [178, 9]. Government agencies are formulating a growing number of policies to regulate the privacy of data. The General Data Protection Regulation (GDPR) is a representative legal framework, which pushes enterprises to take effective measures for user privacy protection.
In addition to internal privacy protection within an enterprise, recent developments in data exchange across AI stakeholders has yielded new scenarios and challenges for privacy protection. For example, when training a medical AI model, each healthcare institution typically only has data from the local residents, which might be insufficient. This leads to the demand to collaborate with other institutions and jointly train a model [299] without leaking private information across institutions.
Existing protection techniques penetrate the entire lifecycle of AI systems to address rising concerns about privacy. In Section 3.2.5 we briefly review techniques for protecting the privacy of data used in data collection and processing (data anonymization and differential privacy in Section 3.1.2), model training (secure multi-party computing, and federated learning, in Section 3.2.5), and model deployment (hardware security in Section 3.4.4). The realization of privacy protection is also closely related to other aspects of trustworthy AI. For example, the transparency principle is widely used in AI systems. It informs users of personal data collection and enables privacy settings. In the development of privacy-preserving ML software, such as federated learning (e.g., FATE and PySyft), open source is a common practice to increase transparency and certify the protectiveness of the system.
Evaluation Laws for data privacy protection like the GDPR require data protection impact assessment (DPIA) if any data processing poses a risk to data privacy. Measures have to be taken to address the risk-related concerns and demonstrate compliance with the law [8]. Data privacy protection professionals and other stakeholders need to be involved in the assessment to evaluate it.
Previous research has devised various mathematical methods to formally verify the protectiveness of privacy-preserving approaches. Typical verification can be conducted under assumptions such as semi-honest security, which implies all participating parties follow a protocol to perform the computational task but may try to infer the data of other parties from the intermediate results of computation (e.g., [215]). A stricter assumption is the malicious attack assumption, where each participating party need not follow the given protocol, and can take any possible measure to infer the data [214].
In practical scenarios, the empirical evaluation of the risk of leakage of privacy is usually considered [360, 283]. For example, [283] showed that 15 demographic attributes were sufficient to render 99% of the participants unique. An assessment of such data re-identification intuitively reflects protectiveness when designing a data collection plan.

, Vol. 1, No. 1, Article . Publication date: May 2022.

12

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

Academia

Academic Research

User

Requirement Analysis & Problem Modeling
Product Designer

Data Management
Data Scientist, Data Engineer

Algorithm Design
Algorithm Scientist, Algorithm Engineer

Development
Software Engineer

Deployment
Software Engineer, Business Operator

Industry

Government

Management
Business Operator
Authority Governance

Product Delivery
Business Operator

User

Fig. 2. The AI industry holds a connecting position to organize multi-disciplinary practitioners, including users, academia, and government in the establishment of trustworthy AI. In Section 3, we discuss the current approaches towards trustworthy AI in the five main stages of the lifecycle of an AI system, i.e., data preparation, algorithm design, development, deployment, and management.

2.7 Accountability: A Holistic Evaluation throughout the Above Requirements We have described a series of requirements to build trustworthy AI. Accountability addresses the regulation on AI systems to follow these requirements. With gradually improving legal and institutional norms on AI governance, accountability becomes a crucial factor for AI to sustainably benefit society with trustworthiness [100].
Accountability runs through the entire lifecycle of an AI system, and requires that the stakeholders of an AI system be obligated to justify their design, implementation, and operation as aligned with human values. At the level of executive, this justification is realized by considerate product design, reliable technique architecture, a responsible assessment of the potential impacts, and the disclosure of information on these aspects [209]. Note that in terms of information disclosure, transparency contributes the basic mechanism used to facilitate the accountability of an AI system [100, 94].
From accountability is also derived the concept of auditability, which requires the justification of a system to be reviewed, assessed, and audited [209]. Algorithmic auditing is a recognized approach to ensure the accountability of an AI system and assess its impact on multiple dimensions of human values [273]. See also Section 3.5.2.
Evaluation Checklist-based assessments have been studied to qualitatively evaluate accountability and auditability [8, 315]. As mentioned in this section, we consider accountability to be the comprehensive justification of each concrete requirement of trustworthy AI. Its realization is composed of evaluations of these requirements over the lifecycle of an AI system [273]. Hence, the evaluation of accountability is reflected by the extent to which these requirements of trustworthiness and their impact can be evaluated.
3 Trustworthy AI: A Systematic Approach
We have introduced the concepts relevant to trustworthy AI in Section 2. In the last decade, diverse AI stakeholders have made efforts to enhance AI trustworthiness. In Section A in our appendix, we briefly review their recent practices in multi-disciplinary areas, including research, engineering, and regulation, and studies that are exemplars of industrial applications, including on face recognition, autonomous driving, and NLP. These practices have made important progress in improving AI trustworthiness. However, we find that this work remains insufficient from the
, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

13

industrial perspective. As depicted in Section 1 and Figure 2, the AI industry holds a position to connect multi-disciplinary fields for the establishment of trustworthy AI. This position requires that industrial stakeholders learn and organize these multi-disciplinary approaches and ensure trustworthiness throughout the lifecycle of AI.
In this section, we provide a brief survey of techniques used for building trustworthy AI products and organize them across the lifecycle of product development from an industrial perspective. As shown by the solid-lined boxes in Figure 2, the lifecycle of the development of a typical AI product can be partitioned into data preparation, algorithm design, development–deployment, and management [25]. We review several critical algorithms, guidelines, and government regulations that are closely relevant to the trustworthiness of AI products in each stage of their lifecycle, with the aim of providing a systematic approach and an easy-to-follow guide to practitioners from varying backgrounds to establish trustworthy AI. The approaches and literature mentioned in this section are summarized in Figure 3 and Table 1.
3.1 Data Preparation Current AI technology is largely data driven. The appropriate management and exploitation of data not only improves the performance of an AI system but also affects its trustworthiness. In this section, we consider two major aspects of data preparation, i.e., data collection and data preprocessing. We also discuss the corresponding requirements of trustworthy AI.
3.1.1 Data Collection Data collection is a fundamental stage of the lifecycle of AI systems. An elaborately designed data collection strategy can conduce to the enhancement of AI trustworthiness, such as in terms of fairness and explainability.
Bias mitigation. Training and evaluation data are recognized as a common source of bias for AI systems. Many types of bias might exist and plague fairness in data collection, requiring different processes and techniques to combat it (see [242] for a comprehensive survey). Bias mitigation techniques during data collection can be divided into two broad categories: debias sampling and debias annotation. The former concerns the identification of data points to use or annotate, while the latter focuses on choosing the appropriate annotators.
When sampling data points to annotate, we note that a dataset reflecting the user population does not guarantee fairness because statistical methods and metrics might favor majority groups. This bias can be further amplified if the majority group is more homogeneous for the task (e.g., recognizing speech in less-spoken accents can be naturally harder due to data scarcity [191]). System developers should therefore take task difficulty into consideration when developing and evaluating fair AI systems. On the other hand, choosing the appropriate annotators is especially crucial for underrepresented data (e.g., when annotating speech recognition data, most humans are also poor at recognizing rarely heard accents). Therefore, care must be taken in selecting the correct experts, especially when annotating data from underrepresented groups, to prevent human bias from creeping into the annotated data.
Explanation collection. Aside from model design and development, data collection is also integral to building explainable AI systems. As will be mentioned in Section 3.2.3, adding an explanation task to the AI model can help explain the intermediate features of the model. This strategy is used in tasks like NLP-based reading comprehension by generating supporting sentences [332, 366]. To train the explanation task, it is helpful to consider collecting explanations or information that may not directly be part of the end task, either directly from annotators [355] or with the help of automated approaches [184].
Data Provenance. Data provenance requires recording the data lineage, including the source, dependencies, contexts, and steps of processing [306]. By tracking the data lineage at the highest resolution, data provenance can enhance the transparency, reproducibility, and accountability

, Vol. 1, No. 1, Article . Publication date: May 2022.

14

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

Lifecycle Data Preparation
Algorithm Design
Development
Deployment Management Workflow

Approaches
Data Collection
Data Preprocessing
Adversarial Robustness Explainability ML Model Generalization Algorithmic Fairness
Privacy Computing
Functional Testing Performance Benchmarking Simulation Formal Verification Anomaly Monitoring Human-AI Interaction Fail-Safe Mechanism Hardware Security Documentation Auditing Cooperation MLOps

Literature Bias Mitigation [242, 65, 339] Data Provenance [154, 172] Anomaly Detection [316, 80, 69, 257] Data Anonymozation [13, 232, 232, 36] Differential Privacy [107, 110, 177] Adversarial Robustness [304, 11, 68, 18, 373, 44, 348] Poisoning defense [213] Explainability ML [23, 140, 46, 101, 250, 101] Model Generalization [124, 133, 377, 22, 91, 183, 37] Domain Generalization [391, 343] Fairness and bias mitigation [242, 65, 339] SMPC [387, 114] Federated Learning [365, 180] [164, 381, 235]
[87, 153, 55, 127, 328, 39, 123, 307, 238, 93, 248]
[192, 324, 103, 359, 41, 323, 205, 49, 53] [335, 164, 298] [69, 257, 394] [301, 71, 179, 326] [126, 252, 160, 230] [364, 287, 264] [10, 246, 129, 40, 156, 21] [273, 57, 228, 292, 277, 290, 356] [26, 89] [233, 165, 303]

Table 1. Representative papers of approaches or research directions mentioned in Section 3. For research directions that are widely studied, we provide the corresponding surveys for readers to refer to. For approaches or research directions without surveys available, we provide representative technical papers.

of an AI system [154, 172]. Moreover, recent research has shown that data provenance can be used to mitigate data poisoning [32], thus enhancing the robustness and security of an AI system. The technical realization of data provenance has been provided in [154]. Tool chains [293] and documentation [129] guides have also been studied for specific scenarios involving AI systems. To ensure that the provenance is tamper proof, the blockchain has been recently considered as a promising tool to certify data provenance in AI [96, 14]. 3.1.2 Data Preprocessing Before feeding data into an AI model, data preprocessing helps remove inconsistent pollution of the data that might harm model behavior and sensitive information that might compromise user privacy.
Anomaly Detection. Anomaly detection is also known as outlier detection, and has long been an active area of research in ML [316, 80, 69, 257]. Due to the sensitivity of ML models to outlier data, data cleaning by anomaly detection serves as an effective approach to enhance performance. In recent studies, anomaly detection has been shown to be useful in addressing some requirements of AI trustworthiness. For example, fraudulent data can challenge the robustness and security of
, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

15

AI System Lifecycle

Management

Deployment

Development

Algorithms Design

Data Preparation

AI Trustworthiness Aspects

Robustness
Anomaly Detection
Adv Robust Adversarial
Training Adversarial Regularization Poisoning
Defense
Metamorphic Testing
Neural Coverage Testing Robustness Benchmarking
Software Simulation
HIL Simulation
Formal Verification
Attack Monitoring
User Interface
Human Intervention
Fallback Plan
Trusted Exec. Environment

Generalization Explainability
Explanation Collection

Model Gen
Classic Mechanisms
Domain Generalization

Explainable ML
Explainable Model Design
Post-hoc Explaination

Metamorphic Testing

Held-out Acc. Benchmarking
Software Simulation
HIL Simulation
Formal Verification
Data Drift Monitoring
Human Intervention

Explainability Benchmarking
User Interface Human
Intervention

Transparency Reproducibility

Fairness

Data Provenance

Bias Mitigation

Algo Fairness
Pre-processing Methods
In-processing Methods
Post-processing Methods

Software Simulation

Fairness Benchmarking
Software Simulation

User Interface
Human Intervention

Data Drift Monitoring
Human Intervention

Human Intervention

Documentation

Documentation

Privacy Protection
Data Anonymization
Differential Privacy
Privacy Comp Secure MPC
Federated Learning
Misuse Monitoring
Human Intervention

Auditing

Auditing

Auditing

Auditing

Auditing

Auditing

Auditing

Collaborative R&D
Co-op Dev of Regulation

Collaborative R&D
Co-op Dev of Regulation

Collaborative R&D
Co-op Dev of Regulation

Incidents Sharing
Collaborative R&D
Co-op Dev of Regulation

Incidents Sharing
Collaborative R&D
Co-op Dev of Regulation

Collaborative R&D
Co-op Dev of Regulation

Trustw Data Exchange
Collaborative R&D
Co-op Dev of Regulation

Value Alignment

Accountability

Data Provenance

Data Collection

Data Preprocessing

Functional Testing
Benchmarking

Simulation

Misuse Monitoring User Interface
Human Intervention
Auditing
Incidents Sharing Collaborative
R&D Co-op Dev of
Regulation

Formal Verification Anomaly Monitoring

Human Intervention

Human-AI Interaction

Fail-safe Mechanism

Hardware Security

Documentation

Documentation

Auditing

Auditing

Incidents Sharing
Collaborative R&D
Co-op Dev of Regulation

Cooperation & Information Sharing

Fig. 3. A look-up table that organizes surveyed approaches to AI trustworthiness from different perspectives

and in different stages of the lifecycle of the AI system. Some approaches can be used to improve AI

trustworthiness from multiple aspects, and are in multiple columns. We dim these duplicated blocks here

through stride-filling for better visualization. See the corresponding paragraphs in Section 3 for the details of

the approaches.

, Vol. 1, No. 1, Article . Publication date: May 2022.

16

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

systems in areas such as banking and insurance. Various approaches have been proposed to address this issue by using anomaly detection [69]. The detection and mitigation of adversarial inputs is also considered to be a means to defend against evasion attacks and data poisoning attacks [304, 213, 11]. It is noteworthy that the effectiveness of detection in high dimensions (e.g., images) is still limited [62]. The mitigation of adversarial attacks is also referred to as data sanitization [70, 258, 86].
Data Anonymization (DA). DA modifies the data so that the protected private information cannot be recovered. Different principles of quantitative data anonymization have been developed, such as 𝑘-anonymity [288], (𝑐, 𝑘)-safety [236], and 𝛿-presence [253]. Data format-specific DA approaches have been studied for decades [372, 386, 171]. For example, private information in the form of graph data for social networks can be potentially contained in properties of the vertices of the graph, its link relationships, weights, or other graph metrics [389]. Ways of anonymizing such data have been considered in the literature [220, 36]. Specific DA methods have also been designed for relational data [262], set-valued data [320, 151], and image data [239, 97]. Guidelines and standards have been formulated for data anonymization, such as the US HIPAA and the UK ISB1523. Data pseudonymization [251] is also a relevant technique promoted by the GDPR. It replaces private information with non-identifying references.
Desirable data anonymization is expected to be immune from data de-anonymization or reidentification attacks that try to recover private information from anonymized data [111, 176]. For example, [174] introduces several approaches into de-anonymize user information from graph data. To mitigate the risk of privacy leakage, an open-source platform was provided in [175] to evaluate the privacy protection-related performance of graph data anonymization algorithms against de-anonymization attacks.
Differential privacy (DP). DP shares information of groups within datasets while withholding individual samples [108, 110, 109]. Typical DP can be formally defined by 𝜖-differential privacy. It measures the extent to which a (randomized) statistical function on the dataset reflects whether an element has been removed [108]. DP has been explored in various data publishing tasks, such as log data [159, 385], set-valued data [75], correlated network data [74], and crowdsourced data [344, 278]. It has also been applied to single- and multi-machine computing environments, and integrated with ML models for protecting model privacy [120, 1, 349]. Enterprises like Apple have used DP to transform user data into a form from which the true data cannot be reproduced [20]. In [113], researchers proposed the RAPPOR algorithm that satisfies the definition of DP. The algorithm is used for crowdsourcing statistical analyses of user software. DP is also used to improve the robustness of AI models against adversarial samples [204].
3.2 Algorithm Design A number of aspects of trustworthy AI have been addressed as algorithmic problems in the context of AI research, and have attracted widespread interest. We organize recent technical approaches by their corresponding aspects of AI trustworthiness, including robustness, explainability, fairness, generalization, and privacy protection, to provide a quick reference for practitioners.
3.2.1 Adversarial Robustness The robustness of an AI model is significantly affected by the training data and the algorithms used. We describe several representative directions in this section. Comprehensive surveys can be found in the literature such as [304, 11, 68, 18, 213, 373, 44].
Adversarial training Since the discovery of adversarial attacks, it has been recognized that augmenting the training data with adversarial samples provides an intuitive approach for defense against them. This is typically referred to as adversarial training [134, 348, 211]. The augmentation can be carried out in a brute-force manner by feeding both the original data and adversarial samples during training [201], or by using a regularization term to implicitly represent adversarial

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

17

samples [134]. Conventional adversarial training augments data with respect to specific attacks. It can defend against the corresponding attack but is vulnerable to other kinds of attacks. Various improvements have been studied to improve this defense [44, 229, 304]. [327] augmented the training data with adversarial perturbations transferred from other models. It is shown to further provide defense against black-box attacks which do not require knowledge of the model parameters This can help defend against black-box attacks, which do not require knowledge of the model parameters. [231] combined multiple types of perturbations into adversarial training to improve the robustness of the model against multiple types of attacks.
Adversarial regularization In addition to the regularization term that implicitly represents adversarial samples, recent research has further explored network structures or regularization to overcome the vulnerabilities of the DNN to adversarial attacks. An intuitive motivation for this regularization is to prevent the outcome of the network from changing dramatically in case of small perturbations. For example, [139] penalized large partial derivatives of each layer to improve the stability of its output. A similar regularization on gradients was adopted by [286]. Parseval networks [81] train the network by imposing a regularization on the Lipschitz constant in each layer.
Certified robustness Adversarial training and regularization improve the robustness of AI models in practice but cannot theoretically guarantee that the models work reliably. This problem has prompted research to formally verify the robustness of models (a.k.a. certified robustness). Recent research on certified robustness has focused on robust training to deal with perturbations. For example, CNN-Cert [50], CROWN [379], Fast-lin, and Fast-lip [352] aim to minimize an upper bound of the worst-case loss under given input perturbations. [152] instead derives a lower bound on the norm of the input manipulation required to change the decision of the classifier, and uses it as a regularization term for robust training. To address the issue that the exact computation of such bounds is intractable for large networks, various relaxations or approximations, such as [380, 352], have been proposed as alternatives to regularization. Note that the above research mainly optimizes robustness only locally near the given training data. To achieve certified robustness on unseen inputs as well, global robustness has recently attracted the interest of the AI community [76, 206].
It is also worth noting the recent trend of the intersection of certified robustness and the perspective of formal verification, which aims at developing rigorous mathematical specification and techniques of verification for assurances of software correctness [82]. A recent survey by [335] provided a thorough review of the formal verification of neural networks.
Poisoning defense Typical poisoning or backdoor attacks contaminate the training data to mislead model behavior. Besides avoiding suspicious data in the data sanitization stage, defensive algorithms against poisoning data are an active area of recent ML research [213]. The defense has been studied at different stages of a DNN model. For example, based on the observation that backdoor-related neurons are usually inactivated for benign samples, [219] proposed pruning these neurons from a network to remove the hidden backdoor. Neural Cleanse [342] proactively discovers backdoor patterns in a model. The backdoor can then be avoided by the early detection of backdoor patterns from the data or retraining the model to mitigate the backdoor. The detection of backdoor attacks can also be carried out by analyzing the prediction on the model on specifically designed benchmarking inputs [194].
3.2.2 Model Generalization Techniques of model generalization not only aim to improve model performance, but also explore training AI models with limited data and at limited cost. We review representative approaches to model generalization, categorized as classic generalization and domain generalization.

, Vol. 1, No. 1, Article . Publication date: May 2022.

18

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

Classic generalization mechanisms As a fundamental principle of model generalization theory, the bias–variance trade-off indicates that a generalized model should maintain a balance between underfitting and overfitting [38, 124]. For an overfitted model, reducing complexity/capacity might lead to better generalization. Consider the neural network as an example. Adding a bottleneck layer, which has fewer neurons than the layers both below and above, to it can help reduce model complexity and reduce overfitting.
Other than adjusting the architecture of the model, one can mitigate overfitting to obtain better generalization via various explicit or implicit regularizations, such as early stopping [370], batch normalization [167], dropout [309], data augmentation, and weight decay [196]. These regularizations are standard techniques to improve model generalization when the training data are much smaller in size than the number of model parameters [337]. They aim to push learning to a sub-space of a hypothesis with manageable complexity and reduce model complexity [377]. However, [377] also observed that explicit regularization may improve generalization performance but is insufficient to reduce generalization error. The generalization of the deep neural network is thus still an open problem.
Domain generalization A challenge for modern deep neural networks is their generalization of out-of-distribution data. This challenge arises from various practical AI tasks [391, 343] in the area of transfer learning [255, 350]. Domain adaptation [391, 343] aims to find domain-invariant features such that an algorithm can achieve similar performances across domains. As another example, the goal of few-shots learning is to generalize model to new tasks using only a few examples [346, 77, 371]. Meta-learning [336] attempts to learn prior knowledge of generalization from a large number of similar tasks. Feature similarity [190, 308] has been used as a representative type of knowledge prior in works such as the MAML [119], reinforcement learning [212], and memory-augmented NN [291].
Model pre-training is a popular mechanism to leverage knowledge learned from other domains, and has recently achieved growing success in both academia and the industry. For example, in computer vision, an established successful paradigm involves pre-training models on a largescale dataset, such as ImageNet, and then fine-tuning them on target tasks with fewer training data [131, 375, 224]. This is because pre-trained feature representation can be used to transfer information to the target tasks [375]. Unsupervised pre-training has recently been very successful in language processing (e.g., BERT [92] and GPT [269]) and computer vision tasks (e.g., MoCo [150] and SeCo [368]). In addition, self-supervised learning provides a good mechanism to learn a crossmodal feature representation. These include the vision and language models VL-BERT [313] and Auto-CapTIONs [256]. To explain the effectiveness of unsupervised pre-training, [112] conducted a series of experiments to illustrate that it can drive learning to the basins of minima that yield better generalization.
3.2.3 Explainable ML In this section, we review representative approaches for the two aspects of ML explainability mentioned in Section 2.3.1 and their application to different tasks.
Explainable ML model design Although they have been recognized as being disadvantageous in terms of performance, explainable models have been actively researched in recent years, and a variety of fully or partially explainable ML models have been studied to push their performance limit.
Self-explainable ML models. A number of self-explainable models have been studied in ML over the years. The representative ones include the k-nearest neighbors (KNN), linear/logistic regression, decision trees/rules, and probabilistic graphical models [23, 140, 46, 250]. Note that the self-explainability of these models is sometimes undermined by their complexity. For example, very complex tree or rule structures might sometimes be considered incomprehensible or unexplainable.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

19

Some learning paradigms other than conventional models are also considered to be explainable, such as causal inference [197, 259] and the knowledge graph [345]. These methods are also expected to provide valuable inspiration to solve the problem of explainability for ML.
Beyond self-explainable ML models. Compared with black-box models, such as the DNN, conventional self-explainable models have poor performance on complex tasks, such as image classification and text comprehension. To achieve a compromise between explainability and performance, hybrid combinations of self-explainable models and black-box models have been proposed. A typical design involves embedding an explainable bottleneck model into a DNN. For example, previous research has embedded linear models and prototype selection to the DNN [15, 72, 19]. In the well-known class activation mapping (CAM) [390], an average pooling layer at the end of a DNN can also be regarded as an explainable linear bottleneck. Attention mechanisms [29, 363] have also attracted recent interest, and have been regarded as an explainable bottleneck in a DNN in some studies [78, 237]. However, this claim continues to be debated because attention weights representing different explanations can produce similar final predictions [170, 354].
Post-hoc model explanation In addition to designing self-explainable models, understanding how a specific decision is made by black-box models is also an important problem. A major part of research on this problem has addressed the methodology of post-hoc model explanation and proposed various approaches.
Explainer approximation aims to mimic the behavior of a given model with explainable models. This is also referred to as the global explanation of a model. Various approaches have been proposed to approximate ML models, such as random forests [392, 317] and neural networks [85, 27, 393]. With the rise of deep learning in the past decade, explainer approximation on the DNN has advanced as the knowledge distillation problem on explainers such as trees [384, 125].
Feature importance has been a continually active area of research on explainability. A representative aspect uses local linear approximation to model the contribution of each feature to the prediction. LIME [279] and SHAP [225] are influential approaches that can be used for predictions on tabular data, computer vision, and NLP. Gradients can reflect how features contribute to the predicted outcome, and have drawn great interest in work on the explainability of the DNN [305, 297]. In NLP or CV, gradients or their variants are used to back-trace the decision of the model to the location of the most intimately related input, in the form of saliency maps and sentence highlights [314, 302, 375, 250].
Feature introspection aims to provide a semantic explanation of intermediate features. A representative aspect attaches an extra branch to a model to generate an explanatory outcome that is interpretable by a human. For example, in NLP-based reading comprehension, the generation of a supporting sentence serves as an explanatory task in addition to answer generation [332, 366]. In image recognition, part-template masks can be used to regularize feature maps to focus on local semantic parts [383]. Concept attribution [46] is another aspect that maps a given feature space to human-defined concepts. Similar ideas have been used in generative networks to gain control over attributes, such as the gender, age, and ethnicity, in a face generator [221].
Example-based explanation explains outcomes of the AI model by using the sample data. For example, an influential function was borrowed from robust statistics in [193] to find the most influential data instance for a given outcome. Counterfactual explanation [341, 184, 12] works in a contrary way by finding the boundary case to flip the outcome. This helps users better understand the decision surface of the model.
3.2.4 Algorithmic Fairness Methods to reduce bias in AI models during algorithm development can intervene before the data are fed into the model (pre-processing), when the model is being trained (in-processing), or into model predictions after it has been trained (post-processing).

, Vol. 1, No. 1, Article . Publication date: May 2022.

20

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

Pre-processing methods Aside from debiasing the data collection process, we can debias data before model training. Common approaches include the following:
Adjusting sample importance. This is helpful especially if debiasing the data collection is not sufficient or no longer possible. Common approaches include resampling [5], which involves selecting a subset of the data, reweighting [59], which involves assigning different importance values to data examples, and adversarial learning [229], which can be achieved through resampling or reweighting with the help of a trained model to find the offending cases.
Aside from helping to balance the classification accuracy, these approaches can be applied to balance the cost of classification errors to improve performance on certain groups [163] (e.g., for the screening of highly contagious and severe diseases, false negatives can be costlier than false positives; see cost-sensitive learning [321]).
Adjusting feature importance. Inadvertent correlation between features and sensitive variables can lead to unfairness. Common approaches of debiasing include representation transformation [60], which can help adjust the relative importance of features, and blinding [73], which omits features that are directly related to the sensitive variables.
Data augmentation. Besides making direct use of the existing data samples, it is possible to introduce additional samples that typically involves making changes to the available samples, including through perturbation and relabeling [59, 84].
In-processing methods Pre-processing techniques are not guaranteed to have the desired effect during model training because different models can leverage features and examples in different ways. This is where in-processing techniques can be helpful:
Adjusting sample importance. Similar to pre-processing methods, reweighting [195] and adversarial learning [67] can be used for in-processing, with the potential of either making use of the model parameters or predictions that are not yet fully optimized to more directly debias the model.
Optimization-related techniques. Alternatively, model fairness can be enforced more directly via optimization techniques. For instance, quantitative fairness metrics can be used as regularization [6] or constraints for the optimization of the model parameters [66].
Post-processing methods Even if all precautions have been taken with regard to data curation and model training, the resulting models might still exhibit unforeseen biases. Post-processing techniques can be applied for debiasing, often with the help of auxiliary models or hyperparameters to adjust the model output. For instance, optimization techniques (e.g., constraint optimization) can be applied to train a smaller model to transform model outputs or calibrate model confidence [186]. Reweighting the predictions of multiple models can also help reduce bias [168].
3.2.5 Privacy Computing Apart from privacy-preserving data processing methods, which were introduced in Section 3.1.2, another line of methods preserve data privacy during model learning. In this part, we briefly review the two popular categories of such algorithms: secure multi-party computing, and federated learning.
Secure Multi-party Computing (SMPC) deals with the task whereby multiple data owners compute a function, with the privacy of the data protected and no trusted third party serving as coordinator. A typical SMPC protocol satisfies properties of privacy, correctness, independence of inputs, guaranteed output delivery, and fairness [387, 114]. The garbled circuit is a representative paradigm for secure two-party computation [367, 244]. Oblivious transfer is among the key techniques. It guarantees that the sender does not know what information the receiver obtains from the transferred message. For the multi-party condition, secret sharing is one of the generic frameworks [181]. Each data instance is treated as a secret and split into several shares. These

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

21

shares are then distributed to the multiple participating parties. The computation of the function value is decomposed into basic operations that are computed following the given protocol.
The use of SMPC in ML tasks has been studied in the context of both model-specific learning tasks, e.g., linear regression [128] and logistic regression [300], and generic model learning tasks [247]. Secure inference is the an emerging topic that tailors the SMPC for ML use. Its application to ML is as a service in which the server holds the model and clients hold the private data. To reduce the high costs of computation and communication of the SMPC, parameter quantization and function approximation were used together with cryptographic protocols in [31, 7]. Several tools have been open-sourced, such as MP2ML [47], CryptoSPN [330], CrypTFlow [200, 276], and CrypTen [188].
Federated learning (FL) was initially proposed as a secure scheme to collaboratively train an ML model on a data of user interactions with their devices [241]. It quickly gained extensive interest in academia and the industry as a solution to collaborative model training tasks by using data from multiple parties. It aims to address data privacy concerns that hinder ML algorithms from properly using multiple data sources. It has been applied to numerous domains, such as healthcare [282, 299] and finance [223].
Existing FL algorithms can be categorized into horizontal FL, vertical FL, and federated transfer learning algorithms [365]. Horizontal FL refers to the scenario in which each party has different samples but the samples share the same feature space. A training step is decomposed as to first compute optimization updates on each client and then aggregate them on a centralized server without knowing the clients’ private data [241]. Vertical FL refers to the setting in which all parties share the same sample ID space but have different features. [148] used homomorphic encryption for vertical logistic regression-based model learning. In [138], an efficient method of kernel learning was proposed. Federated transfer learning is applicable to the condition in which none of the parties overlaps in either the sample or the feature space [222]. The connection between FL and other research topics, such as multi-task learning, meta-learning, and fairness learning, has been discussed in [180]. To expedite FL-related research and development, many open-source libraries have been released, such as FATE, FedML [149], and Fedlearn-Algo [217].
3.3 Development The manufacture of reliable products requires considerable effort in software engineering, and this is sometimes overlooked by AI developers. This lack of diligence, such as insufficient testing and monitoring, may incur long-term costs in the subsequent lifecycle of AI products (a.k.a. technical debt [296]). Software engineering in the stages of development and deployment has recently aroused wide concern as an essential condition for reliable AI systems [17, 203]. Moreover, various techniques researched for this stage can contribute to the trustworthiness of an AI system [17]. In this section, we survey the representative techniques.
3.3.1 Functional Testing Inherited from the workflow of canonical software engineering, the testing methodology has drawn growing attention in the development of an AI system. In terms of AI trustworthiness, testing serves as an effective approach to certify that the system is fulfilling specific requirements. Recent research has explored new approaches to adapt functional testing to AI systems. This has been reviewed in the literature, such as [164, 381, 235]. We describe two aspects of adaption from the literature that are useful to enhance the trustworthiness of an AI system.
Test criteria. Different from canonical software engineering where exact equity is tested between the actual and the expected outputs of a system, an AI system is usually tested by its predictive accuracy on a specific testing dataset. Beyond accuracy, various test criteria have been studied to further reflect and test more complex properties of an AI system. The concept of test coverage in software testing has been transplanted into highly entangled DNN models [260, 226]. The name

, Vol. 1, No. 1, Article . Publication date: May 2022.

22

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

of a representative metric—neuron coverage [260]—figuratively illustrates that it measures the coverage of activated neurons in a DNN in analogy to code branches in canonical software testing. Such coverage criteria are effective for certifying the robustness of a DNN against adversarial attacks [226].
Test case generation. Human-annotated datasets are insufficient for thoroughly testing an AI system, and large-scale automatically generated test cases are widely used. Similar to canonical software testing, the problem to automatically generate the expected ground truth, known as the oracle problem [33], also occurs in the AI software testing scenario. The hand-crafted test case template is an intuitive but effective approach in applications of NLP [281]. Metamorphic testing is also a practical approach that converts the input/output pairs into new test cases. For example, [382] transfers images of road scenes taken in daylight to rainy images by using GAN as new test cases, and re-uses the original, invariant annotation to test an autonomous driving systems. These testing cases are useful for evaluating the generalization performance of an AI model. A similar methodology was adopted by adding adversarial patterns to normal images to test adversarial robustness [226]. Simulated environments are also widely used to test applications such as computer vision and reinforcement learning. We further review this topic in Section 3.3.3.
3.3.2 Performance Benchmarking Unlike conventional software, the functionality of AI systems is often not easily captured in functional test alone. To ensure that systems are trustworthy in terms of different aspects of interest, benchmarking (a.k.a. performance testing in software engineering) is often applied to ensure system performance and stability when these characteristics can be automatically measured.
Robustness is an important aspect of trustworthiness that is relatively amenable to automatic evaluation. [87] and [153] introduced a suite of black-box and white-box attacks to automatically evaluate the robustness of AI systems. It can potentially be performed as a sanity check before such systems are deployed to affect millions of users. Software fairness has also been a concern since conventional software testing [55, 127]. Criteria for AI systems have been studied to spot issues of unfairness by investigating the correlation among sensitive attributes, system outcomes, and the true label when applicable to well-designed diagnostic datasets [328]. Well-curated datasets and metrics have been proposed in the literature to evaluate performance on fairness metrics that are of interest for different tasks [39, 123, 307].
More recently, there has been growing interest in benchmarking explainability when models output explanations in NLP applications. For instance, [238] asks crowd workers to annotate salient pieces of text that lead to their belief that the text is hateful or offensive, and examines how well model-predicted importance fits human annotations. [93] instead introduces partial perturbations to the text for human annotators, and observes if the system’s explanations match perturbations that change human decisions. In the meantime, [267] reported that explainability benchmarking remains relatively difficult because visual stimuli are higher dimensional and continuous.
3.3.3 Development by Simulation While benchmarks serve to evaluate AI systems in terms of predictive behavior given static data, the behavior of many systems is deeply rooted in their interactions with the world. For example, benchmarking autonomous vehicle systems on static scenarios is insufficient to help us evaluate their performance on dynamic roadways. For these systems, simulation often plays an important role in ensuring their trustworthiness before deployment.
Robotics is a sub-fields of AI where simulations are most commonly used. Control systems for robots can be compared and benchmarked in simulated environments such as Gazebo [192], MuJoCo [324], and VerifAI [103]. Similarly, simulators for autonomous driving vehicles have been widely used, including CARLA [102], TORCS [359], CarSim [41], and PRESCAN [323]. These software platforms simulate the environment in which robots and vehicles operate as well as the actuation

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

23

of controls on the simulated robots or cars. In NLP, especially conversational AI, simulators are widely used to simulate user behavior to test system ability and fulfill user needs by engaging in a dialog [205]. These simulators can help automatically ensure the performance of AI systems in an interactive environment and diagnose issues before their deployment.
Despite the efficiency, flexibility, and replicability afforded by software simulators, they often still fall short of perfectly simulating constraints faced by the AI system when deployed as well as environmental properties or variations in them. For AI systems that are deployed on embedded or otherwise boxed hardware, it is important to understand the system behavior when they are run on the hardware used in real-world scenarios. Hardware-in-the-loop simulations can help developers understand system performance when it is run on chips, sensors, and actuators in a simulated environment, and is particularly helpful for latency- and power-critical systems like autonomous driving systems [49, 53]. By taking real-world simulations a step further, one can also construct controlled real-world environments for fully integrated AI systems to roam around in (e.g., test tracks for self-driving cars with road signs and dummy obstacles). This provides more realistic measurements and assurances of performance before releasing such systems to users.
3.4 Deployment After development, AI systems are deployed on realistic products, and interact with the environment and users. To guarantee that the systems are trustworthy, a number of approaches should be considered at the deployment stage, such as adding additional components to monitor anomalies, and developing specific human–AI interaction mechanisms for transparency and explainability.
3.4.1 Anomaly Monitoring Anomaly monitoring has become a well-established methodology in software engineering. In terms of AI systems, the range of monitoring has been further extended to cover data outliers, data drifts, and model performance. As a keying safeguard for the successful operation of an AI system, monitoring provides the means to enhance the system’s trustworthiness in multiple aspects. Some representative examples are discussed below.
Attack monitoring has been widely adopted in conventional SaaS, such as fraud detection [2] in e-commerce systems. In terms of the recent emerging adversarial attacks, detection and monitoring [243] of such attack inputs is also recognized as an important means to ensure system robustness. Data drift monitoring [268] provides important means to maintain the generalization of an AI system under concept change [394] caused by dynamic environment such as market change [289]. Misuse monitoring is recently also adopted in several cloud AI services [173] to avoid improper use such as unauthorized population surveillance or individual tracking by face recognition, which helps ensure the proper alignment of ethical values.
3.4.2 Human–AI Interaction As an extension of human–computer interaction (HCI), human–AI interaction has aroused wide attention in the AI industry [3, 16]. Effective human–AI interaction affects the trustworthiness of an AI system in multiple aspects. We briefly illustrate two topics.
User interface serves as the most intuitive factor affecting user experience. It is a major medium for an AI system to disclose its internal information and decision-making procedure to users, and thus has an important effect on the transparency and explainability of the system [301, 351]. Various approaches of interaction have been studied to enhance the explainability of AI, including the visualization of ML models [71] and interactive parameter-tuning [351]. In addition to transparency and explainability, the accessibility of the interface also significantly affects user experience of trustworthiness. AI-based techniques of interaction have enabled various new forms of human– machine interfaces, such as chatbots, audio speech recognition, and gesture recognition, and might result in accessibility problems for disabled people. Mitigating such unfairness has aroused concerns in recent research [179, 326].

, Vol. 1, No. 1, Article . Publication date: May 2022.

24

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

Human intervention, such as by monitoring failure or participating in decisions [295], has been applied to various AI systems to compensate for limited performance. Advanced Driving Assistance System (ADAS) can be considered a typical example of systems involving human intervention, where the AI does the low-level driving work and the human makes the high-level decision. In addition to compensating for decision-making, human intervention provides informative supervision to train or fine-tune AI systems in many scenarios, such as the shadow mode [319] of autonomous driving vehicles. To minimize and make the best use of human effort in such interaction, efficient design of patterns of human–machine cooperation is an emerging topic in inter-disciplinary work on HCI and AI, and is referred to as human-in-the-loop or interactive machine learning [157] in the literature.
3.4.3 Fail-Safe Mechanisms Considering the imperfection of current AI systems, it is important to avoid harm when the system fails in exceptional cases. By learning from conventional real-time automation systems, the AI community has realized that a fail-safe mechanism or fallback plan should be an essential part of the design of an AI system if its failure can cause harm or loss. This mechanism is also emerging as an important requirement in recent AI guidelines, such as [9]. The fail-safe design has been observed in multiple areas of robotics in the past few years. In the area of UAV, the fail-safe algorithm has been studied for a long time to avoid frequent collision of quadrocopters [126] and to ensure safe landing upon system failure [252]. In autonomous driving where safety is critical, a fail-safe mechanism like standing still has become an indispensable component in ADAS products [160], and is being researched at a higher level of automation [230].
3.4.4 Hardware Security AI systems are widely deployed on various hardware platforms to cope with the diverse scenarios, ranging from servers in computing centers to cellphones and embedded systems. Attacks on OS and hardware lead to new risks, such as data tampering or stealing, and threaten the robustness, security, and privacy of AI systems. Various approaches have been studied to address this new threat [364]. From the perspective of hardware security, the concept of a trusted execution environment (TEE) is a recent representative technique that has been adopted by many hardware manufacturers [287]. The general mechanism of the TEE is to provide a secure area for the data and the code. This area is not interfered with by the standard OS such that the protected program cannot be attacked. ARM processors support TEE implementation using the TrustZone design [264]. They simultaneously run a secure OS and a normal OS on a single core. The secure part provides a safe environment for sensitive information. The Intel Software Guard Extensions (SGX) implement the TEE by hardware-based memory encryption [240]. Its enclave mechanism allows for the allocation of protected memory to hold private information. Such security mechanisms have been used to protect sensitive information like biometric ID and financial account passwords, and are applicable to other AI use cases.
3.5 Management AI practitioners such as researchers and developers have studied various techniques to improve AI trustworthiness in the aforementioned stages of the data, algorithm, development, and deployment stages. Beyond these concrete approaches, appropriate management and governance provide a holistic guarantee that trustworthiness is consistently aligned throughout the lifecycle of a AI system. In this section, we introduce several executable approaches which facilitate the AI community to improve management and governance on AI trustworthiness.
3.5.1 Documentation Conventional software engineering has accumulated a wealth of experience in leveraging documentation to assist in development. Representative documentation types include requirement documents, product design documents, architecture documents, code documents, and test documents [10]. Beyond conventional software engineering, multiple new types of documents have been proposed to adapt to the ML training and testing mechanisms. The scope of these

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

25

documents may include the purposes and characteristics of the model [246], datasets [129, 40, 156], and services [21]. As mentioned in Sections 2.3.2 and 2.7, documentation is an effective and important approach to enhance the system’s transparency and accountability by tracking, guiding, and auditing its entire lifecycle [273], and serves as a cornerstone of building a trustworthy AI system.
3.5.2 Auditing With lessons learned from safety-critical industries, e.g., finance and aerospace, auditing has been recently recognized as an effective mechanism to examine whether an AI system complies with specific principles [57, 356]. In terms of the position of auditors, the auditing process can be categorized as internal or external. Internal auditing enables self-assessment and iterative improvement for manufacturers to follow the principles of trustworthiness. It can cover the lifecycle of the system without the leakage of trade secrets [273]. On the other hand, external auditing by independent parties is more effective in gaining public trust [57].
Auditing might involve the entire or selective parts of the lifecycle of an AI system. A comprehensive framework of internal auditing can be found in [273]. The means of auditing might include interviews, documented artifacts, checklists, code review, testing, and impact assessment. For example, documentation like product requirement documents (PRDs), model cards [246], and datasheets [129] serve as important references to understand the principle alignment during development. Checklists are widely used as a straightforward qualitative approach to evaluate fairness [228], transparency [292], and reproducibility [263]. Quantitative testing also serves as a powerful approach, and has been successfully executed to audit fairness in, for example, the Gender Shade study [57]. Inspired by the EU’s Data Protection Impact Assessment (DPIA), the concept of Algorithmic Impact Assessment (AIA) has been proposed to evaluate the claims of trustworthiness and discover negative effects [277]. Besides the above representatives, designs of approaches to algorithmic auditing can be found in [290, 356].
3.5.3 Cooperation and Information Sharing As shown in Figure 2, the establishment of trustworthy AI requires cooperation between stakeholders. From the perspective of industry, cooperation with academia enables the fast application of new technology to enhance the performance of the product and reduce the risk posed by it. Cooperation with regulators certifies the products as appropriately following the principles of trustworthiness. Moreover, cooperation between industrial enterprises helps address consensus-based problems, such as data exchange, standardization, and ecosystem building [26]. Recent practices of AI stakeholders have shown the efficacy of cooperation in various dimensions. We summarize these practices in the following aspects below.
Collaborative research and development. Collaboration has been a successful driving force in the development of AI technology. To promote research on AI trustworthiness, stakeholders are setting-up various forms of collaboration, such as research workshops on trustworthy AI and cooperative projects like DARPA XAI [144].
Trustworthy data exchange. The increasing business value of data raises the demand to exchange them across companies in various scenarios (e.g., the medical AI system in Section 2.6). Beyond privacy-based computing techniques, the cooperation between data owners, technology providers, and regulators is making progress in establishing an ecosystem of data exchange, and solving problems such as data pricing and data authorization.
Cooperative development of regulation. Active participation in the development of standards and regulations serves as an important means for academia, the industry, and regulators to align their requirements and situations.
Incident sharing. The AI community has recently recognized incident sharing as an effective approach to highlight and prevent potential risks to AI systems [56]. The AI Incident Database [90]

, Vol. 1, No. 1, Article . Publication date: May 2022.

26

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

provides an inspiring example for stakeholders to share negative AI incidents so that the industry can avoid similar problems.
3.6 TrustAIOps: A Continuous Workflow toward Trustworthiness The problem of trustworthy AI arises from the fast development of AI technology and its emerging applications. AI trustworthiness is not a well-studied static bar to reach by some specific solutions. The establishment of trustworthiness is a dynamic procedure. We have witnessed the evolution of different dimensions of trustworthiness over the past decade [178]. For example, research on adversarial attacks has increased concerns regarding adversarial robustness. The applications of safety-critical scenarios have rendered more stringent the requirements of accountability of an AI system. The development of AI research, the evolution of the forms of AI products, and the changing perspectives of society imply the continual reformulation of the requirements of and solutions to trustworthiness. Therefore, we argue that beyond the requirements of an AI product, the AI industry should consider trustworthiness as an ethos of its operational routine and be prepared to continually enhance the trustworthiness of its products.
The constant enhancement of AI trustworthiness positions requirements on a new workflow for the AI industry. Recent studies on industrial AI workflow extend the mechanism of DevOps [35] to MLOps [233], to enable improvements in ML products. The concept of DevOps has been adopted in modern software development to continually deploy software features and improve their quality. MLOps [233] and its variants, such as ModelOps [165] and SafetyOps [303], extend DevOps to cover the ML lifecycle of data preparation, training, validation, and deployment, in its workflow. The workflow of MLOps provides a start point to build the workflow for trustworthy AI. By integrating the ML lifecycle, MLOps connects research, experimentation, and product development to enable the rapid leveraging of the theoretical development of trustworthy AI. A wealth of toolchains of MLOps have been released recently to track AI artifacts such as data, model, and meta-data to increase the accountability and reproducibility of products [165]. Recent research has sought to extend MLOps to further integrate trustworthiness into the AI workflow. For example, [303] extended MLOps with safety engineering as SafetyOps for autonomous driving.
As we have illustrated in this section, building trustworthiness requires the continual and systematic upgrade of the AI lifecycle. By extending MLOps, we summarize this upgrade of practices as a new workflow, TrustAIOps, which focuses on imposing the requirements of trustworthiness over the entire AI lifecycle. This new workflow contains the following properties:
• Close collaboration between inter-disciplinary roles. Building trustworthy AI requires organizing different roles, such as ML researchers, software engineers, safety engineers, and legal experts. Close collaboration mitigates the gap in knowledge between forms of expertise (e.g., [208], c.f., Sections 3.5.3 and A.2).
• Aligned principles of trustworthiness. The risk of untrustworthiness exists in every stage in the lifecycle of an AI system. Mitigating such risks requires that all stakeholders in the AI industry be aware of and aligned with unified trustworthy principles (e.g., [301], c.f., Section A.2).
• Extensive management of artifacts. An industrial AI system is built upon various artifacts such as data, code, models, configuration, product design, and operation manuals. The elaborate management of these artifacts helps assess risk and increases reproducibility and auditability (c.f., Section 3.5.1).
• Continuous feedback loops. Classical continuous integration and continuous development (CI/CD) workflows provide effective mechanisms to improve the software through feedback loops. In a trustworthy AI system, these feedback loops should connect and iteratively

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

27

improve the five stages of its lifecycle, i.e., data, algorithm, development, deployment, and management (e.g., [273, 310]). The evolution of the industrial workflow of AI is a natural reflection of the dynamic procedure to establish its trustworthiness. By systematically organizing stages of the AI lifecycle and inter-disciplinary practitioners, the AI industry is able to understand the requirements of trustworthiness from various perspectives, including technology, law, and society, and deliver continual improvements.
4 Conclusion, Challenges and Opportunities
In this survey, we outlined the key aspects of trustworthiness that we think are essential to AI systems. We introduced how AI systems can be evaluated and assessed on each of these aspects, and reviewed current efforts in this direction in the industry. We further proposed a systematic approach to consider these aspects of trustworthiness in the entire lifecycle of real-world AI systems, which offers recommendations for every step of the development and use of these systems. We recognize that fully adopting this systematic approach to build trustworthy AI systems requires that practitioners embrace the concepts underlying the key aspects that we have identified. More importantly, it requires a shift of focus from performance-driven AI to trust-driven AI. In the short run, this shift will inevitably involve side-effects, such as longer learning time, slowed development, and/or increased cost to build AI systems. However, we encourage practitioners to focus on the long-term benefits of gaining the trust of all stakeholders for the sustained use and development of these systems. In this section, we conclude by discussing some of the open challenges and potential opportunities in the future development of trustworthy AI.
4.1 AI Trustworthiness as Long-Term Research Our understanding of AI trustworthiness is far from complete or universal, and will inevitably evolve as we develop new AI technologies and understand their societal impact more clearly. This procedure requires long-term research in multiple key areas of AI. In this section, we discuss several open questions that we think are crucial to address for the future development of AI trustworthiness. 4.1.1 Immaturity of Approaches to Trustworthiness. As mentioned in Section 2, several aspects of AI trustworthiness, such as explainability and robustness, address the limitation of current AI technologies. Despite wide interest in AI research, satisfactory solutions are still far from reach.
Consider explainability as an example. Despite being an active field of AI research, it remains poorly understood. Both current explanatory models and post-hoc model explanation techniques share a few common issues, e.g., 1) the explanation is fragile to perturbations [130], 2) the explanation is not always consistent with human interpretation [46], and 3) it is difficult to judge if the explanation is correct or faithful [250]. These problems pose important questions in the study of explainability and provide valuable directions of research in theoretical research on AI.
Another example is robustness. The arms race between adversarial attack and defense reflects the immaturity of our understanding of the robustness of AI. As in other areas of security, attacks evolve along with the development of defenses. Conventional adversarial training [134] has been shown to be easily fooled by subsequently developed attacks [327]. The corresponding defense [327] is later shown to be vulnerable against new attacks [99]. This not only requires that practitioners be agile in adopting defensive techniques to mitigate the risk of new attacks in a process of long-term and continual development, but also poses long-term challenges to theoretical research [271]. 4.1.2 Frictional Impact of Trustworthy Aspects. As we have shown in Section 2, there are a wealth of connections and support between different aspects of trustworthiness. On the other hand, research has shown that there are frictions or trade-offs between these aspects in some cases, which we review here.

, Vol. 1, No. 1, Article . Publication date: May 2022.

28

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

Increased transparency improves trust in AI systems through information disclosure. However, disclosing inappropriate information might increase potential risks. For example, excessive transparency on datasets and algorithms might leak private data and commercial intellectual property. Disclosure of detailed algorithmic mechanisms can also lead to the risk of targeted hacking [11]. On the other hand, an inappropriate explanation might also cause users to overly rely on the system and follow wrong decisions of AI [311]. Therefore, the extent of transparency of an AI system should be specified carefully and differently for the roles of public users, operators, and auditors.
From an algorithmic perspective, the effects of different objectives of trustworthiness on model performance remain insufficiently understood. Adversarial robustness increases the model’s generalizability and reduces overfitting, but tends to negatively impact its overall accuracy [331, 378]. A similar loss of accuracy occurs in explainable models [46]. Besides this trust–accuracy trade-off, algorithmic friction exists between the dimensions of trustworthiness. For example, adversarial robustness and fairness can negatively affect each other during training [361, 284]. Furthermore, studies on fairness and explainability have shown several approaches to explanation to be unfair [98].
These frictional effects suggest that AI trustworthiness cannot be achieved through hillclimbing on a set of disjoint criteria. Compatibility should be carefully considered when integrating multiple requirements into a single system. Recent studies [378, 361] provide a good reference to start with.
4.1.3 Limitations in Current Evaluations of Trustworthiness. Repeatable and quantitative measurements are the cornerstone of scientific and engineering progress. However, despite increasing research interest and efforts, the quantification of many aspects of AI trustworthiness remains elusive. Of the various aspects that we have discussed in this paper, the explainability, transparency, and accountability of AI systems are still seldom evaluated quantitatively, which makes it difficult to accurately compare systems. Developing good methods of quantitative evaluation for these desiderata, we believe, will be an important first step in research on these aspects of AI trustworthiness as a scientific endeavor, rather than a purely philosophical one.
4.1.4 Challenges and Opportunities in the Era of Large Scale Pre-trained Models. Large scale pretrained models have brought dramatic breakthroughs for AI. They not only show the potential to a more general form of AI [234], but also bring new challenges and opportunities to the establishment of trustworthy AI. One of the most important properties of a large scale pre-trained model is the ability to transfer its learned knowledge to new tasks in a manner of few-shot or zero-shot learning [54]. This largely fulfills people’s requirement on the generalization of AI, and is recognized to hold great value in commercial applications. On the other hand, the risks of the large scale pre-trained models to be untrustworthy have been revealed by recent studies. For example, the training procedure is known to be costly and difficult to reproduce for third-parties, as mentioned in Section 2.2. Most downstream tasks have to directly adapt the pre-trained model without auditing its entire lifecycle. This business model poses a risk that downstream users might be affected by any biases presented in these models [234]. Privacy leakage is another issue that is recently revealed. Some pre-trained models are reported to output training text data containing private user information such as addresses [63].
The development and application of the large scale pre-trained models is accelerating. To guarantee that this advancement will benefit the society without causing new risks, it is worthwhile for both academia and the industry to carefully study its potential impact in the perspective of AI trustworthiness.
4.2 End-User Awareness of the Importance of AI Trustworthiness Other than developers and providers of AI systems, end-users are an important yet opt-ignored group of stakeholders.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

29

Besides educating the general public about the basic concepts of AI trustworthiness, developers should consider how it can be presented to users to deliver hands-on experiences of trustworthy AI systems. One positive step in this direction involves demonstrating system limitations (e.g., Google Translate displays translations in multiple gendered pronouns when the input text is ungendered) or explainable factors used for system prediction (e.g., recommendation systems can share user traits that are used in curating ads, like “female aged 20–29” and “interested in technology”). Taking this a step further, we believe that it would enable the user to directly control these factors (e.g., user traits) counterfactually, and judge for themselves whether the system is fair, robust, and trustworthy.
Finally, we recognize that not all aspects of trustworthiness can be equally easily conveyed to end-users. For instance, the impact of privacy preservation or transparency cannot be easily demonstrated to the end-user when AI systems are deployed. We believe that media coverage and government regulations regarding these aspects can be very helpful in raising public awareness.
4.3 Inter-disciplinary and International Cooperation An in-depth understanding of AI trustworthiness involves not only the development of better and newer AI technologies, but also requires us to better understand the interactions between AI and human society. We believe this calls for collaboration across various disciplines that reach far beyond computer science. First, AI practitioners should work closely with domain experts whenever AI technologies are deployed to the real world and has impacts on people, e.g., in medicine, finance, transportation, and agriculture. Second, AI practitioners should seek advice from social scientists to better understand the (often unintended) societal impacts of AI and work together to remedy them, e.g., the impact of AI-automated decisions, job displacement in AI-impacted sectors, and the effect of the use of AI systems in social networks. Third, AI practitioners should carefully consider how the technology is presented to the public as well as inter-disciplinary collaborators, and make sure to communicate the known limitations of AI systems honestly and clearly.
In the meantime, the development of trustworthy AI is by no means a unique problem of any single country, nor does the potential positive or negative effect of AI systems respect geopolitical borders. Despite the common portrayal of AI as a race between countries (see Section 3.2 of [145] for a detailed discussion), technological advances in the climate of increased international collaboration are far from a zero-sum game. It not only allows us to build better technological solutions by combining diverse ideas from different backgrounds, but also helps us better serve the world’s population by recognizing our shared humanity as well as our unique differences. We believe that tight-knit inter-disciplinary and international cooperation will serve as the bedrock for rapid and steady developments in trustworthy AI technology, which will in turn benefit humanity at large.
5 Acknowledgments
The authors would like to thank Yanqing Chen, Jing Huang, Shuguang Zhang, and Liping Zhang for their valuable suggestions. We also thank Yu He, Wenhan Xu, Xinyuan Shan, Chenliang Wang, Peng Liu, Jingling Fu, Baicun Zhou, Hongbao Tian and Qili Wang for their assistance with the experiment in the appendix.
References
[1] M. Abadi et al. “Deep learning with differential privacy”. Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016.
[2] A. Abdallah et al. “Fraud detection system: A survey”. Journal of Network and Computer Applications (2016). [3] A. Abdul et al. “Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda”. Proceedings
of the 2018 CHI conference on human factors in computing systems. 2018. [4] A. Adadi et al. “Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)”. IEEE access (2018).

, Vol. 1, No. 1, Article . Publication date: May 2022.

30

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

[5] P. Adler et al. “Auditing black-box models for indirect influence”. Knowledge and Information Systems (2018). [6] S. Aghaei et al. “Learning optimal and fair decision trees for non-discriminative decision-making”. Proceedings of the AAAI Confer-
ence on Artificial Intelligence. 2019. [7] N. Agrawal et al. “QUOTIENT: two-party secure neural network training and prediction”. Proceedings of the 2019 ACM SIGSAC
Conference on Computer and Communications Security. 2019. [8] AI HLEG. Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment. 2020. [9] AI HLEG. Ethics Guidelines for Trustworthy AI. Accessed: 2021-02-20. 2018. [10] A. Aimar. “Introduction to software documentation” (1998). [11] N. Akhtar et al. “Threat of adversarial attacks on deep learning in computer vision: A survey”. Ieee Access (2018). [12] A. R. Akula et al. “CX-ToM: Counterfactual explanations with theory-of-mind for enhancing human trust in image recognition
models”. Iscience (2022). [13] D. Al-Azizy et al. “A literature survey and classifications on data deanonymisation”. International Conference on Risks and Security
of Internet and Systems. Springer. 2015. [14] M. AlShamsi et al. “Artificial intelligence and blockchain for transparency in governance”. Artificial Intelligence for Sustainable
Development: Theory, Practice and Future Applications. Springer, 2021. [15] D. Alvarez Melis et al. “Towards robust interpretability with self-explaining neural networks”. Advances in neural information
processing systems (2018). [16] S. Amershi et al. “Guidelines for human-AI interaction”. Proceedings of the 2019 chi conference on human factors in computing systems.
2019. [17] S. Amershi et al. “Software engineering for machine learning: A case study”. 2019 IEEE/ACM 41st International Conference on Software
Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE. 2019. [18] D. Amodei et al. “Concrete problems in AI safety”. arXiv:1606.06565 (2016). [19] P. Angelov et al. “Towards explainable deep neural networks (xDNN)”. Neural Networks (2020). [20] Apple. Differential Privacy. Accessed: 2021-02-20. 2017. [21] M. Arnold et al. “FactSheets: Increasing trust in AI services through supplier’s declarations of conformity”. IBM Journal of Research
and Development (2019). [22] D. Arpit et al. “A closer look at memorization in deep networks”. International Conference on Machine Learning. PMLR. 2017. [23] A. B. Arrieta et al. “Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible
AI”. Information fusion (2020). [24] V. Arya et al. “One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques”. arXiv:1909.03012 (2019). [25] R. Ashmore et al. “Assuring the machine learning lifecycle: Desiderata, methods, and challenges”. ACM Computing Surveys (CSUR)
(2021). [26] A. Askell et al. “The role of cooperation in responsible AI development”. arXiv:1907.04534 (2019). [27] M. G. Augasta et al. “Reverse engineering the neural networks for rule extraction in classification problems”. Neural processing
letters (2012). [28] S. Ayvaz et al. “Witness of Things: Blockchain-based distributed decision record-keeping system for autonomous vehicles”. Inter-
national Journal of Intelligent Unmanned Systems (2019). [29] D. Bahdanau et al. “Neural machine translation by jointly learning to align and translate” (2015). [30] J. Baker-Brunnbauer. “Trustworthy AI Implementation (TAII) Framework for AI Systems”. Available at SSRN 3796799 (2021). [31] M. Ball et al. “Garbled neural networks are practical”. Cryptology ePrint Archive (2019). [32] N. Baracaldo et al. “Mitigating poisoning attacks on machine learning models: A data provenance based approach”. Proceedings of
the 10th ACM Workshop on Artificial Intelligence and Security. 2017. [33] E. T. Barr et al. “The oracle problem in software testing: A survey”. IEEE transactions on software engineering (2014). [34] P. L. Bartlett et al. “Rademacher and Gaussian complexities: Risk bounds and structural results”. Journal of Machine Learning Research
(2002). [35] L. Bass et al. DevOps: A software architect’s perspective. Addison-Wesley Professional, 2015. [36] G. Beigi et al. “A survey on privacy in social media: Identification, mitigation, and applications”. ACM Transactions on Data Science
(2020). [37] M. Belkin et al. “Reconciling modern machine learning practice and the bias-variance trade-off”. arXiv preprint arXiv:1812.11118
(2018). [38] M. Belkin et al. “Reconciling modern machine-learning practice and the classical bias–variance trade-off”. Proceedings of the National
Academy of Sciences (2019). [39] R. K. Bellamy et al. “AI Fairness 360: An Extensible Toolkit for Detecting”. Understanding, and Mitigating Unwanted Algorithmic
Bias (2018). [40] E. M. Bender et al. “Data statements for natural language processing: Toward mitigating system bias and enabling better science”.
Transactions of the Association for Computational Linguistics (2018). [41] R. F. Benekohal et al. “CARSIM: Car-following model for simulation of traffic in normal and stop-and-go conditions”. Transportation
research record (1988). [42] R. Berk et al. “Fairness in criminal justice risk assessments: The state of the art”. Sociological Methods & Research (2021). [43] M. Bertrand et al. “Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimina-
tion”. American economic review (2004). [44] S. Bhambri et al. “A survey of black-box adversarial attacks on computer vision models”. arXiv:1912.01667 (2019). [45] A. Blanco-Justicia et al. “Machine learning explainability via microaggregation and shallow decision trees”. Knowledge-Based Sys-
tems (2020). [46] F. Bodria et al. “Benchmarking and Survey of Explanation Methods for Black Box Models”. arXiv:2102.13076 (2021). [47] F. Boemer et al. “MP2ML: A mixed-protocol machine learning framework for private inference”. Proceedings of the 15th International
Conference on Availability, Reliability and Security. 2020. [48] M. Bogen et al. HELP WANTED: An Examination of Hiring Algorithms, Equity, and Bias. 2018. [49] T. Bokc et al. “Validation of the vehicle in the loop (vil); a milestone for the simulation of driver assistance systems”. 2007 IEEE
Intelligent vehicles symposium. IEEE. 2007.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

31

[50] A. Boopathy et al. “Cnn-cert: An efficient framework for certifying robustness of convolutional neural networks”. Proceedings of the AAAI Conference on Artificial Intelligence. 2019.
[51] N. E. Boudette. ‘It Happened So Fast’: Inside a Fatal Tesla Autopilot Accident. [52] T. Breuer et al. “How to Measure the Reproducibility of System-oriented IR Experiments”. Proceedings of the 43rd International ACM
SIGIR Conference on Research and Development in Information Retrieval. 2020. [53] C. Brogle et al. “Hardware-in-the-loop autonomous driving simulation without real-time constraints”. IEEE Transactions on Intelli-
gent Vehicles (2019). [54] T. Brown et al. “Language models are few-shot learners”. Advances in neural information processing systems (2020). [55] Y. Brun et al. “Software fairness”. Proceedings of the 2018 26th ACM joint meeting on european software engineering conference and
symposium on the foundations of software engineering. 2018. [56] M. Brundage et al. “Toward trustworthy AI development: mechanisms for supporting verifiable claims”. arXiv:2004.07213 (2020). [57] J. Buolamwini et al. “Gender shades: Intersectional accuracy disparities in commercial gender classification”. Conference on fairness,
accountability and transparency. PMLR. 2018. [58] T. Calders et al. “Controlling attribute effect in linear regression”. 2013 IEEE 13th international conference on data mining. IEEE. 2013. [59] T. Calders et al. “Three naive bayes approaches for discrimination-free classification”. Data Mining and Knowledge Discovery (2010). [60] F. P. Calmon et al. “Optimized pre-processing for discrimination prevention”. Proceedings of the 31st International Conference on
Neural Information Processing Systems. 2017. [61] R. Cammarota et al. “Trustworthy AI Inference Systems: An Industry Research View”. arXiv:2008.04449 (2020). [62] N. Carlini et al. “Adversarial examples are not easily detected: Bypassing ten detection methods”. Proceedings of the 10th ACM
workshop on artificial intelligence and security. 2017. [63] N. Carlini et al. “Extracting training data from large language models”. 30th USENIX Security Symposium (USENIX Security 21). 2021. [64] N. Carlini et al. “Towards evaluating the robustness of neural networks”. 2017 ieee symposium on security and privacy (sp). IEEE.
2017. [65] S. Caton et al. “Fairness in Machine Learning: A Survey”. arXiv:2010.04053 (2020). [66] L. E. Celis et al. “Classification with fairness constraints: A meta-algorithm with provable guarantees”. Proceedings of the conference
on fairness, accountability, and transparency. 2019. [67] L. E. Celis et al. “Improved adversarial learning for fair classification”. arXiv:1901.10443 (2019). [68] A. Chakraborty et al. “Adversarial attacks and defences: A survey”. arXiv:1810.00069 (2018). [69] R. Chalapathy et al. “Deep learning for anomaly detection: A survey”. arXiv:1901.03407 (2019). [70] P. P. Chan et al. “Data sanitization against adversarial label contamination based on data complexity”. International Journal of
Machine Learning and Cybernetics (2018). [71] A. Chatzimparmpas et al. “The state of the art in enhancing trust in machine learning models with the use of visualizations”.
Computer Graphics Forum. Wiley Online Library. 2020. [72] C. Chen et al. “This looks like that: deep learning for interpretable image recognition”. Advances in neural information processing
systems (2019). [73] I. Chen et al. “Why is my classifier discriminatory?” Advances in Neural Information Processing Systems (2018). [74] R. Chen et al. “Correlated network data publication via differential privacy”. The VLDB Journal (2014). [75] R. Chen et al. “Publishing set-valued data via differential privacy”. Proceedings of the VLDB Endowment (2011). [76] Y. Chen et al. “Learning security classifiers with verified global robustness properties”. Proceedings of the 2021 ACM SIGSAC Confer-
ence on Computer and Communications Security. 2021. [77] Y. Cheng et al. “Few-shot learning with meta metric learners”. arXiv:1901.09890 (2019). [78] E. Choi et al. “Retain: An interpretable predictive model for healthcare using reverse time attention mechanism”. Advances in neural
information processing systems (2016). [79] A. Chouldechova. “Fair prediction with disparate impact: A study of bias in recidivism prediction instruments”. Big data (2017). [80] X. Chu et al. “Data cleaning: Overview and emerging challenges”. Proceedings of the 2016 international conference on management
of data. 2016. [81] M. Cisse et al. “Parseval networks: Improving robustness to adversarial examples”. International Conference on Machine Learning.
PMLR. 2017. [82] E. M. Clarke et al. “Formal methods: State of the art and future directions”. ACM Computing Surveys (CSUR) (1996). [83] S. Corbett-Davies et al. “The measure and mismeasure of fairness: A critical review of fair machine learning”. arXiv preprint
arXiv:1808.00023 (2018). [84] B. Cowgill et al. “Algorithmic bias: A counterfactual perspective”. NSF Trustworthy Algorithms (2017). [85] M. Craven et al. “Extracting tree-structured representations of trained networks”. Advances in neural information processing systems
(1995). [86] G. F. Cretu et al. “Casting out demons: Sanitizing training data for anomaly sensors”. 2008 IEEE Symposium on Security and Privacy
(sp 2008). IEEE. 2008. [87] F. Croce et al. “RobustBench: a standardized adversarial robustness benchmark”. Thirty-fifth Conference on Neural Information Pro-
cessing Systems Datasets and Benchmarks Track (Round 2). 2021. [88] L. M. Cysneiros et al. “An Initial Analysis on How Software Transparency and Trust Influence each other.” WER. Citeseer. 2009. [89] A. Dafoe et al. “Open Problems in Cooperative AI”. arXiv:2012.08630 (2020). [90] D. Dao. Awful AI. 2020. [91] “Deep nets don’t learn via memorization” (2017). [92] J. Devlin et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019. [93] J. DeYoung et al. “ERASER: A Benchmark to Evaluate Rationalized NLP Models”. Transactions of the Association for Computational Linguistics (2020). [94] N. Diakopoulos. “Accountability in algorithmic decision making”. Communications of the ACM (2016). [95] W. Dieterich et al. “COMPAS risk scales: Demonstrating accuracy equity and predictive parity”. Northpointe Inc (2016). [96] D. N. Dillenberger et al. “Blockchain analytics and artificial intelligence”. IBM Journal of Research and Development (2019).

, Vol. 1, No. 1, Article . Publication date: May 2022.

32

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

[97]
[98]
[99]
[100] [101]
[102] [103]
[104] [105]
[106] [107]
[108] [109] [110] [111] [112]
[113]
[114]
[115] [116] [117]
[118]
[119]
[120] [121] [122] [123]
[124] [125] [126]
[127]
[128]
[129] [130] [131]
[132]
[133] [134] [135] [136] [137]
[138]
[139] [140] [141] [142]
[143]
[144]

A. E. Dirik et al. “Analysis of seam-carving-based anonymization of images against PRNU noise pattern-based source attribution”. IEEE Transactions on Information Forensics and Security (2014). J. Dodge et al. “Explaining models: an empirical study of how explanations impact fairness judgment”. Proceedings of the 24th International Conference on Intelligent User Interfaces. 2019. Y. Dong et al. “Evading defenses to transferable adversarial examples by translation-invariant attacks”. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019. F. Doshi-Velez et al. “Accountability of AI Under the Law: The Role of Explanation”. Privacy Law Scholars Conference. (2018). F. K. Došilović et al. “Explainable artificial intelligence: A survey”. 2018 41st International convention on information and communication technology, electronics and microelectronics (MIPRO). IEEE. 2018. A. Dosovitskiy et al. “CARLA: An open urban driving simulator”. Conference on robot learning. PMLR. 2017. T. Dreossi et al. “Verifai: A toolkit for the formal design and analysis of artificial intelligence-based systems”. International Conference on Computer Aided Verification. Springer. 2019. J. Dressel et al. “The accuracy, fairness, and limits of predicting recidivism”. Science advances (2018). P. Drozdowski et al. “Demographic bias in biometrics: A survey on an emerging challenge”. IEEE Transactions on Technology and Society (2020). C. Drummond. “Replicability is not reproducibility: nor is it good science” (2009). C. Dwork. “Differential privacy: A survey of results”. International conference on theory and applications of models of computation. Springer. 2008. C. Dwork et al. “Calibrating noise to sensitivity in private data analysis”. Theory of cryptography conference. Springer. 2006. C. Dwork et al. “Calibrating noise to sensitivity in private data analysis”. Journal of Privacy and Confidentiality (2016). C. Dwork et al. “The algorithmic foundations of differential privacy.” Foundations and Trends in Theoretical Computer Science (2014). K. El Emam et al. “A systematic review of re-identification attacks on health data”. PloS one (2011). D. Erhan et al. “Why does unsupervised pre-training help deep learning?” Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings. 2010. Ú. Erlingsson et al. “Rappor: Randomized aggregatable privacy-preserving ordinal response”. Proceedings of the 2014 ACM SIGSAC conference on computer and communications security. 2014. D. Evans et al. “A pragmatic introduction to secure multi-party computation”. Foundations and Trends® in Privacy and Security (2018). Exforsys. What is Monkey Testing. Accessed: 2021-07-09. 2011. A. Fawzi et al. “Adversarial vulnerability for any classifier”. Advances in neural information processing systems (2018). M. Feldman et al. “Certifying and removing disparate impact”. proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. 2015. N. Ferro et al. “Overview of CENTRE@ CLEF 2018: a first tale in the systematic reproducibility realm”. International Conference of the Cross-Language Evaluation Forum for European Languages. Springer. 2018. C. Finn et al. “Model-agnostic meta-learning for fast adaptation of deep networks”. International Conference on Machine Learning. PMLR. 2017. S. Fletcher et al. “Decision tree classification with differential privacy: A survey”. ACM Computing Surveys (CSUR) (2019). L. Floridi. “Establishing the rules for building trustworthy AI”. Nature Machine Intelligence (2019). L. Floridi et al. “A unified framework of five principles for AI in society”. Harvard Data Science Review (2019). S. A. Friedler et al. “A comparative study of fairness-enhancing interventions in machine learning”. Proceedings of the conference on fairness, accountability, and transparency. 2019. J. Friedman et al. The elements of statistical learning. Springer series in statistics New York, 2001. N. Frosst et al. “Distilling a neural network into a soft decision tree”. arXiv:1711.09784 (2017). C. Fu et al. “Monocular visual-inertial SLAM-based collision avoidance strategy for fail-safe UAV using fuzzy logic controllers”. Journal of Intelligent & Robotic Systems (2014). S. Galhotra et al. “Fairness testing: testing software for discrimination”. Proceedings of the 2017 11th Joint meeting on foundations of software engineering. 2017. A. Gascón et al. “Privacy-preserving distributed linear regression on high-dimensional data”. Proceedings on Privacy Enhancing Technologies (2017). T. Gebru et al. “Datasheets for datasets”. Communications of the ACM (2021). A. Ghorbani et al. “Interpretation of neural networks is fragile”. Proceedings of the AAAI Conference on Artificial Intelligence. 2019. R. Girshick et al. “Rich feature hierarchies for accurate object detection and semantic segmentation”. Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. J. R. Goodall et al. “Situ: Identifying and explaining suspicious behavior in networks”. IEEE transactions on visualization and computer graphics (2018). I. Goodfellow et al. “Machine learning basics”. Deep learning (2016). I. J. Goodfellow et al. “Explaining and harnessing adversarial examples”. ICLR (Poster) 2015 (2015). B. Goodman et al. “European Union regulations on algorithmic decision-making and a “right to explanation””. AI magazine (2017). Google. Responsible AI with TensorFlow. Accessed: 2021-02-20. 2020. N. Grgić-Hlača et al. “Beyond distributive fairness in algorithmic decision making: Feature selection for procedurally fair learning”. Proceedings of the AAAI Conference on Artificial Intelligence. 2018. B. Gu et al. “Federated doubly stochastic kernel learning for vertically partitioned data”. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020. S. Gu et al. “Towards deep neural network architectures robust to adversarial examples” (2015). R. Guidotti et al. “A survey of methods for explaining black box models”. ACM computing surveys (CSUR) (2018). R. Guidotti et al. “Local rule-based explanations of black box decision systems”. arXiv:1805.10820 (2018). O. E. Gundersen et al. “On reproducible AI: Towards reproducible research, open science, and digital scholarship in AI publications”. AI magazine (2018). O. E. Gundersen et al. “State of the art: Reproducibility in artificial intelligence”. Proceedings of the AAAI Conference on Artificial Intelligence. 2018. D. Gunning et al. “DARPA’s explainable artificial intelligence (XAI) program”. AI magazine (2019).

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

33

[145] [146] [147] [148]
[149] [150]
[151] [152]
[153] [154] [155] [156] [157]
[158]
[159]
[160]
[161]
[162] [163]
[164]
[165]
[166] [167]
[168] [169]
[170]
[171]
[172]
[173]
[174]
[175]
[176]
[177] [178] [179]
[180] [181] [182]
[183] [184]
[185]
[186] [187] [188]
[189]

T. Hagendorff. “The ethics of AI ethics: An evaluation of guidelines”. Minds and Machines (2020). K. Hao. AI is sending people to jail-and getting it wrong. Accessed: 2021-02-20. 2019. M. Hardt et al. “Equality of opportunity in supervised learning”. Advances in neural information processing systems (2016). S. Hardy et al. “Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption”. arXiv:1711.10677 (2017). C. He et al. “Fedml: A research library and benchmark for federated machine learning”. arXiv:2007.13518 (2020). K. He et al. “Momentum contrast for unsupervised visual representation learning”. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. Y. He et al. “Anonymization of set-valued data via top-down, local generalization”. Proceedings of the VLDB Endowment (2009). M. Hein et al. “Formal guarantees on the robustness of a classifier against adversarial manipulation”. Advances in neural information processing systems (2017). D. Hendrycks et al. “Benchmarking neural network robustness to common corruptions and perturbations”. arXiv:1903.12261 (2019). M. Herschel et al. “A survey on provenance: What for? What form? What from?” The VLDB Journal (2017). R. R. Hoffman et al. “Metrics for explainable AI: Challenges and prospects”. arXiv:1812.04608 (2018). S. Holland et al. “The dataset nutrition label: A framework to drive higher data quality standards”. arXiv:1805.03677 (2018). A. Holzinger. “Interactive machine learning for health informatics: when do we need the human-in-the-loop?” Brain Informatics (2016). J. Hong et al. “Federated robustness propagation: Sharing adversarial robustness in federated learning”. arXiv preprint arXiv:2106.10196 (2021). Y. Hong et al. “Collaborative search log sanitization: Toward differential privacy and boosted utility”. IEEE Transactions on Dependable and Secure Computing (2014). M. Hörwick et al. “Strategy and architecture of a safety concept for fully automatic and autonomous driving assistance systems”. 2010 IEEE Intelligent Vehicles Symposium. IEEE. 2010. A. Howard et al. “The ugly truth about ourselves and our robot creations: the problem of bias and social inequity”. Science and engineering ethics (2018). X. Hu et al. “Model complexity of deep learning: A survey”. Knowledge and Information Systems (2021). C. Huang et al. “Learning deep representation for imbalanced classification”. Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. X. Huang et al. “A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability”. Computer Science Review (2020). W. Hummer et al. “Modelops: Cloud-based lifecycle management for reliable and trusted ai”. 2019 IEEE International Conference on Cloud Engineering (IC2E). IEEE. 2019. IBM. Trusting AI. Accessed: 2021-02-20. S. Ioffe et al. “Batch normalization: Accelerating deep network training by reducing internal covariate shift”. International conference on machine learning. PMLR. 2015. V. Iosifidis et al. “Fae: A fairness-aware ensemble framework”. 2019 IEEE International Conference on Big Data (Big Data). IEEE. 2019. R. Isdahl et al. “Out-of-the-box reproducibility: A survey of machine learning platforms”. 2019 15th international conference on eScience (eScience). IEEE. 2019. S. Jain et al. “Attention is not Explanation”. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019. C. E. Jakob et al. “Design and evaluation of a data anonymization pipeline to promote Open Science on COVID-19”. Scientific data (2020). M. Janssen et al. “Data governance: Organizing data for trustworthy Artificial Intelligence”. Government Information Quarterly (2020). S. A. Javadi et al. “Monitoring Misuse for Accountable’Artificial Intelligence as a Service’”. Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 2020. S. Ji et al. “Graph data anonymization, de-anonymization attacks, and de-anonymizability quantification: A survey”. IEEE Communications Surveys & Tutorials (2016). S. Ji et al. “{SecGraph}: A Uniform and Open-source Evaluation System for Graph Data Anonymization and De-anonymization”. 24th USENIX Security Symposium (USENIX Security 15). 2015. S. Ji et al. “Structural data de-anonymization: Quantification, practice, and implications”. Proceedings of the 2014 ACM SIGSAC conference on computer and communications security. 2014. Z. Ji et al. “Differential privacy and machine learning: a survey and review”. arXiv:1412.7584 (2014). A. Jobin et al. “The global landscape of AI ethics guidelines”. Nature Machine Intelligence (2019). S. Kafle et al. “Artificial intelligence fairness in the context of accessibility research on intelligent systems for people who are deaf or hard of hearing”. ACM SIGACCESS Accessibility and Computing (2020). P. Kairouz et al. “Advances and open problems in federated learning”. Foundations and Trends® in Machine Learning (2021). E. Karnin et al. “On secret sharing systems”. IEEE Transactions on Information Theory (1983). D. Kaur et al. “Requirements for trustworthy artificial intelligence–a review”. International Conference on Network-Based Information Systems. Springer. 2020. K. Kawaguchi et al. “Generalization in deep learning”. arXiv:1710.05468 (2017). B. Kim et al. “Examples are not enough, learn to criticize! criticism for interpretability”. Advances in neural information processing systems (2016). B. Kim et al. “Introduction to interpretable machine learning”. Proceedings of the CVPR 2018 Tutorial on Interpretable Machine Learning for Computer Vision, Salt Lake City, UT, USA (2018). M. Kim et al. “Fairness through computationally-bounded awareness”. Advances in Neural Information Processing Systems (2018). J. Kleinberg et al. “Inherent trade-offs in the fair determination of risk scores”. arXiv:1609.05807 (2016). B. Knott et al. “Crypten: Secure multi-party computation meets machine learning”. Advances in Neural Information Processing Systems (2021). B. Knowles et al. “The Sanction of Authority: Promoting Public Trust in AI”. Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021.

, Vol. 1, No. 1, Article . Publication date: May 2022.

34

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

[190] [191] [192]
[193]
[194]
[195]
[196] [197] [198]
[199]
[200] [201]
[202] [203] [204]
[205]
[206] [207] [208] [209]
[210]
[211]
[212] [213] [214] [215] [216]
[217] [218] [219]
[220]
[221]
[222] [223] [224]
[225]
[226]
[227]
[228]
[229]
[230]
[231]
[232] [233]
[234] [235]

G. Koch et al. “Siamese neural networks for one-shot image recognition”. ICML deep learning workshop. Lille. 2015. A. Koenecke et al. “Racial disparities in automated speech recognition”. Proceedings of the National Academy of Sciences (2020). N. Koenig et al. “Design and use paradigms for gazebo, an open-source multi-robot simulator”. 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No. 04CH37566). IEEE. P. W. Koh et al. “Understanding black-box predictions via influence functions”. International Conference on Machine Learning. PMLR. 2017. S. Kolouri et al. “Universal litmus patterns: Revealing backdoor attacks in cnns”. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. E. Krasanakis et al. “Adaptive sensitive reweighting to mitigate bias in fairness-aware classification”. Proceedings of the 2018 World Wide Web Conference. 2018. A. Krogh et al. “A simple weight decay can improve generalization”. Advances in neural information processing systems. 1992. K. Kuang et al. “Causal inference”. Engineering (2020). T. Kulesza et al. “Tell me more? The effects of mental model soundness on personalizing an intelligent agent”. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2012. A. Kumar et al. “Trustworthy AI in the Age of Pervasive Computing and Big Data”. 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops). IEEE. 2020. N. Kumar et al. “Cryptflow: Secure tensorflow inference”. 2020 IEEE Symposium on Security and Privacy (SP). IEEE. 2020. A. Kurakin et al. “Adversarial examples in the physical world”. Artificial intelligence safety and security. Chapman and Hall/CRC, 2018. H. Lakkaraju et al. “Interpretable & explorable approximations of black box models”. arXiv:1707.01154 (2017). A. Lavin et al. “Technology readiness levels for machine learning systems”. arXiv:2101.03989 (2021). M. Lecuyer et al. “Certified robustness to adversarial examples with differential privacy”. 2019 IEEE Symposium on Security and Privacy (SP). IEEE. 2019. S. Lee et al. “ConvLab: Multi-Domain End-to-End Dialog System Platform”. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. Florence, Italy: Association for Computational Linguistics, July 2019. K. Leino et al. “Globally-robust neural networks”. International Conference on Machine Learning. PMLR. 2021. J. C. S. d. P. Leite et al. “Software transparency”. Business & Information Systems Engineering (2010). B. Lepri et al. “Fair, transparent, and accountable algorithmic decision-making processes”. Philosophy & Technology (2018). D. Leslie. “Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector”. Available at SSRN 3403301 (2019). D. Lewis et al. “An Ontology for Standardising Trustworthy AI”. Factoring Ethics in Technology, Policy Making, Regulation and AI (2021). B. Li et al. “A general retraining framework for scalable adversarial classification”. NIPS 2016 Workshop on Adversarial Training (2016). K. Li et al. “Learning to optimize”. arXiv:1606.01885 (2016). Y. Li et al. “Backdoor learning: A survey”. arXiv:2007.08745 (2020). Y. Lindell. “Secure multiparty computation (MPC)”. Cryptology ePrint Archive (2020). Y. Lindell et al. “A proof of security of Yao’s protocol for two-party computation”. Journal of cryptology (2009). Z. C. Lipton. “The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.” Queue (2018). B. Liu et al. “Fedlearn-Algo: A flexible open-source privacy-preserving machine learning platform”. arXiv:2107.04129 (2021). H. Liu et al. “Trustworthy AI: A Computational Perspective”. arXiv:2107.06641 (2021). K. Liu et al. “Fine-pruning: Defending against backdooring attacks on deep neural networks”. International Symposium on Research in Attacks, Intrusions, and Defenses. Springer. 2018. K. Liu et al. “Towards identity anonymization on graphs”. Proceedings of the 2008 ACM SIGMOD international conference on Management of data. 2008. M. Liu et al. “STGAN: A unified selective transfer network for arbitrary image attribute editing”. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019. Y. Liu et al. “A secure federated transfer learning framework”. IEEE Intelligent Systems (2020). G. Long et al. “Federated learning for open banking”. Federated learning. Springer, 2020. J. Long et al. “Fully convolutional networks for semantic segmentation”. Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. S. M. Lundberg et al. “A Unified Approach to Interpreting Model Predictions”. Advances in Neural Information Processing Systems (2017). L. Ma et al. “Deepgauge: Multi-granularity testing criteria for deep learning systems”. Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. 2018. G. R. Machado et al. “Adversarial Machine Learning in Image Classification: A Survey Toward the Defender’s Perspective”. ACM Computing Surveys (CSUR) (2021). M. A. Madaio et al. “Co-designing checklists to understand organizational challenges and opportunities around fairness in AI”. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 2020. A. Madry et al. “Towards Deep Learning Models Resistant to Adversarial Attacks”. International Conference on Learning Representations. 2018. S. Magdici et al. “Fail-safe motion planning of autonomous vehicles”. 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC). IEEE. 2016. P. Maini et al. “Adversarial robustness against the union of multiple perturbation models”. International Conference on Machine Learning. PMLR. 2020. A. Majeed et al. “Anonymization techniques for privacy preserving data publishing: A comprehensive survey”. IEEE Access (2020). S. Mäkinen et al. “Who Needs MLOps: What Data Scientists Seek to Accomplish and How Can MLOps Help?” 2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineering for AI (WAIN). IEEE. 2021. C. D. Manning. “Human Language Understanding & Reasoning”. Daedalus (2022). D. Marijan et al. “Software Testing for Machine Learning”. Proceedings of the AAAI Conference on Artificial Intelligence. 2020.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

35

[236]
[237]
[238]
[239]
[240]
[241]
[242] [243] [244] [245] [246] [247]
[248] [249]
[250] [251]
[252]
[253]
[254] [255] [256] [257] [258]
[259] [260]
[261]
[262]
[263]
[264] [265] [266] [267]
[268]
[269] [270] [271] [272]
[273]
[274]
[275]
[276]
[277] [278]
[279]
[280]

D. J. Martin et al. “Worst-case background knowledge for privacy-preserving data publishing”. 2007 IEEE 23rd International Conference on Data Engineering. IEEE. 2007. A. Martins et al. “From softmax to sparsemax: A sparse model of attention and multi-label classification”. International Conference on Machine Learning. PMLR. 2016. B. Mathew et al. “HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection”. Proceedings of the AAAI Conference on Artificial Intelligence. 2021. M. Maximov et al. “Ciagan: Conditional identity anonymization generative adversarial networks”. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. F. McKeen et al. “Intel® software guard extensions (intel® sgx) support for dynamic memory management inside an enclave”. Proceedings of the Hardware and Architectural Support for Security and Privacy 2016. 2016. B. McMahan et al. “Communication-efficient learning of deep networks from decentralized data”. Artificial intelligence and statistics. PMLR. 2017. N. Mehrabi et al. “A survey on bias and fairness in machine learning”. ACM Computing Surveys (CSUR) (2021). J. H. Metzen et al. “On detecting adversarial perturbations” (2017). S. Micali et al. “How to play any mental game”. Proceedings of the Nineteenth ACM Symp. on Theory of Computing, STOC. ACM. 1987. T. Miller. “Explanation in artificial intelligence: Insights from the social sciences”. Artificial intelligence (2019). M. Mitchell et al. “Model cards for model reporting”. Proceedings of the conference on fairness, accountability, and transparency. 2019. P. Mohassel et al. “Secureml: A system for scalable privacy-preserving machine learning”. 2017 IEEE Symposium on Security and Privacy (SP). IEEE. 2017. S. Mohseni et al. “A human-grounded evaluation benchmark for local explanations of machine learning”. arXiv:1801.05075 (2018). S. Mohseni et al. “A multidisciplinary survey and framework for design and evaluation of explainable AI systems”. ACM Transactions on Interactive Intelligent Systems (TiiS) (2021). C. Molnar. Interpretable machine learning. Lulu. com, 2020. M. Mourby et al. “Are ‘pseudonymised’data always personal data? Implications of the GDPR for administrative data research in the UK”. Computer Law & Security Review (2018). M. W. Müller. “Increased autonomy for quadrocopter systems: trajectory generation, fail-safe strategies and state estimation”. PhD
thesis. ETH Zurich, 2016. M. E. Nergiz et al. “Hiding the presence of individuals from shared databases”. Proceedings of the 2007 ACM SIGMOD international conference on Management of data. 2007. OpenAI. OpenAI Charter. Accessed: 2021-02-20. 2018. S. J. Pan et al. “A survey on transfer learning”. IEEE Transactions on knowledge and data engineering (2009). Y. Pan et al. “Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training”. arXiv:2007.02375 (2020). G. Pang et al. “Deep learning for anomaly detection: A review”. ACM Computing Surveys (CSUR) (2021). A. Paudice et al. “Label sanitization against label flipping poisoning attacks”. Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer. 2018. J. Pearl. “Causal inference in statistics: An overview”. Statistics surveys (2009). K. Pei et al. “Deepxplore: Automated whitebox testing of deep learning systems”. proceedings of the 26th Symposium on Operating Systems Principles. 2017. A. Pérez et al. “Argos: An advanced in-vehicle data recorder on a massively sensorized vehicle for car driver behavior experimentation”. IEEE Transactions on Intelligent Transportation Systems (2010). Z. Pervaiz et al. “Accuracy-constrained privacy-preserving access control mechanism for relational data”. IEEE Transactions on Knowledge and Data Engineering (2013). J. Pineau et al. “Improving reproducibility in machine learning research: a report from the NeurIPS 2019 reproducibility program”. Journal of Machine Learning Research (2021). S. Pinto et al. “Demystifying arm trustzone: A comprehensive survey”. ACM Computing Surveys (CSUR) (2019). D. Piorkowski et al. “Towards evaluating and eliciting high-quality documentation for intelligent systems”. arXiv:2011.08774 (2020). G. Pleiss et al. “On fairness and calibration”. Advances in neural information processing systems (2017). F. Poursabzi-Sangdeh et al. “Manipulating and measuring model interpretability”. Proceedings of the 2021 CHI conference on human factors in computing systems. 2021. S. Rabanser et al. “Failing loudly: An empirical study of methods for detecting dataset shift”. Advances in Neural Information Processing Systems (2019). A. Radford et al. “Improving language understanding by generative pre-training” (2018). A. Raghunathan et al. “Adversarial training can hurt generalization”. arXiv:1906.06032 (2019). A. Raghunathan et al. “Certified Defenses against Adversarial Examples”. International Conference on Learning Representations. 2018. I. D. Raji et al. “About ml: Annotation and benchmarking on understanding and transparency of machine learning lifecycles”. arXiv:1912.06166 (2019). I. D. Raji et al. “Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing”. Proceedings of the 2020 conference on fairness, accountability, and transparency. 2020. B. Rakova et al. “Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices”. Proceedings of the ACM on Human-Computer Interaction (2021). R. Ramachandra et al. “Presentation attack detection methods for face recognition systems: A comprehensive survey”. ACM Computing Surveys (CSUR) (2017). D. Rathee et al. “CrypTFlow2: Practical 2-party secure inference”. Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security. 2020. D. Reisman et al. “Algorithmic impact assessments: A practical framework for public agency accountability”. AI Now Institute (2018). X. Ren et al. “LoPub: high-dimensional crowdsourced data publication with local differential privacy”. IEEE Transactions on Information Forensics and Security (2018). M. T. Ribeiro et al. “" Why should i trust you?" Explaining the predictions of any classifier”. Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 2016. M. T. Ribeiro et al. “Anchors: High-precision model-agnostic explanations”. Proceedings of the AAAI conference on artificial intelligence. 2018.

, Vol. 1, No. 1, Article . Publication date: May 2022.

36

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

[281]
[282] [283]
[284]
[285] [286]
[287] [288]
[289] [290]
[291]
[292] [293]
[294]
[295] [296] [297]
[298] [299]
[300]
[301]
[302]
[303] [304] [305]
[306] [307]
[308] [309]
[310] [311]
[312]
[313]
[314] [315]
[316]
[317]
[318] [319] [320] [321]
[322] [323] [324]
[325]

M. T. Ribeiro et al. “Beyond Accuracy: Behavioral Testing of NLP Models with CheckList”. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020. N. Rieke et al. “The future of digital health with federated learning”. NPJ digital medicine (2020). L. Rocher et al. “Estimating the success of re-identifications in incomplete datasets using generative models”. Nature communications (2019). Y. Roh et al. “FR-Train: A mutual information-based approach to fair and robust training”. International Conference on Machine Learning. PMLR. 2020. A. Rosenfeld et al. “Explainability in human–agent systems”. Autonomous Agents and Multi-Agent Systems (2019). A. Ross et al. “Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients”. Proceedings of the AAAI Conference on Artificial Intelligence. 2018. M. Sabt et al. “Trusted execution environment: what it is, and what it is not”. 2015 IEEE Trustcom/BigDataSE/ISPA. IEEE. 2015. P. Samarati et al. “Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and
suppression” (1998). E. Samuylova. Machine Learning Monitoring: What It Is, and What We Are Missing. Accessed: 2021-05-18. 2020. C. Sandvig et al. “Auditing algorithms: Research methods for detecting discrimination on internet platforms”. Data and discrimination: converting critical concerns into productive inquiry (2014). A. Santoro et al. “Meta-learning with memory-augmented neural networks”. International conference on machine learning. PMLR. 2016. L. Schelenz et al. “Applying Transparency in Artificial Intelligence based Personalization Systems”. arXiv e-prints (2020). S. Schelter et al. “Automatically tracking metadata and provenance of machine learning experiments”. Machine Learning Systems Workshop at NIPS. 2017. D. Schiff et al. “AI Ethics in the Public, Private, and NGO Sectors: A Review of a Global Document Collection”. IEEE Transactions on Technology and Society (2021). A. Schmidt et al. “Intervention user interfaces: a new interaction paradigm for automated systems”. Interactions (2017). D. Sculley et al. “Hidden technical debt in machine learning systems”. Advances in neural information processing systems (2015). R. R. Selvaraju et al. “Grad-cam: Visual explanations from deep networks via gradient-based localization”. Proceedings of the IEEE international conference on computer vision. 2017. S. A. Seshia et al. “Towards verified artificial intelligence”. arXiv:1606.08514 (2016). M. J. Sheller et al. “Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data”. Scientific reports (2020). H. Shi et al. “Secure multi-pArty computation grid LOgistic REgression (SMAC-GLORE)”. BMC medical informatics and decision making (2016). B. Shneiderman. “Bridging the gap between ethics and practice: Guidelines for reliable, safe, and trustworthy Human-Centered AI systems”. ACM Transactions on Interactive Intelligent Systems (TiiS) (2020). A. Shrikumar et al. “Learning important features through propagating activation differences”. International conference on machine learning. PMLR. 2017. U. Siddique. “SafetyOps”. arXiv:2008.04461 (2020). S. H. Silva et al. “Opportunities and challenges in deep learning adversarial robustness: A survey”. arXiv:2007.00753 (2020). K. Simonyan et al. “Deep inside convolutional networks: Visualising image classification models and saliency maps”. 2nd International Conference on Learning Representations, ICLR (2014). J. Singh et al. “Decision provenance: Harnessing data flow for accountable systems”. IEEE Access (2018). T. Sixta et al. “Fairface challenge at eccv 2020: Analyzing bias in face recognition”. European conference on computer vision. Springer. 2020. J. Snell et al. “Prototypical networks for few-shot learning”. Advances in neural information processing systems (2017). N. Srivastava et al. “Dropout: a simple way to prevent neural networks from overfitting”. The journal of machine learning research (2014).
B. Stanton et al. “Trust and artificial intelligence” (). S. Stumpf et al. “Explanations considered harmful? user interactions with machine learning systems”. Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI). 2016. D. Su et al. “Is Robustness the Cost of Accuracy?–A Comprehensive Study on the Robustness of 18 Deep Image Classification Models”. Proceedings of the European Conference on Computer Vision (ECCV). 2018. W. Su et al. “VL-BERT: Pre-training of Generic Visual-Linguistic Representations”. International Conference on Learning Representations. 2019. M. Sundararajan et al. “Axiomatic attribution for deep networks”. International conference on machine learning. PMLR. 2017. Supreme Audit Institutions of Finland, Germany, the Netherlands, Norway and the UK. Auditing machine learning algorithms. https: //www.auditingalgorithms.net/index.html, 2020. K. H. Tae et al. “Data cleaning for accurate, fair, and robust models: A big data-AI integration approach”. Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning. 2019. S. Tan et al. “Tree space prototypes: Another look at making tree ensembles interpretable”. Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference. 2020. X. Tan et al. “Night-time semantic segmentation with a large real dataset”. arXiv e-prints (2020). B. Templaton. Tesla’s "Shadow" Testing Offers A Useful Advantage On The Biggest Problem In Robocars. Accessed: 2021-06-15. 2019. M. Terrovitis et al. “Privacy-preserving anonymization of set-valued data”. Proceedings of the VLDB Endowment (2008). N. Thai-Nghe et al. “Cost-sensitive learning methods for imbalanced data”. The 2010 International joint conference on neural networks (IJCNN). IEEE. 2010. S. Thiebes et al. “Trustworthy artificial intelligence”. Electronic Markets (2020). M. Tideman. “Scenario-based simulation environment for assistance systems”. ATZautotechnology (2010). E. Todorov et al. “Mujoco: A physics engine for model-based control”. 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE. 2012. L. Tong et al. “Improving robustness of {ML} classifiers against realizable evasion attacks using conserved features”. 28th USENIX Security Symposium (USENIX Security 19). 2019.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices

37

[326]
[327] [328]
[329]
[330] [331] [332]
[333] [334] [335] [336] [337] [338] [339] [340] [341]
[342]
[343] [344]
[345]
[346] [347]
[348] [349]
[350] [351] [352]
[353]
[354]
[355] [356]
[357] [358] [359] [360]
[361]
[362] [363]
[364]
[365]
[366]
[367] [368] [369]
[370] [371] [372]
[373]

C. Torres et al. “Accessibility in Chatbots: The State of the Art in Favor of Users with Visual Impairment”. International Conference on Applied Human Factors and Ergonomics. Springer. 2018. F. Tramèr et al. “Ensemble Adversarial Training: Attacks and Defenses”. International Conference on Learning Representations. 2018. F. Tramer et al. “Fairtest: Discovering unwarranted associations in data-driven applications”. 2017 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE. 2017. F. Tramèr et al. “Stealing Machine Learning Models via Prediction {APIs}”. 25th USENIX security symposium (USENIX Security 16). 2016. A. Treiber et al. “CryptoSPN: Privacy-preserving Sum-Product Network Inference”. arXiv:2002.00801 (2020). D. Tsipras et al. “Robustness May Be at Odds with Accuracy” (2019). M. Tu et al. “Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents”. Proceedings of the AAAI Conference on Artificial Intelligence. 2020. M. Turilli et al. “The ethics of information transparency”. Ethics and Information Technology (2009). U. G. Union. “Top 10 principles for ethical artificial intelligence”. The future world of work (2017). C. Urban et al. “A Review of Formal Methods applied to Machine Learning”. arXiv:2104.02466 (2021). J. Vanschoren. “Meta-learning: A survey”. arXiv:1810.03548 (2018). V. Vapnik. The nature of statistical learning theory. Springer science & business media, 2013. K. R. Varshney. “Trustworthy machine learning and artificial intelligence”. XRDS: Crossroads, The ACM Magazine for Students (2019). S. Verma et al. “Fairness definitions explained”. 2018 ieee/acm international workshop on software fairness (fairware). IEEE. 2018. Y. Vorobeychik et al. “Adversarial machine learning”. Synthesis Lectures on Artificial Intelligence and Machine Learning (2018). S. Wachter et al. “Counterfactual explanations without opening the black box: Automated decisions and the GDPR”. Harv. JL & Tech. (2017). B. Wang et al. “Neural cleanse: Identifying and mitigating backdoor attacks in neural networks”. 2019 IEEE Symposium on Security and Privacy (SP). IEEE. 2019. J. Wang et al. “Generalizing to Unseen Domains: A Survey on Domain Generalization” (2021). Q. Wang et al. “Real-time and spatio-temporal crowd-sourced social network data publishing with differential privacy”. IEEE Transactions on Dependable and Secure Computing (2016). X. Wang et al. “Explainable reasoning over knowledge graphs for recommendation”. Proceedings of the AAAI Conference on Artificial Intelligence. 2019. Y. Wang et al. “Generalizing from a few examples: A survey on few-shot learning”. ACM Computing Surveys (CSUR) (2020). Y. Wang et al. “Measurement of the cognitive functional complexity of software”. The Second IEEE International Conference on Cognitive Informatics, 2003. Proceedings. IEEE. 2003. Y. Wang et al. “On the Convergence and Robustness of Adversarial Training.” ICML. 2019. K. Wei et al. “Federated learning with differential privacy: Algorithms and performance analysis”. IEEE Transactions on Information Forensics and Security (2020). K. Weiss et al. “A survey of transfer learning”. Journal of Big data (2016). D. S. Weld et al. “The challenge of crafting intelligible intelligence”. Communications of the ACM (2019). T.-W. Weng et al. “Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach”. International Conference on Learning Representations. 2018. C. S. Wickramasinghe et al. “Trustworthy AI Development Guidelines for Human System Interaction”. 2020 13th International Conference on Human System Interaction (HSI). IEEE. 2020. S. Wiegreffe et al. “Attention is not not Explanation”. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. S. Wiegreffe et al. “Teach Me to Explain: A Review of Datasets for Explainable NLP”. arXiv:2102.12060 (2021). C. Wilson et al. “Building and auditing fair algorithms: A case study in candidate screening”. Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021. J. M. Wing. “Trustworthy ai”. Communications of the ACM (2021). T. Wu et al. “Defending against physically realizable attacks on image classification” (2020). B. Wymann et al. “Torcs, the open racing car simulator”. Software available at http://torcs. sourceforge. net (2000). W. Xia et al. “Enabling realistic health data re-identification risk assessment through adversarial modeling”. Journal of the American Medical Informatics Association (2021). H. Xu et al. “To be robust or to be fair: Towards fairness in adversarial training”. International Conference on Machine Learning. PMLR. 2021. H. Xu et al. “Robustness and generalization”. Machine learning (2012). K. Xu et al. “Show, attend and tell: Neural image caption generation with visual attention”. International conference on machine learning. PMLR. 2015. Q. Xu et al. “Security of Neural Networks from Hardware Perspective: A Survey and Beyond”. 2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC). IEEE. 2021. Q. Yang et al. “Federated machine learning: Concept and applications”. ACM Transactions on Intelligent Systems and Technology (TIST) (2019). Z. Yang et al. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering”. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018. A. C. Yao. “Protocols for secure computations”. 23rd annual symposium on foundations of computer science (sfcs 1982). IEEE. 1982. T. Yao et al. “Seco: Exploring sequence supervision for unsupervised representation learning”. AAAI. 2021. Y. Yao et al. “The smart black box: A value-driven high-bandwidth automotive event data recorder”. IEEE Transactions on Intelligent Transportation Systems (2020). Y. Yao et al. “On early stopping in gradient descent learning”. Constructive Approximation (2007). M. Yu et al. “Diverse Few-Shot Text Classification with Multiple Metrics”. NAACL-HLT. 2018. M. Yuan et al. “Protecting sensitive labels in social network data anonymization”. IEEE Transactions on Knowledge and Data Engineering (2011). X. Yuan et al. “Adversarial examples: Attacks and defenses for deep learning”. IEEE transactions on neural networks and learning systems (2019).

, Vol. 1, No. 1, Article . Publication date: May 2022.

38

Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou

[374] [375] [376] [377] [378]
[379]
[380]
[381] [382]
[383]
[384]
[385]
[386]
[387] [388] [389]
[390]
[391] [392] [393] [394]

M. Zaharia et al. “Accelerating the Machine Learning Lifecycle with MLflow.” IEEE Data Eng. Bull. (2018). M. D. Zeiler et al. “Visualizing and understanding convolutional networks”. European conference on computer vision. Springer. 2014. R. Zemel et al. “Learning fair representations”. International conference on machine learning. PMLR. 2013. C. Zhang et al. “Understanding deep learning requires rethinking generalization” (2017). H. Zhang et al. “Theoretically principled trade-off between robustness and accuracy”. International Conference on Machine Learning. PMLR. 2019. H. Zhang et al. “Efficient neural network robustness certification with general activation functions”. Advances in neural information processing systems (2018). H. Zhang et al. “Towards stable and efficient training of verifiably robust neural networks”. Journal of Environmental Sciences (China) English Ed (2019). J. M. Zhang et al. “Machine learning testing: Survey, landscapes and horizons”. IEEE Transactions on Software Engineering (2020). M. Zhang et al. “Deeproad: Gan-based metamorphic testing and input validation framework for autonomous driving systems”. 2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE. 2018. Q. Zhang et al. “Interpretable convolutional neural networks”. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. Q. Zhang et al. “Interpreting cnns via decision trees”. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019. S. Zhang et al. “Anonymizing query logs by differential privacy”. Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. 2016. X. Zhang et al. “A scalable two-phase top-down specialization approach for data anonymization using mapreduce on cloud”. IEEE Transactions on Parallel and Distributed Systems (2013). C. Zhao et al. “Secure multi-party computation: theory, practice and applications”. Information Sciences (2019). Z. Zhong. A Tutorial on Fairness in Machine Learning. B. Zhou et al. “A brief survey on anonymization techniques for privacy preserving publishing of social network data”. ACM Sigkdd Explorations Newsletter (2008). B. Zhou et al. “Learning deep features for discriminative localization”. Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. K. Zhou et al. “Domain generalization: A survey”. arXiv:2103.02503 (2021). Y. Zhou et al. “Interpreting models via single tree approximation”. arXiv:1610.09036 (2016). Z.-H. Zhou et al. “Extracting symbolic rules from trained neural network ensembles”. Ai Communications (2003). I. Žliobaite˙ et al. “An overview of concept drift applications”. Big data analysis: new algorithms for a new society (2016).

, Vol. 1, No. 1, Article . Publication date: May 2022.

arXiv:2110.01167v2 [cs.AI] 26 May 2022

Trustworthy AI: From Principles to Practices – Appendix
A RECENT PRACTICES TRUSTWORTHY AI PRACTITIONERS
Diverse AI stakeholders have made efforts to enhance AI trustworthiness in the last decade. We review their recent practices in this section. We begin by introducing how various aspects of trustworthiness are being implemented in case studies of different AI applications. We then briefly review the recent progress toward trustworthy AI in multi-disciplinary areas including research, engineering, and regulation. We then show that a systematic framework is necessary to organize all these efforts.
A.1 Industrial Case Study A.1.1 Face Recognition. Face recognition has been one of the earliest AI techniques widely adopted in real-world scenarios. Compared to other biometric measurements, facial feature provides a much more convenient interface for users to leverage for identification. However, the recent widespread applications of face recognition bring new challenges and risks such as malicious attacks and privacy breaches. From the perspective of building trustworthy AI, we show recent representative problems and their solutions.
Face recognition has been widely adopted in security-critical scenarios such as border control and e-bank authentication. These applications raise concerns about the robustness of face recognition against malicious attacks. Naive face recognition can be easily spoofed by a photo of a person targeted for identity theft. This is known as the presentation attack on the face recognition system [66]. Liveness detection [18] is a typical approach to defend against presentation attacks by photos or displays. However, liveness detection is still vulnerable against new types of attacks by fake videos generated by AI techniques like DeepFake [54]. To comprehensively defend against presentation attacks, a number of new research directions have aroused attention from the AI community, including liveness detection, face anti-spoofing, and synthetic image detection [68]. Besides presentation attacks, adversarial attacks against face recognition have recently been proved as a new type of threat. Research has shown that face recognition systems can be easily spoofed by impersonation attackers wearing a small printed adversarial patch [90, 30, 43, 73]. Concerns on this new type of risk have made the adversarial attack and defense in face recognition an important applicational research direction of adversarial ML.
Modern face recognition systems are known to be sensitive to domain shift of input data such as the change of acquisition sensors or racial groups. Conventionally the mitigation of this performance degeneracy requires a large of annotated data from different domains. To overcome the problem of data scarcity, domain generalization techniques have been adopted in face recognition scenarios to enable unsupervised training. For example, some approaches cluster unlabeled face images to generate pseudo labels for training [93]. Domain adaption techniques are also studied to enhance the generalizability of face recognition models between different data domains [84].
Fairness issue is another challenge with which modern face recognition systems are confronted [33]. It is known that conventional face recognition algorithms are biased in performance on different groups of genders, races or ages [23]. This problem not only harms the user experience
© 2022 Association for Computing Machinery. This is the author’s version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in , https://doi.org/10.1145/nnnnnnn.nnnnnnn.
, Vol. 1, No. 1, Article . Publication date: May 2022.

2

Applications Requirements

Literature

Robustness

[66] [18] [90, 30, 43, 73]

Face Recognition

Generalization Fairness Explainability

[93] [84] [23] [26] [83] [95, 87]

Privacy Protection [4, 9]

Robustness

[36, 35] [25, 16] [89, 35]

Autonomous Driving

Generalization Transparency Explainability

[97, 70] [21] [96, 48]

Accountability

[60, 94]

Robustness

[39]

NLP

Explainability Fairness

[86] [44, 76, 80]

Value Alignment [20, 59, 17, 24, 69, 91]

Table a. Trustworthiness is raising concern in various scenarios of AI applications. We summarize representative papers addressing different aspects of trustworthy AI in applications of face recognition, autonomous driving, and NLP.

of specific groups, but also harms the societal trust on AI. To mitigate bias, debiasing algorithms have recently aroused interest in face recognition research. Representatives either develop training strategies to balance performance between groups [26] or extract features that are invariant across groups [83].
Compared to the conventional image classification problem, the explainability of face recognition is less studied in the past years. However, the value of explainable face recognition should not be neglected in practical high-stake applications where the explanation of model decision helps build trust. Research has explored some potential directions to interpret face recognition procedures [95, 87] by enforcing internal features to express explainable structures.
Last but not the least, the sensitivity of face data arouses the concern of privacy leakage for modern face recognition systems. A typical case is that deep learning models are known to leak training face data when attacked by “deep leakage” [98]. Privacy-preserving techniques like federated learning provide a solution to extract knowledge from distributed user data while preserving the identification-sensitive information [4, 9].
A.1.2 Autonomous Driving. Autonomous driving is one of the most representative AI systems applied in safety crucial scenarios. Its significant impact on social life raises global concern on ensuring its trustworthiness. Various efforts have been made in different aspects to address this problem.
Robustness is one of the most critical requirements for an autonomous driving vehicle. Poor model performance as well as external attacks [25, 16] both threaten its safety. To address this issue, the combination of multiple sensors, including cameras and lidars, is widely adopted when designing a state-of-the-art autonomous driving vehicle [19]. At the development stage, to validate the system robustness in a large number of traffic scenarios, simulation-based testing is widely used. Such simulation not only includes software simulation like CarSim and CARLA. but also includes hybrid realization with hardware-in-the-loop or vehicle-in-the-loop design [67]. In addition, a fallback plan [89, 35] is also required as an essential component on autonomous driving products. Inherited from the strict safety requirement in the automotive industry, standards like ISO 26262
, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices – Appendix

3

also provide effective guidelines on ensuring the robustness and safety of the autonomous driving system. The safety-critical character of autonomous driving further raises the requirement of auditability. Inspired by the success of flight data recorders in aviation, event data recorders for autonomous driving [60, 94] are proposed to collect audit trails.
Transparency and explainability are also important requirements for autonomous driving systems. At the product design stage, a clear specification of the functionality, e.g., traffic jam pilot (TJP), and autonomy level, i.e., L1–L5, provide the basic transparency of the functionality boundary of an autonomous driving system. At the algorithm level, though DNN has achieved impressive performance in autonomous driving tasks like perception and planning, its lack of explainability still places a demand to require explicit explanations or impose rules to ensure that the vehicle is behaving in a regulated manner [96, 48].
Last but not least, ethics and value alignment have always been concerns along with the evolving autonomous driving technology. For example, besides human drivers, the AI driver is now also often challenged by the public with the well-known trolley dilemma [61]. Based on different objectives, autonomous driving system might make different decisions to minimize the overall physical harm or protect those who obey traffic rules. Debates on different decision objectives reflect the ambiguity of human morality, especially fairness in this case, and lead to new challenges to build the trustworthiness of autonomous driving systems to the society.

A.1.3 Natural Language Processing. Unlike autonomous driving, natural language processing (NLP) applications are often less safetycritical. However, its widely available applications have challenged trustworthiness from different aspects, especially the alignment of human values and ethics. As more research and media attention is directed to these issues, industry practitioners have also introduced methods to address these concerns recently.
Society has noticed a series of negative events caused by the lack of ethical consideration in conversational AI products. A notable example of the breach in trust is the prevalent gender bias in machine translation. Specifically, when translating from languages that do not have gender distinction for third-person pronouns (e.g., Turkish) into one that does (e.g., English), translations usually exhibit bias that might stem from the data (e.g., “He is a doctor. She is a nurse.” even if gender cannot be inferred from the original text). Providers such as Google have taken steps to provide alternative gendered translations in these cases to help mitigate such issues [44].
On the other hand, systems that are designed by taking human values into consideration can also help avoid unforeseen risks. For instance, conversational AI systems that help serve customers in e-businesses could be equipped with flagging mechanisms for risky inquiries or unstable mental conditions of its users, and help us intervene in suicide attempts and work to improve social well-being [91].
Last but not least, voice assistants are becoming more and more prevalent in our daily lives, helping us operate devices in smart homes, as well as conveniently automating our daily workflows. However, most commercially available voice assistants (Apple Siri, Amazon Alexa, Google Assistant, etc.) are made exclusively available in female voices, or use female personas by default [76, 80]. This could potentially strengthen gender stereotypes, and potentially associate aggressive user behaviors with feminine service personnel in real life, which is especially concerning when a new generation of children grow up around these devices [24, 69]. More recently, most major voice assistant service providers have made them available in more voice options, which helps these systems better align with human values by reducing the implicit gender inequity.

, Vol. 1, No. 1, Article . Publication date: May 2022.

4
A.2 Recent Multi-disciplinary Progresses
As can be seen from the above recent practices in different applications, establishing AI trustworthiness requires consideration, design, development, and governance throughout different stages of the system lifecycle to fulfill the requirements as mentioned in Section 2. This fact has motivated efforts from various fields to building trustworthy AI. In this section, we review the recent representative progresses, and demonstrate that a framework to organize these fields along the AI system lifecycle is a necessity.
A.2.1 Multi-disciplinary research. Trustworthy AI is more than a subdiscipline of computer science or ML. Its profound impact on human society has derived various research topics demanding prompt solutions.
Societal understanding of trustworthy AI. The impact of AI on human society is profound and double-edged. It becomes an important problem to understand and forecast both the negative and positive impact of AI. Literature such as [41, 42, 92] has collected and studied representative AI misuse cases, and summarized high-stake patterns that AI failures tend to follow [92], such as vulnerability to attacks, underperformance on noises, and biased prediction. Understanding these patterns naturally derives a number of specific requirements of AI trustworthiness, such as robustness, generalization, and fairness. In addition to lessons learned from the AI failures, potential areas for AI to make positive impact are also important to study [78, 32]. For example, as noted in [78, 62], various scenarios have been studied to apply AI to advance the United Nations’ Sustainable Development Goals (SDGs) for social good.
In addition to the social impact of AI application, the societal and psychological outline of trustworthiness should be a prerequisite for the establishment of trustworthy AI. A main aspect of this study starts by analyzing the risk, and threat of AI and derives the requirements of trustworthiness [77]. Beyond empirical analysis, the formalization of AI trustworthiness is also studied. For example, [38] formalizes human–AI trust by considering and extending the concepts of interpersonal trust. In terms of specific dimensions of trustworthy AI, previous works such as [29, 53, 50] have studied the problem of explainability and transparency from philosophy, cognitive psychology, and social psychology. Analysis of ethical values regarding AI application is also an important problem in the aspect of social science and can be found in recent literature such as [61, 28, 11, 52].
Technical approaches. As is shown in Section 2, AI trustworthiness is decomposed as the fulfillment of a series of requirements. It is recognized that technology improvement serves as a core solution to many of these requirements, such as robustness, generalization, explainability, fairness, and privacy. In the last decade, these requirements have derived a variety of active directions in AI research. Recently there has been some literature that briefly surveys the related technology to facilitate the establishment of trustworthy AI. For example, [79, 49] list example trustworthy research directions in the area of ML algorithms. [85] surveys trustworthy AI development techniques from the aspect of human–system interaction. [45] comprehensively decomposes requirements of trustworthy AI to executable techniques that are of the current interest in the community of AI research. Due to the wide coverage of trustworthy AI related techniques, it is difficult to exhaustively survey its every dimension in a single paper. Fortunately, there have been surveys available that covers several subdivisions of research mentioned in this paper. We refer to these surveys in Sections 2 and 3.
The establishment of trustworthy AI systems involves the development of both AI algorithms and engineering techniques. Take the requirement of robustness as an example. Adversarial attack and defense are becoming popular research directions. They have been studied in various high-stake tasks such as face recognition [43], autonomous driving [16], and recommendation systems [47]. Besides algorithms to enhance model robustness against attacks, engineering techniques are
, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices – Appendix

5

increasingly raising concern to enhance system robustness. At the development stage of the system, comprehensive testing on malicious inputs serves as an essential guarantee to avoid malfunction on attacks. At the deployment stage, a fail-safe mechanism prevents the system to cause loss even if malfunction happens.
Non-technical approaches. The achievement of trustworthy AI requires more than just technical approaches. The creation and operation of AI systems involve the participation of multiple roles including researchers, software developers, business operators, and regulators. The systems’ lifecycle ranges over the steps of product design, algorithm design, system development, deployment, and business operation. It becomes a prerequisite of trustworthy AI to align the understanding and efforts of these various roles and steps, which raises the demand for appropriate management strategies.
Literature such as [74, 10, 65] has pointed a series of reforms to the organization of participants and procedures to build trustworthy AI. These reforms include responsible product design aligned with trustworthiness principles [6], new human resource strategies [74, 65], and justification of trustworthiness via continuous auditing [74]. We continue to consider the aforementioned example of enhancing AI robustness. Algorithm researchers and software engineers should be aligned with the same requirement of robustness so that they apply appropriate techniques in their area to prevent the risk of attacks. In high-stake applications, safety engineers [75] should be involved to oversight the lifecycle of a system and assess potential risks. Further, business operators should be aware of the working condition and limitation of the system to avoid exceptional use cases. When a new type of attacks happen, feedback from business operators to system manufacturers serves as a prerequisite to upgrade the system against the problem. Besides direct participants in the system, third-party auditors provide independent certification of robustness and security, which serves as an external driving force to the fulfillment of trustworthiness requirements [99, 64].

A.2.2 Engineering practices. With the development of the study on trustworthy AI, a number of common approaches in building AI trustworthiness have been organized as tools to address various aspects of AI trustworthiness.
Development tools. Various toolboxes have been released in the past years to assist the algorithm experimentation and software development to enhance AI trustworthiness in different aspects such as fairness [12, 13, 71], XAI [56, 1, 8], and adversarial robustness [55, 2]. For the management stage, tools and references are released to facilitate the improvement of documentation and thus enhance transparency [7]. Development platforms such as CodaLab provide workspaces for reproducible AI experimentation and development. Tools to test and audit the system trustworthiness in specific requirements such as fairness [88] provide further verification to the system development.
To manage and evaluate different tools, the OECD Network of Experts on Implementing Trustworthy AI is working on a generic framework to help practitioners choose suitable tools for their specific use case [3]. The OECD has conducted a survey and shows that three types of tools are of the common focus: 1) Technical tools aiming to address technical aspects such as the aforementioned robustness, explainability, and bias detection; 2) Procedural tools providing implementation guidance; 3) Educational tools training stakeholders of the AI systems.
Development frameworks. As can be seen from the above examples and categorization, existing tools reflect the consideration of trustworthy AI at different stages in the lifecycle of an AI system, such as algorithm design, development, and management [5]. Each of these tools mainly focuses on a specific requirement of trustworthy AI and improves a specific stage of the lifecycle. To build a complete trustworthy AI system, there is a necessity to select and organize these tools systematically.

, Vol. 1, No. 1, Article . Publication date: May 2022.

6
Some institutions have released their collection of tools as a framework to facilitate the development of trustworthy AI systems, e.g., Amazon SageMaker, IBM OpenScale. We take Google’s Responsible AI with TensorFlow as a representative practical collection of modules considering requirements of trustworthy AI at multiple stages when building AI systems [27]. The People+AI Research (PAIR) Guidebook provides guidelines and best practices to better understand the human factors when designing an AI product. At the model development stage, libraries and tools are provided within the framework to enhance the explainability, fairness, and privacy of the model. Tool-chains are also available to track the meta information regarding the training process [51]. Responsible AI reflects the consideration of AI trustworthiness from the perspective of the AI system lifecycle. It provides well-developed libraries that can be directly adopted at the stages of problem definition, data preparation, model training, model evaluation, and model deployment. However, considering the case studies aforementioned in this section, this collection of tools is still insufficient to guide and support the industrial practices to address trustworthy AI through the system lifecycle.
A.2.3 Institutionalization. Though the value of trustworthy AI has been widely recognized, the motivation, interpretation, and consideration on AI trustworthiness vary over different stakeholders. Such inconsistency is even more evident in the industry due to the commercial consideration. Even within a single company, different departments of such as business, technology, and legality are likely to hold different interpretation on requirements of AI trustworthiness. To address this issue, guidelines, standards, and regulations become necessary to unify the understanding of different AI stakeholders.
Guidelines. To regulate the development of AI, a variety of guidelines and principles have been proposed by governmental organizations in recent years [5, 34, 57, 81, 46]. Representatives of these guidelines include the EU Ethics Guidelines for Trustworthy AI [5]. Besides governmental organizations, a wealth of companies also release their guidelines or principles to demonstrate their compliance with AI trustworthiness [27, 58, 37, 15, 14, 82]. Thorough review and discussion of these guidelines can be found from [40, 72, 31]. Though differences exist, it is shown that global convergence on the requirement of AI trustworthiness is recognized as summarized in Section 2.
Standardization. With various ethics guidelines released globally as soft recommendations for AI development and converges to consensus principles, standards to concretize these principles in specific AI application areas serve as the further means to regulate the trustworthiness of AI products [100]. Multiple standardization activities are ongoing parallelly, including IEEE P7000 and ISO/IEC SC 42 [46], which specifically focuses on AI trustworthiness. There are multiple different aspects addressed in the current standardization. For example, at the holistic level, ISO/IEC SC 42 Working Group 3 (WG3) works on establishing trustworthiness for general ML systems through transparency, robustness, explainability, and so on [46]. Besides generic AI-centered standards, certain applications of AI should also seek compliance with domain-specific standards to build trustworthy AI products, e.g., ISO 26262 for the functional safety of intelligent vehicles and ISO 14155 for clinical practice on medical devices.
Regulation. Governmental legislation is catching up its pace with the rapid development of AI technology and industrial application. Recently EU releases its proposal for Artificial Intelligence Act [22]. A notable point of the act is the requirement to execute risk management approaches throughout different stages in the lifecycle of an AI system. These approaches include but not limited to data governance, testing for risks, detailed documentation, logging, and human oversight. Besides regulation on AI systems, it should also be noted that a wealth of regulation in the area of general information technology is also bringing significant impact on the AI industry, such as GDPR, the Chinese Cybersecurity Law, and the Personal Information Security Specification.
, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices – Appendix

7

A.2.4 Toward systematic organization. As can be seen from the above recent practices, the establishment of trustworthy AI requires efforts at different stages in the lifecycle of an AI system. This fact motivates the need to organize all these fragmented efforts in a unified framework. A number of works have conducted studies on such frameworks, mainly on a specific area such as technique or management. For example, [45] briefly reviews executive techniques and organizes them by the specified requirement dimensions of trustworthy AI. [85] categorizes the interactions between AI systems, developers, and users to organize the technical approaches and responsibility in the lifecycle of trustworthy AI. [77] simplifies the system lifecycle as the flow of data and technically analyzes threats existing at each flow stage. The aforementioned tool-chains such as Google Responsible AI also reflect the idea of organizing existing techniques in the frame of system development lifecycle. In terms of management, [10] studies a management workflow to guarantee that ethics are aligned throughout the creation and update of an AI system. [63] specifically organizes auditing approaches for the management of system lifecycle.
Whilst the above existing literature has studied the organization of technical and management approaches to build trustworthy AI, we argue that these works are still insufficient from the industrial perspective. Considering the case study of e.g., autonomous driving in Section A.1, this research of frameworks has not yet covered all aspects that should be addressed. As depicted in Section 1 and Figure 2, the AI industry holds a connecting position to organize efforts from multi-disciplinary fields in the establishment of trustworthy AI. This position requires industrial stakeholders to learn and organize the diverse approaches from areas of academic research, engineering, and management so that trustworthy AI can be ensured throughout the lifecycle of products. By surveying the above previous works, we find a framework covering all these areas is still lacking. In Section 3, we push a step forward toward this requirement by a multi-disciplinary survey of approaches to build trustworthy AI. These approaches cover active research areas, engineering techniques, and management strategies, and are organized in the frame of an industrial system lifecycle. A workflow is then proposed to integrate these approaches and facilitate dynamic feedback and upgrade from academia and governmental regulation over these approaches.

REFERENCES
[1] https://github.com/EthicalML/xai. [2] https://github.com/Trusted-AI/adversarial-robustness-toolbox. [3] https://oecd.ai/wonk/tools-for-trustworthy-ai. [4] Divyansh Aggarwal, Jiayu Zhou, and Anil K Jain. Fedface: Collaborative learning of face recognition model. In 2021
IEEE International Joint Conference on Biometrics (IJCB), pages 1–8. IEEE, 2021. [5] AI HLEG. Ethics guidelines for trustworthy AI, 2018. Accessed: 2021-02-20. [6] Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi
Iqbal, Paul N Bennett, Kori Inkpen, et al. Guidelines for human-ai interaction. In Proceedings of the 2019 chi conference on human factors in computing systems, pages 1–13, 2019. [7] Matthew Arnold, Rachel KE Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Aleksandra Mojsilović, Ravi Nair, K Natesan Ramamurthy, Alexandra Olteanu, David Piorkowski, et al. Factsheets: Increasing trust in ai services through supplier’s declarations of conformity. IBM Journal of Research and Development, 63(4/5):6–1, 2019. [8] Vijay Arya, Rachel KE Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C Hoffman, Stephanie Houde, Q Vera Liao, Ronny Luss, Aleksandra Mojsilović, et al. One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques. arXiv:1909.03012, 2019. [9] Fan Bai, Jiaxiang Wu, Pengcheng Shen, Shaoxin Li, and Shuigeng Zhou. Federated face recognition. arXiv:2105.02501, 2021. [10] Josef Baker-Brunnbauer. Trustworthy ai implementation (taii) framework for ai systems. Available at SSRN 3796799, 2021.
, Vol. 1, No. 1, Article . Publication date: May 2022.

8
[11] Sarah Bankins and Paul Formosa. Ethical ai at work: The social contract for artificial intelligence and its implications for the workplace psychological contract. Redefining the Psychological Contract in the Digital Era: Issues for Research and Practice, pages 55–72, 2021.
[12] Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, et al. Ai fairness 360: An extensible toolkit for detecting. Understanding, and Mitigating Unwanted Algorithmic Bias, 2018.
[13] Sarah Bird, Miro Dudík, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan, Mehrnoosh Sameki, Hanna Wallach, and Kathleen Walker. Fairlearn: A toolkit for assessing and improving fairness in ai. Microsoft, Tech. Rep. MSR-TR-2020-32, 2020.
[14] Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen Krueger, Gillian Hadfield, Heidy Khlaaf, Jingying Yang, Helen Toner, Ruth Fong, et al. Toward trustworthy ai development: mechanisms for supporting verifiable claims. arXiv:2004.07213, 2020.
[15] Rosario Cammarota, Matthias Schunter, Anand Rajan, Fabian Boemer, Ágnes Kiss, Amos Treiber, Christian Weinert, Thomas Schneider, Emmanuel Stapf, Ahmad-Reza Sadeghi, et al. Trustworthy ai inference systems: An industry research view. arXiv:2008.04449, 2020.
[16] Yulong Cao, Chaowei Xiao, Dawei Yang, Jing Fang, Ruigang Yang, Mingyan Liu, and Bo Li. Adversarial objects against lidar-based autonomous driving systems. Journal of Environmental Sciences (China) English Ed, 2019.
[17] Yixuan Chai, Guohua Liu, Ziwei Jin, and Donghong Sun. How to keep an online learning chatbot from being corrupted. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2020.
[18] Saptarshi Chakraborty and Dhrubajyoti Das. An overview of face liveness detection. arXiv:1405.2227, 2014. [19] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object detection network for autonomous
driving. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1907–1915, 2017. [20] Merav Chkroun and Amos Azaria. “ did i say something wrong?”: Towards a safe collaborative chatbot. In Thirty-Second
AAAI Conference on Artificial Intelligence, 2018. [21] Luiz Marcio Cysneiros, Majid Raffi, and Julio Cesar Sampaio do Prado Leite. Software transparency as a key requirement
for self-driving cars. In 2018 IEEE 26th international requirements engineering conference (RE), pages 382–387. IEEE, 2018. [22] Kristof De Vulder, Florian De Rouck, and Gilles Hachez. The world’s first artificial intelligence act: Europe’s proposal to lead in human-centered AI. https://www.technologyslegaledge.com/2021/04/the-worlds-first-artificial-intelligenceact-europes-proposal-to-lead-in-human-centered-ai, 2021. Accessed: 2021-04-30. [23] Pawel Drozdowski, Christian Rathgeb, Antitza Dantcheva, Naser Damer, and Christoph Busch. Demographic bias in biometrics: A survey on an emerging challenge. IEEE Transactions on Technology and Society, 1(2):89–103, 2020. [24] Mike Elgan. The case against teaching kids to be polite to Alexa. https://www.fastcompany.com/40588020/the-caseagainst-teaching-kids-to-be-polite-to-alexa, 2018. Accessed: January 22, 2021. [25] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1625–1634, 2018. [26] Sixue Gong, Xiaoming Liu, and Anil K Jain. Mitigating face recognition bias via group adaptive classifier. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3414–3424, 2021. [27] Google. Responsible AI with tensorflow. https://blog.tensorflow.org/2020/06/responsible-ai-with-tensorflow.html, 2020. Accessed: 2021-02-20. [28] Daniel Greene, Anna Lauren Hoffmann, and Luke Stark. Better, nicer, clearer, fairer: A critical assessment of the movement for ethical artificial intelligence and machine learning. In Proceedings of the 52nd Hawaii international conference on system sciences, 2019. [29] David Gunning and David Aha. Darpa’s explainable artificial intelligence (xai) program. AI magazine, 40(2):44–58, 2019. [30] Ying Guo, Xingxing Wei, Guoqiu Wang, and Bo Zhang. Meaningful adversarial stickers for face recognition in physical world. arXiv:2104.06728, 2021. [31] Thilo Hagendorff. The ethics of ai ethics: An evaluation of guidelines. Minds and Machines, 30(1):99–120, 2020. [32] Gregory D Hager, Ann Drobnis, Fei Fang, Rayid Ghani, Amy Greenwald, Terah Lyons, David C Parkes, Jason Schultz, Suchi Saria, Stephen F Smith, et al. Artificial intelligence for social good. arXiv:1901.05406, 2019. [33] Karen Hao. Making face recognition less biased doesn’t make it less scary. MIT Technology Review, https://www. technologyreview.com/2019/01/29/137676/making-face-recognition-less-biased-doesnt-make-it-less-scary/, 2019. Accessed: 2021-02-20. [34] John P Holdren, Megan Smith, et al. Preparing for the future of artificial intelligence. White House: Washington, DC, USA, 2016.
, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices – Appendix

9

[35] Markus Hörwick and Karl-Heinz Siedersberger. Strategy and architecture of a safety concept for fully automatic and autonomous driving assistance systems. In 2010 IEEE Intelligent Vehicles Symposium, pages 955–960. IEEE, 2010.
[36] WuLing Huang, Kunfeng Wang, Yisheng Lv, and FengHua Zhu. Autonomous vehicles testing methods review. In 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC), pages 163–168. IEEE, 2016.
[37] IBM. Trusting AI. https://www.research.ibm.com/artificial-intelligence/trusted-ai/. Accessed: 2021-02-20.
[38] Alon Jacovi, Ana Marasović, Tim Miller, and Yoav Goldberg. Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in ai. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 624–635, 2021.
[39] Robin Jia. Building Robust Natural Language Processing Systems. Stanford University, 2020. [40] Anna Jobin, Marcello Ienca, and Effy Vayena. The global landscape of ai ethics guidelines. Nature Machine Intelligence,
1(9):389–399, 2019. [41] Nektaria Kaloudi and Jingyue Li. The ai-based cyber threat landscape: A survey. ACM Computing Surveys (CSUR),
53(1):1–34, 2020.
[42] Thomas C King, Nikita Aggarwal, Mariarosaria Taddeo, and Luciano Floridi. Artificial intelligence crime: An interdisciplinary analysis of foreseeable threats and solutions. Science and engineering ethics, 26(1):89–120, 2020.
[43] Stepan Komkov and Aleksandr Petiushko. Advhat: Real-world adversarial attack on arcface face id system. In 2020 25th International Conference on Pattern Recognition (ICPR), pages 819–826. IEEE, 2021.
[44] James Kuczmarski. Reducing gender bias in google translate. https://blog.google/products/translate/reducing-gender-
bias-google-translate/, 2018. Accessed: April 28, 2021.
[45] Abhishek Kumar, Tristan Braud, Sasu Tarkoma, and Pan Hui. Trustworthy ai in the age of pervasive computing and big data. In 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops), pages 1–6. IEEE, 2020.
[46] Dave Lewis, David Filip, and Harshvardhan J Pandit. An ontology for standardising trustworthy ai. Factoring Ethics in Technology, Policy Making, Regulation and AI, page 65, 2021.
[47] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on factorization-based collaborative filtering. Advances in neural information processing systems, 29:1885–1893, 2016.
[48] Amarildo Likmeta, Alberto Maria Metelli, Andrea Tirinzoni, Riccardo Giol, Marcello Restelli, and Danilo Romano.
Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving. Robotics and Autonomous Systems, 131:103568, 2020.
[49] Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Yaxin Li, Shaili Jain, Anil K Jain, and Jiliang Tang. Trustworthy ai: A computational perspective. arXiv:2107.06641, 2021.
[50] Tim Miller. Explanation in artificial intelligence: Insights from the social sciences. Artificial intelligence, 267:1–38,
2019.
[51] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency, pages 220–229, 2019.
[52] Brent Mittelstadt. Principles alone cannot guarantee ethical ai. Nature Machine Intelligence, 1(11):501–507, 2019.
[53] Sina Mohseni, Niloofar Zarei, and Eric D Ragan. A multidisciplinary survey and framework for design and evaluation of explainable ai systems. ACM Transactions on Interactive Intelligent Systems (TiiS), 11(3-4):1–45, 2021.
[54] Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Cuong M Nguyen, Dung Nguyen, Duc Thanh Nguyen, and Saeid Nahavandi. Deep learning for deepfakes creation and detection: A survey. arXiv preprint arXiv:1909.11573, 2019.
[55] Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, et al. Adversarial robustness toolbox v1. 0.0. arXiv:1807.01069,
2018.
[56] Harsha Nori, Samuel Jenkins, Paul Koch, and Rich Caruana. Interpretml: A unified framework for machine learning interpretability. arXiv:1909.09223, 2019.
[57] Beijing Academy of Artificial Intelligence. Ethics guidelines for trustworthy AI, 2019.
[58] OpenAI. OpenAI charter. https://openai.com/charter/, 2018. Accessed: 2021-02-20.
[59] Namkee Park, Kyungeun Jang, Seonggyeol Cho, and Jinyoung Choi. Use of offensive language in human-artificial
intelligence chatbot interaction: The effects of ethical ideology, social competence, and perceived humanlikeness. Computers in Human Behavior, 121:106795, 2021.
[60] Antonio Pérez, M Isabel García, Manuel Nieto, José L Pedraza, Santiago Rodríguez, and Juan Zamorano. Argos: An advanced in-vehicle data recorder on a massively sensorized vehicle for car driver behavior experimentation. IEEE Transactions on Intelligent Transportation Systems, 11(2):463–473, 2010.
[61] Samuele Lo Piano. Ethical principles in machine learning and artificial intelligence: cases from the field and possible ways forward. Humanities and Social Sciences Communications, 7(1):1–7, 2020.

, Vol. 1, No. 1, Article . Publication date: May 2022.

10
[62] Peng Qi, Jing Huang, Youzheng Wu, Xiaodong He, and Bowen Zhou. Conversational ai systems for social good: Opportunities and challenges. arXiv:2105.06457, 2021.
[63] Inioluwa Deborah Raji, Andrew Smart, Rebecca N White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes. Closing the ai accountability gap: Defining an end-to-end framework for internal algorithmic auditing. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pages 33–44, 2020.
[64] Inioluwa Deborah Raji and Jingying Yang. About ml: Annotation and benchmarking on understanding and transparency of machine learning lifecycles. arXiv:1912.06166, 2019.
[65] Bogdana Rakova, Jingying Yang, Henriette Cramer, and Rumman Chowdhury. Where responsible ai meets reality: Practitioner perspectives on enablers for shifting organizational practices. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1):1–23, 2021.
[66] Raghavendra Ramachandra and Christoph Busch. Presentation attack detection methods for face recognition systems: A comprehensive survey. ACM Computing Surveys (CSUR), 50(1):1–37, 2017.
[67] Francisca Rosique, Pedro J Navarro, Carlos Fernández, and Antonio Padilla. A systematic review of perception system and simulators for autonomous vehicles research. Sensors, 19(3):648, 2019.
[68] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. Faceforensics++: Learning to detect manipulated facial images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1–11, 2019.
[69] Olivia Rudgard. ‘alexa generation’ may be learning bad manners from talking to digital assistants, report warns. https://www.telegraph.co.uk/news/2018/01/31/alexa-generation-could-learning-bad-manners-talking-digital/, 2018. Accessed: January 10, 2021.
[70] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Guided curriculum model adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7374–7383, 2019.
[71] Pedro Saleiro, Benedict Kuester, Loren Hinkson, Jesse London, Abby Stevens, Ari Anisfeld, Kit T Rodolfa, and Rayid Ghani. Aequitas: A bias and fairness audit toolkit. arXiv:1811.05577, 2018.
[72] Daniel Schiff, Jason Borenstein, Justin Biddle, and Kelly Laas. Ai ethics in the public, private, and ngo sectors: A review of a global document collection. IEEE Transactions on Technology and Society, 2021.
[73] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. A general framework for adversarial examples with objectives. ACM Transactions on Privacy and Security (TOPS), 22(3):1–30, 2019.
[74] Ben Shneiderman. Bridging the gap between ethics and practice: Guidelines for reliable, safe, and trustworthy human-centered ai systems. ACM Transactions on Interactive Intelligent Systems (TiiS), 10(4):1–31, 2020.
[75] Umair Siddique. Safetyops. arXiv:2008.04461, 2020. [76] Benedict Tay, Younbo Jung, and Taezoon Park. When stereotypes meet robots: the double-edge sword of robot gender
and personality in human–robot interaction. Computers in Human Behavior, 38:75–84, 2014. [77] Scott Thiebes, Sebastian Lins, and Ali Sunyaev. Trustworthy artificial intelligence. Electronic Markets, pages 1–18,
2020. [78] Nenad Tomašev, Julien Cornebise, Frank Hutter, Shakir Mohamed, Angela Picciariello, Bec Connelly, Danielle CM
Belgrave, Daphne Ezer, Fanny Cachat van der Haert, Frank Mugisha, et al. Ai for social good: unlocking the opportunity for positive impact. Nature Communications, 11(1):1–6, 2020. [79] Ehsan Toreini, Mhairi Aitken, Kovila Coopamootoo, Karen Elliott, Carlos Gonzalez Zelaya, and Aad van Moorsel. The relationship between trust in ai and trustworthy machine learning technologies. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 272–283, 2020. [80] UNESCO and EQUALS Skills Coalition. I’d blush if I could: closing gender divides in digital skills through education. https://unesdoc.unesco.org/ark:/48223/pf0000367416.page=1, 2019. Accessed: January 21, 2021. [81] UNI Global Union. Top 10 principles for ethical artificial intelligence. The future world of work, 2017. [82] Kush R Varshney. Trustworthy machine learning and artificial intelligence. XRDS: Crossroads, The ACM Magazine for Students, 25(3):26–29, 2019. [83] Hao Wang, Dihong Gong, Zhifeng Li, and Wei Liu. Decorrelated adversarial learning for age-invariant face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3527–3536, 2019. [84] Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial faces in the wild: Reducing racial bias by information maximization adaptation network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 692–702, 2019. [85] Chathurika S Wickramasinghe, Daniel L Marino, Javier Grandio, and Milos Manic. Trustworthy ai development guidelines for human system interaction. In 2020 13th International Conference on Human System Interaction (HSI), pages 130–136. IEEE, 2020.
, Vol. 1, No. 1, Article . Publication date: May 2022.

Trustworthy AI: From Principles to Practices – Appendix

11

[86] Sarah Wiegreffe and Ana Marasović. Teach me to explain: A review of datasets for explainable nlp. arXiv:2102.12060,
2021. [87] Jonathan R Williford, Brandon B May, and Jeffrey Byrne. Explainable face recognition. In European Conference on
Computer Vision, pages 248–263. Springer, 2020.
[88] Christo Wilson, Avijit Ghosh, Shan Jiang, Alan Mislove, Lewis Baker, Janelle Szary, Kelly Trindel, and Frida Polli. Building and auditing fair algorithms: A case study in candidate screening. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 666–677, 2021.
[89] Matthew Wood et al. Safety first for autonomous driving. https://www.daimler.com/documents/innovation/other/
safety-first-for-automated-driving.pdf, 2019. Accessed: 2021-02-20.
[90] Zihao Xiao, Xianfeng Gao, Chilin Fu, Yinpeng Dong, Wei Gao, Xiaolu Zhang, Jun Zhou, and Jun Zhu. Improving transferability of adversarial patches on face recognition with generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11845–11854, 2021.
[91] XinhuaNet. Precisely detecting desperate emotions, this AI customer service saved a life by alerting help. http:
//www.xinhuanet.com/tech/2020-06/24/c_1126152953.htm, 2020. Accessed: January 19, 2021 (in Chinese). [92] Roman V Yampolskiy. Predicting future ai failures from historic examples. foresight, 2019.
[93] Lei Yang, Xiaohang Zhan, Dapeng Chen, Junjie Yan, Chen Change Loy, and Dahua Lin. Learning to cluster faces on an affinity graph. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
2298–2306, 2019. [94] Yu Yao and Ella Atkins. The smart black box: A value-driven high-bandwidth automotive event data recorder. IEEE
Transactions on Intelligent Transportation Systems, 2020.
[95] Bangjie Yin, Luan Tran, Haoxiang Li, Xiaohui Shen, and Xiaoming Liu. Towards interpretable face recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9348–9357, 2019.
[96] Éloi Zablocki, Hédi Ben-Younes, Patrick Pérez, and Matthieu Cord. Explainability of vision-based autonomous driving systems: Review and challenges. arXiv:2101.05307, 2021.
[97] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khurshid. Deeproad: Gan-based metamorphic testing and input validation framework for autonomous driving systems. In 2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 132–142. IEEE, 2018.
[98] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. Advances in Neural Information Processing Systems, 32, 2019.
[99] Roberto V Zicari, John Brodersen, James Brusseau, Boris Düdder, Timo Eichhorn, Todor Ivanov, Georgios Kararigas, Pedro Kringen, Melissa McCullough, Florian Möslein, et al. Z-inspection®: A process to assess trustworthy ai. IEEE Transactions on Technology and Society, 2021.
[100] Thomas Zielke. Is artificial intelligence ready for standardization? In European Conference on Software Process Improvement, pages 259–274. Springer, 2020.

, Vol. 1, No. 1, Article . Publication date: May 2022.

