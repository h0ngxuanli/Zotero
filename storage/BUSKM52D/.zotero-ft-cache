é¦–å‘äº AutoML
å†™æ–‡ç« 
ç‚¹å‡»æ‰“å¼€è¦çˆ†äº†çš„ä¸»é¡µ
æºç è¯¦è§£Pytorchçš„state_dictå’Œload_state_dict
marsggbo
marsggbo
â€‹
â€‹ å…³æ³¨ä»–
123 äººèµåŒäº†è¯¥æ–‡ç« 

åœ¨ Pytorch ä¸­ä¸€ç§æ¨¡å‹ä¿å­˜å’ŒåŠ è½½çš„æ–¹å¼å¦‚ä¸‹:

 # save torch . save ( model . state_dict (), PATH ) # load model = MyModel ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) model . eval ()  

model.state_dict() å…¶å®è¿”å›çš„æ˜¯ä¸€ä¸ª OrderDict ï¼Œå­˜å‚¨äº†ç½‘ç»œç»“æ„çš„åå­—å’Œå¯¹åº”çš„å‚æ•°ï¼Œä¸‹é¢çœ‹çœ‹æºä»£ç å¦‚ä½•å®ç°çš„ã€‚
state_dict

 # torch.nn.modules.module.py class Module ( object ): def state_dict ( self , destination = None , prefix = '' , keep_vars = False ): if destination is None : destination = OrderedDict () destination . _metadata = OrderedDict () destination . _metadata [ prefix [: - 1 ]] = local_metadata = dict ( version = self . _version ) for name , param in self . _parameters . items (): if param is not None : destination [ prefix + name ] = param if keep_vars else param . data for name , buf in self . _buffers . items (): if buf is not None : destination [ prefix + name ] = buf if keep_vars else buf . data for name , module in self . _modules . items (): if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _state_dict_hooks . values (): hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination  

å¯ä»¥çœ‹åˆ°state_dictå‡½æ•°ä¸­éå†äº†4ä¸­å…ƒç´ ï¼Œåˆ†åˆ«æ˜¯ _paramters , _buffers , _modules å’Œ _state_dict_hooks ,å‰é¢ä¸‰è€…åœ¨ä¹‹å‰çš„ æ–‡ç«  å·²ç»ä»‹ç»åŒºåˆ«ï¼Œæœ€åä¸€ç§å°±æ˜¯åœ¨è¯»å– state_dict æ—¶å¸Œæœ›æ‰§è¡Œçš„æ“ä½œï¼Œä¸€èˆ¬ä¸ºç©ºï¼Œæ‰€ä»¥ä¸åšè€ƒè™‘ã€‚å¦å¤–æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¯»å– Module æ—¶é‡‡ç”¨çš„é€’å½’çš„è¯»å–æ–¹å¼ï¼Œå¹¶ä¸”åå­—é—´ä½¿ç”¨ . åšåˆ†å‰²ï¼Œä»¥æ–¹ä¾¿åé¢ load_state_dict è¯»å–å‚æ•°ã€‚

 class MyModel ( nn . Module ): def __init__ ( self ): super ( MyModel , self ) . __init__ () self . my_tensor = torch . randn ( 1 ) # å‚æ•°ç›´æ¥ä½œä¸ºæ¨¡å‹ç±»æˆå‘˜å˜é‡ self . register_buffer ( 'my_buffer' , torch . randn ( 1 )) # å‚æ•°æ³¨å†Œä¸º buffer self . my_param = nn . Parameter ( torch . randn ( 1 )) self . fc = nn . Linear ( 2 , 2 , bias = False ) self . conv = nn . Conv2d ( 2 , 1 , 1 ) self . fc2 = nn . Linear ( 2 , 2 , bias = False ) self . f3 = self . fc def forward ( self , x ): return x model = MyModel () print ( model . state_dict ()) >>> OrderedDict ([( 'my_param' , tensor ([ - 0.3052 ])), ( 'my_buffer' , tensor ([ 0.5583 ])), ( 'fc.weight' , tensor ([[ 0.6322 , - 0.0255 ], [ - 0.4747 , - 0.0530 ]])), ( 'conv.weight' , tensor ([[[[ 0.3346 ]], [[ - 0.2962 ]]]])), ( 'conv.bias' , tensor ([ 0.5205 ])), ( 'fc2.weight' , tensor ([[ - 0.4949 , 0.2815 ], [ 0.3006 , 0.0768 ]])), ( 'f3.weight' , tensor ([[ 0.6322 , - 0.0255 ], [ - 0.4747 , - 0.0530 ]]))])  

å¯ä»¥çœ‹åˆ°æœ€åçš„ç¡®è¾“å‡ºäº†ä¸‰ç§å‚æ•°ã€‚
load_state_dict

ä¸‹é¢çš„ä»£ç ä¸­æˆ‘ä»¬å¯ä»¥åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†çœ‹ï¼Œ

1. load(self)

è¿™ä¸ªå‡½æ•°ä¼šé€’å½’åœ°å¯¹æ¨¡å‹è¿›è¡Œå‚æ•°æ¢å¤ï¼Œå…¶ä¸­çš„ _load_from_state_dict çš„æºç é™„åœ¨æ–‡æœ«ã€‚

é¦–å…ˆæˆ‘ä»¬éœ€è¦æ˜ç¡® state_dict è¿™ä¸ªå˜é‡è¡¨ç¤ºä½ ä¹‹å‰ä¿å­˜çš„æ¨¡å‹å‚æ•°åºåˆ—ï¼Œè€Œ _load_from_state_dict å‡½æ•°ä¸­çš„ local_state è¡¨ç¤ºä½ çš„ä»£ç ä¸­å®šä¹‰çš„æ¨¡å‹çš„ç»“æ„ã€‚

é‚£ä¹ˆ _load_from_state_dict çš„ä½œç”¨ç®€å•ç†è§£å°±æ˜¯å‡å¦‚æˆ‘ä»¬ç°åœ¨éœ€è¦å¯¹ä¸€ä¸ªåä¸º conv.weight çš„å­æ¨¡å—åšå‚æ•°æ¢å¤ï¼Œé‚£ä¹ˆå°±ä»¥é€’å½’çš„æ–¹å¼å…ˆåˆ¤æ–­ conv æ˜¯å¦åœ¨ state__dict å’Œ local_state ä¸­ï¼Œå¦‚æœä¸åœ¨å°±æŠŠ conv æ·»åŠ åˆ° unexpected_keys ä¸­å»ï¼Œå¦åˆ™é€’å½’çš„åˆ¤æ–­ conv.weight æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœéƒ½å­˜åœ¨å°±æ‰§è¡Œ param.copy_(input_param) ,è¿™æ ·å°±å®Œæˆäº† conv.weight çš„å‚æ•°æ‹·è´ã€‚

2. if strictï¼š

è¿™ä¸ªéƒ¨åˆ†çš„ä½œç”¨æ˜¯åˆ¤æ–­ä¸Šé¢å‚æ•°æ‹·è´è¿‡ç¨‹ä¸­æ˜¯å¦æœ‰ unexpected_keys æˆ–è€… missing_keys ,å¦‚æœæœ‰å°±æŠ¥é”™ï¼Œä»£ç ä¸èƒ½ç»§ç»­æ‰§è¡Œã€‚å½“ç„¶ï¼Œå¦‚æœ strict=False ï¼Œåˆ™ä¼šå¿½ç•¥è¿™äº›ç»†èŠ‚ã€‚

 def load_state_dict ( self , state_dict , strict = True ): missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _metadata = metadata def load ( module , prefix = '' ): local_metadata = {} if metadata is None else metadata . get ( prefix [: - 1 ], {}) module . _load_from_state_dict ( state_dict , prefix , local_metadata , strict , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items (): if child is not None : load ( child , prefix + name + '.' ) load ( self ) if strict : error_msg = '' if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '"{}"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '"{}"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}: \n\t {}' . format ( self . __class__ . __name__ , " \n\t " . join ( error_msgs )))  

    _load_from_state_dict 

 def _load_from_state_dict ( self , state_dict , prefix , local_metadata , strict , missing_keys , unexpected_keys , error_msgs ): for hook in self . _load_state_dict_pre_hooks . values (): hook ( state_dict , prefix , local_metadata , strict , missing_keys , unexpected_keys , error_msgs ) local_name_params = itertools . chain ( self . _parameters . items (), self . _buffers . items ()) local_state = { k : v . data for k , v in local_name_params if v is not None } for name , param in local_state . items (): key = prefix + name if key in state_dict : input_param = state_dict [ key ] # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+ if len ( param . shape ) == 0 and len ( input_param . shape ) == 1 : input_param = input_param [ 0 ] if input_param . shape != param . shape : # local shape should match the one in checkpoint error_msgs . append ( 'size mismatch for {}: copying a param with shape {} from checkpoint, ' 'the shape in current model is {}.' . format ( key , input_param . shape , param . shape )) continue if isinstance ( input_param , Parameter ): # backwards compatibility for serialized parameters input_param = input_param . data try : param . copy_ ( input_param ) except Exception : error_msgs . append ( 'While copying the parameter named "{}", ' 'whose dimensions in the model are {} and ' 'whose dimensions in the checkpoint are {}.' . format ( key , param . size (), input_param . size ())) elif strict : missing_keys . append ( key ) if strict : for key , input_param in state_dict . items (): if key . startswith ( prefix ): input_name = key [ len ( prefix ):] input_name = input_name . split ( '.' , 1 )[ 0 ] # get the name of param/buffer/child if input_name not in self . _modules and input_name not in local_state : unexpected_keys . append ( key )  



MARSGGBOâ™¥åŸåˆ›


å¦‚æœ‰æ„åˆä½œï¼Œæ¬¢è¿ç§æˆ³
é‚®ç®±:marsggbo@foxmail.com
å¾®ä¿¡å…¬ä¼—å·: ã€AutoMLæœºå™¨å­¦ä¹ ã€‘
AutoMLæœºå™¨å­¦ä¹ 




2019-12-20 21:55:21

ç¼–è¾‘äº 2020-01-29 15:31
ã€ŒçœŸè¯šèµèµï¼Œæ‰‹ç•™ä½™é¦™ã€
èµèµ
è¿˜æ²¡æœ‰äººèµèµï¼Œå¿«æ¥å½“ç¬¬ä¸€ä¸ªèµèµçš„äººå§ï¼
PyTorch
TensorLayerï¼ˆæ·±åº¦å­¦ä¹ åº“ï¼‰
TensorFlow å­¦ä¹ 
â€‹ èµåŒ 123 â€‹ â€‹ 4 æ¡è¯„è®º
â€‹ åˆ†äº«
â€‹ å–œæ¬¢ â€‹ æ”¶è— â€‹ ç”³è¯·è½¬è½½
â€‹
è¯„è®ºåƒä¸‡æ¡ï¼Œå‹å–„ç¬¬ä¸€æ¡

4 æ¡è¯„è®º
é»˜è®¤
æœ€æ–°
Taki
Taki
ä¸é”™ï¼
2021-07-23
â€‹ å›å¤ â€‹ 1
ç«¹é¼ å•†äºº
ç«¹é¼ å•†äºº
ä¸é”™ï¼
2022-01-18
â€‹ å›å¤ â€‹ èµ
ç™½æ¨
ç™½æ¨

å¾ˆæ£’ï¼ŒğŸ‘
2022-08-03
â€‹ å›å¤ â€‹ èµ
èµ–å­
èµ–å­
è¿™æ˜¯pytorchä»€ä¹ˆç‰ˆæœ¬ï¼Ÿ
2021-12-17
â€‹ å›å¤ â€‹ èµ
æ–‡ç« è¢«ä»¥ä¸‹ä¸“æ æ”¶å½•

    AutoML
    AutoML
    ä»‹ç»AutoMLç›¸å…³ç ”ç©¶ä»¥åŠæœºå™¨å­¦ä¹ ç­‰åŸºç¡€ï¼Œæ¬¢è¿æŠ•ç¨¿ï¼

æ¨èé˜…è¯»

    æ–‡æ¡£ç¿»è¯‘-PyTorch 1.8 beta feature: torch.fx
    æ–‡æ¡£ç¿»è¯‘-PyTorch 1.8 beta feature: torch.fx
    Gus
    PyTorch 30.ä¸Šä¸‹é‡‡æ ·å‡½æ•°--interpolate
    PyTorch 30.ä¸Šä¸‹é‡‡æ ·å‡½æ•°--interpolate
    ç§‘æŠ€çŒ›å…½ å‘è¡¨äºPytor...
    PyTorch nn.Moduleä¸­çš„self.register_buffer()è§£æ

    PyTorchä¸­å®šä¹‰æ¨¡å‹æ—¶ï¼Œæœ‰æ—¶å€™ä¼šé‡åˆ°self.register_buffer(&#39;name&#39;, Tensor)çš„æ“ä½œï¼Œ è¯¥æ–¹æ³•çš„ä½œç”¨æ˜¯å®šä¹‰ä¸€ç»„å‚æ•°ï¼Œè¯¥ç»„å‚æ•°çš„ç‰¹åˆ«ä¹‹å¤„åœ¨äºï¼šæ¨¡å‹è®­ç»ƒæ—¶ä¸ä¼šæ›´æ–°ï¼ˆå³è°ƒç”¨ optimizer.sâ€¦
    ç­‰å¾…é£ç–¯
    PyTorchä¸­çš„masked_selecté€‰æ‹©å‡½æ•°
    PyTorchä¸­çš„masked_selecté€‰æ‹©å‡½æ•°
    è§¦æ‘¸å£¹ç¼•é˜³å…‰

