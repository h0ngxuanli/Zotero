FLDetector: Defending Federated Learning Against Model Poisoning Attacks via Detecting Malicious Clients

Zaixi Zhang
University of Science and Technology of China zaixi@mail.ustc.edu.cn

Xiaoyu Cao
Duke University xiaoyu.cao@duke.edu

arXiv:2207.09209v4 [cs.CR] 23 Oct 2022

Jinyuan Jia
Duke University jinyuan.jia@duke.edu

Neil Zhenqiang Gong
Duke University neil.gong@duke.edu

ABSTRACT
Federated learning (FL) is vulnerable to model poisoning attacks, in which malicious clients corrupt the global model via sending manipulated model updates to the server. Existing defenses mainly rely on Byzantine-robust or provably robust FL methods, which aim to learn an accurate global model even if some clients are malicious. However, they can only resist a small number of malicious clients. It is still an open challenge how to defend against model poisoning attacks with a large number of malicious clients. Our FLDetector addresses this challenge via detecting malicious clients. FLDetector aims to detect and remove majority of the malicious clients such that a Byzantine-robust or provably robust FL method can learn an accurate global model using the remaining clients. Our key observation is that, in model poisoning attacks, the model updates from a client in multiple iterations are inconsistent. Therefore, FLDetector detects malicious clients via checking their model-updates consistency. Roughly speaking, the server predicts a clientâ€™s model update in each iteration based on historical model updates, and flags a client as malicious if the received model update from the client and the predicted model update are inconsistent in multiple iterations. Our extensive experiments on three benchmark datasets show that FLDetector can accurately detect malicious clients in multiple state-of-the-art model poisoning attacks and adaptive attacks tailored to FLDetector. After removing the detected malicious clients, existing Byzantine-robust FL methods can learn accurate global models.
CCS CONCEPTS
â€¢ Security and privacy â†’ Intrusion/anomaly detection and malware mitigation; â€¢ Computing methodologies â†’ Distributed artificial intelligence.
KEYWORDS
Federated Learning; Model Poisoning Attack; Malicious Client Detection; Anomaly Detection
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD â€™22, August 14â€“18, 2022, Washington, DC, USA Â© 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-9385-0/22/08. . . $15.00 https://doi.org/10.1145/3534678.3539231

ACM Reference Format: Zaixi Zhang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. 2022. FLDetector: Defending Federated Learning Against Model Poisoning Attacks via Detecting Malicious Clients. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™22), August 14â€“18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3534678.3539231
1 INTRODUCTION
Federated Learning (FL) [17, 21] is an emerging learning paradigm over decentralized data. Specifically, multiple clients (e.g., smartphones, IoT devices, edge data centers) jointly learn a machine learning model (called global model) without sharing their local training data with a cloud server. Roughly speaking, FL iteratively performs the following three steps: the server sends the current gloabl model to the selected clients; each selected client finetunes the received global model on its local training data and sends the model update back to the server; the server aggregates the received model updates according to some aggregation rule and updates the global model.
However, due to its distributed nature, FL is vulnerable to model poisoning attacks [1â€“3, 8, 11, 20], in which the attacker-controlled malicious clients corrupt the global model via sending manipulated model updates to the server. The attacker-controlled malicious clients can be injected fake clients [8] or genuine clients compromised by the attacker [1â€“3, 11, 20]. Based on the attack goals, model poisoning attacks can be generally classified into untargeted and targeted. In the untargeted model poisoning attacks [8, 11], the corrupted global model indiscriminately makes incorrect predictions for a large number of testing inputs. In the targeted model poisoning attacks [1â€“3, 20], the corrupted global model makes attacker-chosen, incorrect predictions for attacker-chosen testing inputs, while the global modelâ€™s accuracy on other testing inputs is unaffected. For instance, the attacker-chosen testing inputs could be testing inputs embedded with an attacker-chosen trigger, which are also known as backdoor attacks.
Existing defenses against model poisoning attacks mainly rely on Byzantine-robust FL methods [4, 7, 10, 22] (e.g., Krum [4] and FLTrust [7]) or provably robust FL methods [9]. These methods aim to learn an accurate global model even if some clients are malicious and send arbitrary model updates to the server. Byzantine-robust FL methods can theoretically bound the change of the global model parameters caused by malicious clients, while provably robust FL methods can guarantee a lower bound of testing accuracy under

KDD â€™22, August 14â€“18, 2022, Washington, DC, USA

Zaixi Zhang et al.

malicious clients. However, they are only robust to a small number of malicious clients [4, 9, 22] or require a clean, representative validation dataset on the server [7]. For instance, Krum can theoretically tolerate at most âŒŠ ğ‘›âˆ’2 âŒ‹ malicious clients. FLTrust [7] is robust
2
against a large number of malicious clients but it requires the server to have access to a clean validation dataset whose distribution does not diverge too much from the overall training data distribution. As a result, in a typical FL scenario where the server does not have such a validation dataset, the global model can still be corrupted by a large number of malicious clients.
Li et al. [16] tried to detect malicious clients in model poisoning attacks. Their key assumption is that the model updates from malicious clients are statistically distinguishable with those from benign clients. In particular, they proposed to use a variational autoencoder (VAE) to capture model-updates statistics. Specifically, VAE assumes the server has access to a clean validation dataset that is from the overall training data distribution. Then, the server trains a model using the clean validation dataset. The model updates obtained during this process are used to train a VAE, which takes a model update as input and outputs a reconstructed model update. Finally, the server uses the trained VAE to detect malicious clients in FL. Specifically, if a clientâ€™s model updates lead to high reconstruction errors in the VAE, then the server flags the client as malicious. However, this detection method suffers from two key limitations: 1) it requires the server to have access to a clean validation dataset, and 2) it is ineffective when the malicious clients and benign clients have statistically indistinguishable model updates.
In this work, we propose a new malicious-client detection method called FLDetector. First, FLDetector addresses the limitations of existing detection methods such as the requirement of clean validation datasets. Moreover, FLDetector can be combined with Byzantinerobust FL methods, i.e., after FLDetector detects and removes majority of the malicious clients, Byzantine-robust FL methods can learn accurate global models. Our key intuition is that, benign clients calculate their model updates based on the FL algorithm and their local training data, while malicious clients craft the model updates instead of following the FL algorithm. As a result, the model updates from a malicious client are inconsistent in different iterations. Based on the intuition, FLDetector detects malicious clients via checking their model-updates consistency.
Specifically, we propose that the server predicts each clientâ€™s model update in each iteration based on historical model updates using the Cauchy mean value theorem. Our predicted model update for a client is similar to the clientâ€™s actual model update if the client follows the FL algorithm. In other words, our predicted model update for a benign (or malicious) client is similar (or dissimilar) to the model update that the client sends to the server. We use Euclidean distance to measure the similarity between a predicted model update and the received model update for each client in each iteration. Moreover, we define a suspicious score for each client, which is dynamically updated in each iteration. Specifically, a clientâ€™s suspicious score in iteration ğ‘¡ is the average of such Euclidean distances in the previous ğ‘ iterations. Finally, we leverage ğ‘˜-means with Gap statistics based on the clientsâ€™ suspicious scores to detect malicious clients in each iteration. In particular, if the clients can be grouped into more than one cluster based on the suspicious scores and Gap statistics in a certain iteration, we group

the clients into two clusters using ğ‘˜-means and classify the clients in the cluster with larger average suspicious scores as malicious.
We evaluate FLDetector on three benchmark datasets as well as one untargeted model poisoning attack [11], three targeted model poisoning attacks [1, 2, 20], and adaptive attacks tailored to FLDetector. Our results show that, for the untargeted model poisoning attack, FLDetector outperforms the baseline detection methods; for the targeted model poisoning attacks, FLDetector outperforms the baseline detection methods in most cases and achieves comparable detection accuracy in the remaining cases; and FLDetector is effective against adaptive attacks. Moreover, even if FLDetector misses a small fraction of malicious clients, after removing the clients detected as malicious, Byzantine-robust FL methods can learn as accurate global models as when there are no malicious clients.
In summary, we make the following contributions.
â€¢ We perform a systematic study on defending FL against model poisoning attacks via detecting malicious clients.
â€¢ We propose FLDetector, an unsupervised method, to detect malicious clients via checking the consistency between the received and predicted model updates of clients.
â€¢ We empirically evaluate FLDetector against multiple stateof-the-art model poisoning attacks and adaptive attacks on three benchmark datasets.
2 RELATED WORK 2.1 Model Poisoning Attacks against FL
Model poisoning attacks generally can be untargeted [8, 11, 18] and targeted [1â€“3, 20]. Below, we review one state-of-the-art untargeted attack and three targeted attacks.
Untargeted Model Poisoning Attack: Untargeted model poisoning attacks aim to corrupt the global model such that it has a low accuracy for indiscriminate testing inputs. Fang et al. [11] proposed an untargeted attack framework against FL. Generally speaking, the framework formulates untargeted attack as an optimization problem, whose solutions are the optimal crafted model updates on the malicious clients that maximize the difference between the aggregated model updates before and after the attack. The framework can be applied to any aggregation rule, e.g., they have shown that the framework can substantially reduce the testing accuracy of the global models learnt by FedAvg [17], Krum [4], Trimmed-Mean [22], and Median [22].
Scaling Attack, Distributed Backdoor Attack, and A Little is Enough Attack: In these targeted model poisoning attacks (also known as backdoor attacks), the corrupted global model predicts an attacker-chosen label for any testing input embedded with an attacker-chosen trigger. For instance, the trigger could be a patch located at the bottom right corner of an input image. Specifically, in Scaling Attack [1], the attacker makes duplicates of the local training examples on the malicious clients, embeds the trigger to the duplicated training inputs, and assigns an attacker-chosen label to them. Then, model updates are computed based on the local training data augmented by such duplicated training examples. Furthermore, to amplify the impact of the model updates, the malicious clients further scale them up by a factor before reporting them to the server. In Distributed Backdoor Attack (DBA) [20], the attacker

FLDetector: Defending Federated Learning Against Model Poisoning Attacks via Detecting Malicious Clients

KDD â€™22, August 14â€“18, 2022, Washington, DC, USA

decomposes the trigger into separate local patterns and embeds them into the local training data of different malicious clients. In A Little is Enough Attack [2], the model updates on the malicious clients are first computed following the Scaling Attack [1]. Then, the attacker crops the model updates to be in certain ranges so that the Byzantine-robust aggregation rules fail to eliminate their malicious effects.

2.2 Byzantine-Robust FL Methods
Roughly speaking, Byzantine-robust FL methods view clientsâ€™ model updates as high dimensional vectors and apply robust methods to estimate the aggregated model update. Next, we review several popular Byzantine-robust FL methods.
Krum [4]: Krum tries to find a single model update among the clientsâ€™ model updates as the aggregated model update in each iteration. The chosen model update is the one with the closest Euclidean distances to the nearest ğ‘› âˆ’ ğ‘˜ âˆ’ 2 model updates. Trimmed-Mean and Median [22]: Trimmed-Mean and Median are coordinate-wise aggregation rules that aggregate each coordinate of the model update separately. For each coordinate, TrimmedMean first sorts the values of the corresponding coordinates in the clientsâ€™ model updates. After removing the largest and the smallest ğ‘˜ values, Trimmed-Mean calculates the average of the remaining ğ‘› âˆ’ 2ğ‘˜ values as the corresponding coordinate of the aggregated model update. Median calculates the median value of the corresponding coordinates in all model updates and treats it as the corresponding coordinate of the aggregated model update.
FLTrust [7]: FLTrust leverages an additional validation dataset on the server. In particular, a local model update has a lower trust score if its update direction deviates more from that of the server model update calculated based on the validation dataset. However, it is nontrivial to collect a clean validation dataset and FLTrust has poor performance when the distribution of validation dataset diverges substantially from the overall training dataset.

3 PROBLEM FORMULATION

We consider a typical FL setting in which ğ‘› clients collaboratively

train a global model maintained on a cloud server. We suppose

that each client has a local training dataset ğ·ğ‘–, ğ‘– = 1, 2, Â· Â· Â· , ğ‘›

and we optimal

use ğ· global

m=odâˆªeğ‘›ğ‘–l=ğ‘¤1ğ·âˆ— ğ‘–istoa

denote the joint training data. The solution to the optimization problem:

ğ‘¤âˆ—

=

arg

min
ğ‘¤

âˆ‘ï¸ğ‘›
ğ‘– =1

ğ‘“

(ğ·ğ‘–, ğ‘¤),

where

ğ‘“

(ğ·ğ‘–, ğ‘¤)

is

the

loss

for

client

ğ‘–â€™s local training data. The FL process starts with an initialized

global model ğ‘¤0. At the beginning of each iteration ğ‘¡, the server

first sends the current global model ğ‘¤ğ‘¡ to the clients or a subset of

them.

A

client

ğ‘–

then

computes

the

gradient

ğ‘”ğ‘¡
ğ‘–

of

its

loss

ğ‘“

(ğ·ğ‘–, ğ‘¤ğ‘¡ )

with

respect

to ğ‘¤ğ‘¡

and

sends ğ‘”ğ‘¡
ğ‘–

back

to

the

server,

where ğ‘”ğ‘¡
ğ‘–

is

the

model update from client ğ‘– in the ğ‘¡th iteration. Formally, we have:

ğ‘¡
ğ‘”
ğ‘–

=

âˆ‡ğ‘“

(ğ·ğ‘–, ğ‘¤ğ‘¡ ).

(1)

We note that client ğ‘– can also use stochastic gradient descent (SGD) instead of gradient descent, perform SGD multiple steps locally, and send the accumulated gradients back to the server as model update. However, we assume a client performs the standard gradient descent for one step for simplicity.

After receiving the clientsâ€™ model updates, the server computes a global model update ğ‘”ğ‘¡ via aggregating the clientsâ€™ model updates

based on some aggregation rule. Then, the server updates the global model using the global model update, i.e., ğ‘¤ğ‘¡+1 = ğ‘¤ğ‘¡ âˆ’ ğ›¼ğ‘”ğ‘¡ , where ğ›¼ is the global learning rate. Different FL methods essentially use

different aggregation rules.

Attack model: We follow the attack settings in previous works [1, 2, 8, 11, 20]. Specifically, an attacker controls ğ‘š malicious clients,

which can be fake clients injected by the attacker or genuine ones

compromised by the attacker. However, the server is not compro-

mised. The attacker has the following background knowledge about

the FL system: local training data and model updates on the ma-

licious clients, loss function, and learning rate. In each iteration

ğ‘¡, each benign client calculates and reports the true model update

ğ‘”ğ‘¡
ğ‘–

=

âˆ‡ğ‘“ (ğ·ğ‘–, ğ‘¤ğ‘¡ ), while a malicious client sends carefully crafted

model

update

(i.e., ğ‘”ğ‘¡
ğ‘–

â‰ 

âˆ‡ğ‘“

(ğ·ğ‘–, ğ‘¤ğ‘¡ ))

to

the

server.

Problem definition: We aim to design a malicious-client detection

method in the above FL setting. In each iteration ğ‘¡, the detection

method takes clientsâ€™ model updates in the current and previous

iterations as an input and classifies each client to be benign or

malicious. When at least one client is classified as malicious by our

method in a certain iteration, the server stops the FL process, re-

moves the clients detected as malicious, and restarts the FL process

on the remaining clients. Our goal is to detect majority of malicious

clients as early as possible. After detecting and removing majority

of malicious clients, Byzantine-robust FL methods can learn accu-

rate global models since they are robust against the small number

of malicious clients that miss detection.

4 FLDETECTOR

4.1 Model-Updates Consistency
A benign client ğ‘– calculates its model update ğ‘”ğ‘¡ in the ğ‘¡th itera-
ğ‘–
tion according to Equation 1. Based on the Cauchy mean value
theorem [14], we have the following:

ğ‘¡
ğ‘”
ğ‘–

=

ğ‘¡ âˆ’1
ğ‘”
ğ‘–

+

Hğ‘–ğ‘¡

Â·

(ğ‘¤ğ‘¡

âˆ’ ğ‘¤ğ‘¡âˆ’1),

(2)

where

Hğ‘¡
ğ‘–

=

âˆ«1
0

Hğ‘–

(ğ‘¤ğ‘¡

âˆ’1

+ ğ‘¥ (ğ‘¤ğ‘¡

âˆ’ ğ‘¤ğ‘¡âˆ’1))ğ‘‘ğ‘¥

is

an

integrated

Hes-

sian for client ğ‘– in iteration ğ‘¡, ğ‘¤ğ‘¡ is the global model in iteration

ğ‘¡, and ğ‘¤ğ‘¡âˆ’1 is the global model in iteration ğ‘¡ âˆ’ 1. Equation 2 en-

codes the consistency between client ğ‘–â€™s model updates ğ‘”ğ‘¡ and ğ‘”ğ‘¡âˆ’1.

ğ‘–

ğ‘–

However, the integrated Hessian Hğ‘¡ is hard to compute exactly.
ğ‘–

In our work, we use a L-BFGS algorithm [5] to approximate in-

tegrated Hessian. To be more efficient, we approximate a single
integrated Hessian HË† ğ‘¡ in each iteration ğ‘¡, which is used for all clients. Specifically, we denote by Î”ğ‘¤ğ‘¡ = ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1 the globalmodel difference in iteration ğ‘¡, and we denote by Î”ğ‘”ğ‘¡ = ğ‘”ğ‘¡ âˆ’ ğ‘”ğ‘¡âˆ’1
the global-model-update difference in iteration ğ‘¡, where the global

model update is aggregated from the clientsâ€™ model updates. We de-
note by Î”ğ‘Šğ‘¡ = {Î”ğ‘¤ğ‘¡âˆ’ğ‘ , Î”ğ‘¤ğ‘¡âˆ’ğ‘ +1, Â· Â· Â· , Î”ğ‘¤ğ‘¡âˆ’1} the global-model differences in the past ğ‘ iterations, and we denote by Î”ğºğ‘¡ = {Î”ğ‘”ğ‘¡âˆ’ğ‘ , Î”ğ‘”ğ‘¡âˆ’ğ‘ +1, Â· Â· Â· , Î”ğ‘”ğ‘¡âˆ’1} the global-model-update differences
in the past ğ‘ iterations in iteration ğ‘¡. Then, based on the L-BFGS algorithm, we can estimate HË† ğ‘¡ using Î”ğ‘Šğ‘¡ and Î”ğºğ‘¡ . For simplicity, we denote by HË† ğ‘¡ = L-BFGS(Î”ğ‘Šğ‘¡ , Î”ğºğ‘¡ ). Algorithm 1 shows the
specific implementation of L-BFGS algorithm in the experiments.

KDD â€™22, August 14â€“18, 2022, Washington, DC, USA

The input to L-BFGS are v = ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1, Î”ğ‘Šğ‘¡ , and Î”ğºğ‘¡ . The output of L-BFGS algorithm is the projection of the Hessian matrix in the
direction of ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1. Based on the estimated Hessian HË† ğ‘¡ , we predict a client ğ‘–â€™s model
update in iteration ğ‘¡ as follows:

ğ‘¡
ğ‘”Ë†
ğ‘–

=

ğ‘¡
ğ‘”

âˆ’1

ğ‘–

+ HË† ğ‘¡ (ğ‘¤ğ‘¡

âˆ’ ğ‘¤ğ‘¡âˆ’1),

(3)

where ğ‘”Ë†ğ‘¡ is the predicted model update for client ğ‘– in iteration ğ‘¡.
ğ‘–
When the L-BFGS algorithm estimates the integrated Hessian accurately, the predicted model update ğ‘”Ë†ğ‘¡ is close to the actual model
ğ‘–
update ğ‘”ğ‘¡ for a benign client ğ‘–. In particular, if the estimated Hessian
ğ‘–
is exactly the same as the integrated Hessian, then the predicted model update equals the actual model update for a benign client. However, no matter whether the integrated Hessian is estimated accurately or not, the predicted model update would be different from the model update sent by a malicious client. In other words, the predicted model update and the received one are consistent for benign clients but inconsistent for malicious clients, which we leverage to detect malicious clients.

4.2 Detecting Malicious Clients

Suspicious score for a client: Based on the model-updates consis-

tency discussed above, we assign a suspicious score for each client.

Specifically, we measure the consistency between a predicted model

update ğ‘”Ë†ğ‘¡ and a received model update ğ‘”ğ‘¡ using their Euclidean dis-

ğ‘–

ğ‘–

tance. We denote by ğ‘‘ğ‘¡ the vector of such ğ‘› Euclidean distances for

the ğ‘› clients in iteration ğ‘¡, i.e., ğ‘‘ğ‘¡

=

[

âˆ¥ğ‘”Ë†ğ‘¡
1

âˆ’ğ‘”ğ‘¡
1

âˆ¥

2,

âˆ¥ğ‘”Ë†ğ‘¡
2

âˆ’ğ‘”ğ‘¡
2

âˆ¥2,

Â·

Â·

Â·

, âˆ¥ğ‘”Ë†ğ‘¡ âˆ’
ğ‘›

ğ‘”ğ‘›ğ‘¡ âˆ¥2]. We normalize the vector ğ‘‘ğ‘¡ as ğ‘‘Ë†ğ‘¡ = ğ‘‘ğ‘¡ /âˆ¥ğ‘‘ğ‘¡ âˆ¥1. We use such

normalization to incorporate the model-updates consistency varia-

tions across different iterations. Finally, our suspicious score ğ‘ ğ‘¡ for
ğ‘–

client ğ‘– in iteration ğ‘¡ is the clientâ€™s average normalized Euclidean

distance in the past ğ‘

iterations, i.e., ğ‘ ğ‘¡
ğ‘–

=

1 ğ‘

âˆ‘ï¸ğ‘ âˆ’1
ğ‘Ÿ =0

ğ‘‘Ë†ğ‘–ğ‘¡

âˆ’ğ‘Ÿ

.

We

call

ğ‘

window size.

Unsupervised detection via ğ‘˜-means: In iteration ğ‘¡, we perform

malicious-clients detection based on the clientsâ€™ suspicious scores

ğ‘ ğ‘¡ ,
1

ğ‘ ğ‘¡ ,
2

Â·

Â·

Â·

,

ğ‘ ğ‘›ğ‘¡ .

Specifically,

we

cluster

the

clients

based

on

their

sus-

picious

scores

ğ‘ ğ‘¡, ğ‘ ğ‘¡,
12

Â·

Â·

Â·

,

ğ‘ ğ‘›ğ‘¡ ,

and

we

use

the

Gap

statistics

[19]

to

determine the number of clusters. If the clients can be grouped

into more than 1 cluster based on the Gap statistics, then we use ğ‘˜-means to divide the clients into 2 clusters based on their suspi-

cious scores. Finally, the clients in the cluster with larger average

suspicious score are classified as malicious. When at least one client

is classified as malicious in a certain iteration, the detection fin-

ishes, and the server removes the clients classified as malicious and

restarts the training.

Algorithm 2 shows the pseudo codes of Gap statistics algorithm. The input to Gap statistics are the vectors of suspicious scores ğ‘ ğ‘¡ , the number of sampling ğµ, the number of maximum clusters

ğ¾, and the number of clients ğ‘›. The output of Gap statistics is

the number of clusters ğ¾. Generally, Gap statistics compares the

change in within-cluster dispersion with that expected under a

reference null distribution, i.e., uniform distribution, to determine

the number of clusters. The computation of the gap statistic involves the following steps: 1) Vary the number of clusters ğ‘˜ from 1 to ğ¾ and cluster the suspicious scores with ğ‘˜-means. Calculate ğ‘Šğ‘˜ =

Zaixi Zhang et al.

âˆ‘ï¸ğ‘˜
ğ‘– =1

âˆ‘ï¸
ğ‘¥ğ‘—

âˆˆğ¶ğ‘–

âˆ¥ğ‘¥ ğ‘— âˆ’ğœ‡ğ‘– âˆ¥2. 2) Generate ğµ

reference data

sets

and cluster

each of them with ğ‘˜-means. Compute the estimated gap statistics

ğºğ‘ğ‘ (ğ‘˜)

=

1 ğµ

âˆ‘ï¸ğµ
ğ‘– =1

ğ‘™ğ‘œğ‘”(ğ‘Š âˆ— )
ğ‘˜ğ‘

âˆ’

ğ‘™ğ‘œğ‘”(ğ‘Šğ‘˜ ).

3)

Compute the

standard

deviation ğ‘ ğ‘‘ (ğ‘˜)

=

(1
ğµ

âˆ‘ï¸ğµ
ğ‘– =1

(ğ‘™ğ‘œğ‘”(ğ‘Š âˆ—
ğ‘˜ğ‘

))

âˆ’

ğ‘¤

â€²)

2

)

1 2

and

define ğ‘ ğ‘˜+1

=

âˆšï¸‚

1+ğµ
ğ‘ ğ‘‘

(ğ‘˜ ) .

4)

Choose

the

number

of

clusters

ğ‘˜Ë†

as

the

smallest

ğ‘˜

ğµ

such that ğºğ‘ğ‘ (ğ‘˜) âˆ’ ğºğ‘ğ‘ (ğ‘˜ + 1) + ğ‘ ğ‘˜+1 â‰¥ 0. If there are more than

one cluster, the attack detection ğ¹ğ‘™ğ‘ğ‘” is set to positive because there

are outliers in the suspicious scores.

Algorithm 3 summarizes the algorithm of FLDetector.

4.3 Complexity Analysis

To compute the estimated Hessian, the server needs to save the

global-model differences and global-model-update differences in the

latest ğ‘ iterations. Therefore, the storage overhead of FLDetector for the server is ğ‘‚ (ğ‘ ğ‘), where ğ‘ is the number of parameters

in the global model. Moreover, according to [5], the complexity of estimating the Hessian HË† ğ‘¡ using L-BFGS and computing the Hessian vector product HË† ğ‘¡ (ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1) is ğ‘‚ (ğ‘ 3 + 6ğ‘ ğ‘) in each iteration. The complexity of calculating the suspicious scores is
ğ‘‚ (2ğ‘›ğ‘ +ğ‘ğ‘›) in each iteration, where ğ‘› is the number of clients. The total complexity of Gap statistics and ğ‘˜-means is ğ‘‚ (ğ¾ğµğ‘›2) where

ğ¾ and ğµ are the number of maximum clusters and sampling in Gap

statistics. Therefore, the total time complexity of FLDetector in each

iteration

is

ğ‘‚

(ğ‘

3

+

ğ¾

2
ğµğ‘›

+

(6ğ‘

+

2ğ‘›)ğ‘

+

ğ‘

ğ‘›).

Typically,

ğ¾

,

ğµ,

ğ‘›,

and

ğ‘ are much smaller than ğ‘. Thus, the time complexity of FLDetector

for the server is roughly linear to the number of parameters in the

global model in each iteration. We note that the server is powerful

in FL, so the storage and computation overhead of FLDetector for

the server is acceptable. As for the clients, FLDetector does not

incur extra computation and communication overhead.

4.4 Theoretical Analysis on Suspicious Scores

We compare the suspicious scores of benign and malicious clients theoretically. We first describe the definition of ğ¿-smooth gradient, which is widely used for theoretical analysis on machine learning.

Definition 4.1. We say a clientâ€™s loss function is ğ¿-smooth if we have the following inequality for any ğ’˜ and ğ’˜ â€²:

âˆ¥âˆ‡ğ‘“ (ğ·ğ‘–, ğ’˜) âˆ’ âˆ‡ğ‘“ (ğ·ğ‘–, ğ’˜ â€²) âˆ¥ â‰¤ ğ¿âˆ¥ğ’˜ âˆ’ ğ’˜ â€²âˆ¥,

(4)

where ğ‘“ (ğ·ğ‘–, ğ‘¤) is the clientâ€™s loss function and âˆ¥ Â· âˆ¥ represents â„“2 norm of a vector.

Theorem 1. Suppose the gradient of each clientâ€™s loss function is

ğ¿-smooth, FedAvg is used as the aggregation rule, the clientsâ€™ local

training datasets are iid, the learning rate ğ›¼ satisfies ğ›¼

<

1 (ğ‘ +2)ğ¿

(ğ‘

is

the window size). Suppose the malicious clients perform an untargeted

model poisoning attack in each iteration by reversing the true model

updates as the poisoning ones, i.e., each malicious client ğ‘– sends âˆ’ğ‘”ğ‘¡
ğ‘–
to the server in each iteration ğ‘¡. Then we have the expected suspicious

score of a benign client is smaller than that of a malicious client in

each iteration ğ‘¡. Formally, we have the following inequality:

E(ğ‘ ğ‘–ğ‘¡ ) < E(ğ‘ ğ‘¡ğ‘— ), âˆ€ğ‘– âˆˆ B, âˆ€ğ‘— âˆˆ M,

(5)

where the expectation E is taken with respect to the randomness in the clientsâ€™ local training data, B is the set of benign clients, and M is the set of malicious clients.

FLDetector: Defending Federated Learning Against Model Poisoning Attacks via Detecting Malicious Clients

KDD â€™22, August 14â€“18, 2022, Washington, DC, USA

Algorithm 1 L-BFGS to Compute Hessian Vector Product

Input: Global-model differences Î”ğ‘Šğ‘¡ = {Î”ğ‘¤ğ‘¡âˆ’ğ‘ , Î”ğ‘¤ğ‘¡âˆ’ğ‘ +1, Â· Â· Â· , Î”ğ‘¤ğ‘¡âˆ’1}, global-model-update differences Î”ğºğ‘¡ = {Î”ğ‘”ğ‘¡âˆ’ğ‘ , Î”ğ‘”ğ‘¡âˆ’ğ‘ +1, Â· Â· Â· , Î”ğ‘”ğ‘¡âˆ’1}, vector v = ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1, and window size ğ‘ Output: Hessian vector product HË† ğ‘¡ v

1: Compute Î”ğ‘Šğ‘¡ğ‘‡ Î”ğ‘Šğ‘¡

2:

Compute

Î”ğ‘Š ğ‘‡
ğ‘¡

Î”ğºğ‘¡

,

get

its

diagonal

matrix

ğ·ğ‘¡

and its lower triangular submatrix ğ¿ğ‘¡

3: Compute ğœ = Î”ğ‘”ğ‘‡ğ‘¡âˆ’1Î”ğ‘¤ğ‘¡âˆ’1/(Î”ğ‘¤ğ‘¡ğ‘‡âˆ’1Î”ğ‘¤ğ‘¡âˆ’1)

4:

Compute

the

Cholesky

factorization

for

ğœÎ”ğ‘Š ğ‘‡
ğ‘¡

Î”ğ‘Šğ‘¡

+

ğ¿ğ‘¡

ğ·ğ‘¡

ğ¿ğ‘‡
ğ‘¡

to

get

ğ½ğ‘¡

ğ½ğ‘‡
ğ‘¡

5:

[ï¸„ Compute ğ‘ =

âˆ’ğ· 1/2
ğ‘¡

0

ğ·âˆ’1/2ğ¿ğ‘‡ ]ï¸„ âˆ’1 [ï¸„
ğ‘¡ğ‘¡

1/2
ğ·
ğ‘¡

ğ½ğ‘‡
ğ‘¡

ğ· âˆ’1/2 ğ¿ğ‘‡
ğ‘¡ğ‘¡

]ï¸„ âˆ’1

0

[ï¸ƒ

Î”ğºğ‘‡
ğ‘¡

v

]ï¸ƒ

ğ½ğ‘¡

ğœ

Î”ğ‘Š ğ‘‡
ğ‘¡

v

6: return ğœv âˆ’ [ï¸ Î”ğºğ‘¡

ğœ Î”ğ‘Šğ‘¡

]ï¸ ğ‘

Algorithm 2 Gap Statistics

Input: Clientsâ€™ suspicious scores ğ‘ ğ‘¡ , number of sampling ğµ,

maximum number of clusters ğ¾, and number of clients ğ‘›.

Output: Number of clusters ğ‘˜.

for ğ‘˜ = 1,2, Â· Â· Â· , ğ¾ do

Apply linear transformation on ğ‘ ğ‘¡ so that the minimum of

ğ‘ ğ‘¡ equals 0 and the maximum of ğ‘ ğ‘¡ equals 1.

Apply ğ‘˜-means on the suspicious scores to get clusters {ğ¶ğ‘– }

and means {ğœ‡ğ‘– }.

ğ‘‰ğ‘˜

=

âˆ‘ï¸ğ‘˜
ğ‘– =1

âˆ‘ï¸
ğ‘¥ ğ‘— âˆˆğ¶ğ‘–

âˆ¥ğ‘¥ ğ‘—

âˆ’

ğœ‡ğ‘– âˆ¥2

for ğ‘ = 1,2, Â· Â· Â· , ğµ do

Sample ğ‘› points uniformly in [0,1]

Perform ğ‘˜-means and calculate

ğ‘‰âˆ—
ğ‘˜ğ‘

=

âˆ‘ï¸ğ‘˜
ğ‘– =1

âˆ‘ï¸
ğ‘¥ ğ‘—ğ‘ âˆˆğ¶ğ‘–ğ‘

âˆ¥ğ‘¥ ğ‘—ğ‘

âˆ’

ğœ‡ğ‘–ğ‘ âˆ¥2

end for

ğºğ‘ğ‘ (ğ‘˜)

=

1 ğµ

âˆ‘ï¸ğµ
ğ‘– =1

ğ‘™ğ‘œğ‘”

(ğ‘‰ âˆ—
ğ‘˜ğ‘

)

âˆ’ ğ‘™ğ‘œğ‘”(ğ‘‰ğ‘˜ )

â€²
ğ‘£

=

1 ğµ

âˆ‘ï¸ğµ
ğ‘– =1

ğ‘™ğ‘œğ‘”

(ğ‘‰ âˆ—
ğ‘˜ğ‘

)

ğ‘ ğ‘‘ (ğ‘˜)

=

(

1 ğµ

âˆ‘ï¸ğµ
ğ‘– =1

(ğ‘™ğ‘œğ‘”

(ğ‘‰ âˆ—
ğ‘˜ğ‘

))

âˆ’

ğ‘£ â€²)2)

1 2

âˆšï¸‚

â€²
ğ‘ 

=

1+ğµ ğ‘ ğ‘‘ (ğ‘˜)

ğ‘˜

ğµ

end for

ğ‘˜Ë† = smallest ğ‘˜ such that ğºğ‘ğ‘ (ğ‘˜) âˆ’ ğºğ‘ğ‘ (ğ‘˜ + 1) + ğ‘ ğ‘˜â€² +1 â‰¥ 0.

return ğ‘˜Ë†

Proof. Our idea is to bound the difference between predicted

model updates and the received ones from benign clients. Appendix

shows our detailed proof.

â–¡

4.5 Adaptive Attacks

When the attacker knows that our FLDetector is used to detect malicious clients, the attacker can adapt its attack to FLDetector to evade detection. Therefore, we design and evaluate adaptive attacks to FLDetector. Specifically, we formulate an adaptive attack by adding an extra term to regularize the loss function used to perform existing attacks. Our regularization term measures the Euclidean distance between a predicted model update and a local model update. Formally, a malicious client ğ‘– solves the following optimization problem to perform an adaptive attack in iteration ğ‘¡:

min ğœ†Lğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜
ğ‘”ğ‘¡

+

(1

âˆ’ ğœ†) âˆ¥ğ‘”ğ‘¡
ğ‘–

âˆ’

(ğ‘”ğ‘¡ âˆ’1
ğ‘–

+

HË† ğ‘–ğ‘¡ (ğ‘¤ğ‘¡

âˆ’ ğ‘¤ğ‘¡âˆ’1)) âˆ¥,

(6)

ğ‘–

Algorithm 3 FLDetector

Input: Total training iterations ğ¼ğ‘¡ğ‘’ğ‘Ÿ and window size ğ‘ . Output: Detected malicious clients or none.

1: for ğ‘¡ = 1, 2, Â· Â· Â· , ğ¼ğ‘¡ğ‘’ğ‘Ÿ do 2: HË† ğ‘¡ = L-BFGS(Î”ğ‘Šğ‘¡ , Î”ğºğ‘¡ ).

3: for ğ‘– = 1, 2, Â· Â· Â· , ğ‘› do

4:

ğ‘”Ë†ğ‘¡
ğ‘–

= ğ‘”ğ‘¡âˆ’1
ğ‘–

+

HË† ğ‘¡ (ğ‘¤ğ‘¡

âˆ’ ğ‘¤ğ‘¡âˆ’1).

5: end for

6:

ğ‘‘ğ‘¡

=

[ âˆ¥ğ‘”Ë†ğ‘¡
1

âˆ’

ğ‘”ğ‘¡
1

âˆ¥2,

âˆ¥ğ‘”Ë†ğ‘¡
2

âˆ’

ğ‘”ğ‘¡
2

âˆ¥2,

Â·

Â·

Â·

, âˆ¥ğ‘”Ë†ğ‘›ğ‘¡

âˆ’ ğ‘”ğ‘›ğ‘¡ âˆ¥2].

7: ğ‘‘Ë†ğ‘¡ = ğ‘‘ğ‘¡ /âˆ¥ğ‘‘ğ‘¡ âˆ¥1.

8:

ğ‘ ğ‘¡
ğ‘–

=

1 ğ‘

âˆ‘ï¸ğ‘ âˆ’1
ğ‘Ÿ =0

ğ‘‘Ë†ğ‘–ğ‘¡

âˆ’ğ‘Ÿ

.

9: Determine the number of clusters ğ‘˜ by Gap statistics.

10: if ğ‘˜ > 1 then

11: Perform ğ‘˜-means clustering based on the suspicious scores with ğ‘˜ = 2.

12: return The clients in the cluster with larger average

suspicious score as malicious.

13: end if 14: end for 15: return None.

where Lğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ is the loss function used to perform existing at-

tacks [1, 2, 11, 20], ğ‘”ğ‘¡ is the poisoning local model update on mali-
ğ‘–

cious

client

ğ‘–

in

iteration

ğ‘¡,

ğ‘”ğ‘¡ âˆ’1
ğ‘–

+

HË† ğ‘–ğ‘¡ (ğ‘¤ğ‘¡

âˆ’ ğ‘¤ğ‘¡âˆ’1)

is

the

predicted

model update for client ğ‘–, and HË† ğ‘–ğ‘¡ is the Hessian calculated on client

ğ‘–â€™s dataset to approximate HË† ğ‘¡ . ğœ† âˆˆ (0, 1] is a hyperparameter to

balance the loss function and the regularization term. A smaller ğœ†

makes the malicious clients less likely to be detected, but the attack

is also less effective.

5 EXPERIMENTS 5.1 Experimental Setup
Datasets and global-model architectures: We consider three widely-used benchmark datasets MNIST [15], CIFAR10 [13], and FEMNIST [6] to evaluate FLDetector. For MNIST and CIFAR10, we assume there are 100 clients and use the method in [11] to distribute the training images to the clients. Specifically, this method has a parameter called degree of non-iid ranging from 0.1 to 1.0 to control the distribution of the clientsâ€™ local training data. The clientsâ€™ local

KDD â€™22, August 14â€“18, 2022, Washington, DC, USA

Table 1: The CNN architecture of the global model used for MNIST and FEMNIST.

Layer Input Convolution + ReLU Max Pooling Convolution + ReLU Max Pooling Fully Connected + ReLU Softmax

Size 28Ã— 28 Ã— 1 3Ã— 3 Ã— 30
2Ã—2 3Ã—3Ã—5
2Ã—2
100
10 (62 for FEMNIST)

training data are not independent and identically distributed (iid) when the degree of non-iid is larger than 0.1 and are more noniid when the degree of non-iid becomes larger. Unless otherwise mentioned, we set the degree of non-iid to 0.5. FEMNIST is a 62class classification dataset from the open-source benchmark library of FL [6]. The training images are already grouped by the writers and we randomly sample 300 writers, each of which is treated as a client. We use a four-layer Convolutional Neural Network (CNN) (see Table 1) as the global model for MNIST and FEMNIST. For CIFAR-10, we consider the widely used ResNet20 architecture [12] as the global model. FL settings: We consider four FL methods: FedAvg [17], Krum [4], Trimmed-Mean [22], and Median [22]. We didnâ€™t consider FLTrust [7] due to its additional requirement of a clean validation dataset. Considering the different characteristics of the datasets, we adopt the following parameter settings for FL training: for MNIST, we train 1,000 iterations with a learning rate of 2 Ã— 10âˆ’4; and for CIFAR10 and FEMNIST, we train 2,000 iterations with a learning rate of 1Ã—10âˆ’3. For simplicity, we assume all clients are involved in each iteration of FL training. Note that when FLDetector detects malicious clients in a certain iteration, the server removes the clients classified as malicious, restarts the FL training, and repeats for the pre-defined number of iterations. Attack settings: By default, we randomly sample 28% of the clients as malicious ones. We choose this fraction because in the Distributed Backdoor Attack (DBA), the trigger pattern need to be equally splitted into four parts and embedded into the local training data of four malicious clients groups. Specifically, the number of malicious clients is 28, 28, and 84 for MNIST, CIFAR10, and FEMNIST, respectively. We consider one Untargeted Model Poisoning Attack [11], as well as three targeted model poisoning attacks including Scaling Attack [1], Distributed Backdoor Attack [20], and A Little is Enough Attack [2]. For all the three targeted model poisoning attacks, the trigger patterns are the same as their original papers and label â€™0â€™ is selected as the target label. The scaling factor is set to 100 following [1]. Unless otherwise mentioned, the malicious clients perform attacks in every iteration of FL training. Compared detection methods: There are few works on detecting malicious clients in FL. We compare the following methods:
â€¢ VAE [16]. This method trains a variational autoencoder for benign model updates by simulating model training using a validation dataset on the server and then applies it to detect malicious clients during FL training. We consider the

Zaixi Zhang et al.
validation dataset is the same as the joint local training data of all clients, which gives a strong advantage to VAE. â€¢ FLD-Norm. This is a variant of FLDetector. Specifically, FLDetector considers the Euclidean distance between a predicted model update and the received one in suspicious scores. One natural question is whether the norm of a model update itself can be used to detect malicious clients. In FLDNorm, the distance vector ğ‘‘ğ‘¡ consists of the â„“2 norms of the ğ‘› clientsâ€™ model updates in iteration ğ‘¡, which are further normalized and used to calculate our suspicious scores. â€¢ FLD-NoHVP. This is also a variant of FLDetector. In particular, in this variant, we do not consider the Hessian vector product (HVP) term in Equation 3, i.e., ğ‘”Ë†ğ‘¡ = ğ‘”ğ‘¡âˆ’1. The
ğ‘–ğ‘–
clientsâ€™ suspicious scores are calculated based on such predicted model updates. We use this variant to show that the Hessian vector product term in predicting the model update is important for FLDetector.
Evaluation metrics: We consider evaluation metrics for both detection and the learnt global models. For detection, we use detection accuracy (DACC), false positive rate (FPR), and false negative rate (FNR) as evaluation metrics. DACC is the fraction of clients that are correctly classified as benign or malicious. FPR (or FNR) is the fraction of benign (or malicious) clients that are falsely classified as malicious (or benign). To evaluate the learnt global model, we use testing accuracy (TACC), which is the fraction of testing examples that are correctly classified by the global model. Moreover, for targeted model poisoning attacks, we further use attack success rate (ASR) to evaluate the global model. In particular, we embed the trigger to each testing input and the ASR is the fraction of triggerembedded testing inputs that are classified as the target label by the global model. A lower ASR means that a targeted model poisoning attack is less successful. Detection settings: By default, we start to detect malicious clients in the 50th iteration of FL training, as we found the first dozens of iterations may be unstable. We will show how the iteration to start detection affects the performance of FLDetector. If no malicious clients are detected after finishing training for the pre-defined number of iterations, we classify all clients as benign. We set the window size ğ‘ to 10. Moreover, we set the maximum number of clusters ğ¾ and number of sampling ğµ in Gap statistics to 10 and 20, respectively. We will also explore the impact of hyperparameters in the following section.
5.2 Experimental Results
Detection results: Table 2 shows the detection results on the FEMNIST dataset for different attacks, detection methods, and FL methods. The results on MNIST and CIFAR10 are respectively shown in Table 4 and Table 5 in the Appendix, due to limited space. We have several observations. First, FLDetector can detect majority of the malicious clients. For instance, on FEMNIST, the FNR of FLDetector is always 0.0 for different attacks and FL methods. Second, FLDetector falsely detects a small fraction of benign clients as malicious, e.g., the FPR of FLDetector ranges between 0.0 and 0.20 on FEMNIST for different attacks and FL methods. Third, on FEMNIST, FLDetector outperforms VAE for different attacks and FL methods; on MNIST and CIFAR10, FLDetector outperforms VAE in

FLDetector: Defending Federated Learning Against Model Poisoning Attacks via Detecting Malicious Clients

KDD â€™22, August 14â€“18, 2022, Washington, DC, USA

Table 2: DACC, FPR, and FNR of malicious-client detection for different attacks, detection methods, and aggregation rules. The best detection results are bold for each attack. FEMNIST dataset, CNN global model, and 28 malicious clients are used.

Attack

Detector

FedAvg

Krum

Trimmed-Mean

Median

DACC FPR FNR DACC FPR FNR DACC FPR FNR DACC FPR FNR

Untargeted Model
Poisoning Attack

VAE FLD-Norm FLD-NoHVP FLDetector

0.71 0.02 0.99 0.57 0.36 0.62 0.56 0.37 0.62 0.55 0.35 0.71 0.72 0.03 0.93 0.05 0.93 1.00 0.42 0.42 1.00 0.13 0.82 1.00 0.51 0.38 0.79 0.34 0.83 0.21 0.77 0.32 0.00 0.67 0.28 0.54 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00

Scaling Attack

VAE FLD-Norm FLD-NoHVP FLDetector

0.73 0.05 0.99 0.68 0.44 0.00 0.33 0.54 1.00 0.47 0.42 0.82 0.82 0.14 0.29 0.68 0.44 0.00 0.92 0.00 0.29 0.90 0.03 0.29 0.07 0.98 0.82 0.42 0.42 1.00 0.91 0.13 0.00 0.96 0.05 0.00 0.85 0.20 0.00 1.00 0.00 0.00 0.98 0.03 0.00 1.00 0.00 0.00

Distributed Backdoor
Attack

VAE FLD-Norm FLD-NoHVP FLDetector

0.75 0.07 0.71 0.69 0.43 0.00 0.52 0.28 1.00 0.53 0.68 1.00 0.66 0.33 0.36 0.65 0.42 0.18 0.73 0.28 0.25 0.75 0.22 0.33 0.09 0.98 0.75 0.46 0.64 0.29 0.90 0.11 0.07 0.98 0.03 0.00 0.92 0.11 0.00 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00

A Little is Enough
Attack

VAE FLD-Norm FLD-NoHVP FLDetector

0.80 0.22 0.14 0.77 0.71 0.11 0.92 0.00 0.29 0.93 0.00 0.25 0.05 0.93 1.00 0.11 0.97 0.68 0.02 0.97 1.00 0.08 0.89 1.00 0.49 0.40 0.79 0.47 0.35 1.00 0.23 0.69 0.96 0.26 0.69 0.86 0.93 0.10 0.00 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00

Table 3: TACC and ASR of the global models learnt by Median in different scenarios. The results for the targeted model poisoning attacks are in the form of â€œTACC / ASR (%)â€. 28 malicious clients are used.

Dataset Attack

No Attack w/o FLDetector w/ FLDetector

Untargeted Model Poisoning Attack 97.6

Scaling Attack

97.6

MNIST

Distributed Backdoor Attack

97.6

A Little is Enough Attack

97.6

69.5 97.6/0.5 97.4/0.5 97.8/100.0

97.4 97.6/0.5 97.5/0.4 97.9/0.3

Untargeted Model Poisoning Attack 65.8

Scaling Attack

65.8

CIFAR10

Distributed Backdoor Attack

65.8

A Little is Enough Attack

65.8

27.8 66.6/91.2 66.1/93.5 62.1/95.2

65.9 65.7/2.4 65.2/1.9 64.3/1.8

Untargeted Model Poisoning Attack 64.4

Scaling Attack

64.4

FEMNIST

Distributed Backdoor Attack

64.4

A Little is Enough Attack

64.4

14.3 66.4/57.9 67.5/53.2 66.7/59.6

63.2 64.5/1.7 64.3/2.1 65.0/1.6

most cases and achieves comparable performance in the remaining cases. Fourth, FLDetector outperforms the two variants in most cases while achieving comparable performance in the remaining cases, which means that model-updates consistency and the Hessian vector product in estimating the model-updates consistency are informative at detecting malicious clients. Fifth, FLDetector achieves higher DACC for Byzantine-robust FL methods (Krum, Trimmed-Mean, and Median) than for FedAvg. The reason may be that Byzantine-robust FL methods provide more robust estimations of global model updates under attacks, which makes the estimation of Hessian and FLDetector more accurate.
Performance of the global models: Table 3 shows the TACC and ASR of the global models learnt by Median under no attacks, without FLDetector deployed, and with FLDetector deployed. Table 6 in the Appendix shows the results of other FL methods on MNIST.

â€œNo Attackâ€ means the global models are learnt by Median using the remaining 72% of benign clients; â€œw/o FLDetectorâ€ means the global models are learnt using all clients including both benign and malicious ones; and â€œw/ FLDetectorâ€ means that the server uses FLDetector to detect malicious clients, and after detecting malicious clients, the server removes them and restarts the FL training using the remaining clients.
We observe that the global models learnt with FLDetector deployed under different attacks are as accurate as those learnt under no attacks. Moreover, the ASRs of the global models learnt with FLDetector deployed are very small. This is because after FLDetector detects and removes majority of malicious clients, Byzantinerobust FL methods can resist the small number of malicious clients that miss detection. For instance, FLDetector misses 2 malicious

KDD â€™22, August 14â€“18, 2022, Washington, DC, USA

Zaixi Zhang et al.

1.0

0.8

0.6

0.4

0.2
0.0 10

DACC TACC w/o FLDetector TACC w/ FLDetector

15

20

25

30

35

Number of malicious clients

1.0

0.8

0.6

0.4

DACC

TACC w/o FLDetector

0.2

TACC w/ FLDetector

ASR w/o FLDetector

0.0

ASR w/ FLDetector

40

10

15

20

25

30

35

Number of malicious clients

(a) Untargeted Model Poisoning Attack

(b) Scaling Attack

1.0

0.8

0.6

0.4

DACC

TACC w/o FLDetector

0.2

TACC w/ FLDetector

ASR w/o FLDetector

0.0

ASR w/ FLDetector

40

15

20

25

30

35

Number of malicious clients

(c) Distributed Backdoor Attack

1.0

0.8

0.6

0.4

DACC

TACC w/o FLDetector

0.2

TACC w/ FLDetector

ASR w/o FLDetector

0.0

ASR w/ FLDetector

10

15

20

25

30

35

40

Number of malicious clients

(d) A Little is Enough Attack

Figure 1: Impact of the number of malicious clients on FLDetector, where CIFAR10, Median, and 0.5 degree of non-iid are used.

1.0

1.0

1.0

1.0

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.2

DACC

TACC w/o FLDetector

0.0

TACC w/ FLDetector

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Degree of non-iid

0.4

DACC

TACC w/o FLDetector

0.2

TACC w/ FLDetector

ASR w/o FLDetector

0.0

ASR w/ FLDetector

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Degree of non-iid

0.4

DACC

TACC w/o FLDetector

0.2

TACC w/ FLDetector

ASR w/o FLDetector

0.0

ASR w/ FLDetector

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Degree of non-iid

0.4

DACC

TACC w/o FLDetector

0.2

TACC w/ FLDetector

ASR w/o FLDetector

0.0

ASR w/ FLDetector

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Degree of non-iid

(a) Untargeted Model Poisoning Attack

(b) Scaling Attack

(c) Distributed Backdoor Attack

(d) A Little is Enough Attack

Figure 2: Impact of the degree of non-iid on FLDetector, where CIFAR10, Median, and 28 malicious clients are used.

0.030 0.025 0.020 0.015 0.010 0.005 0.000 âˆ’0.005

Average suspicious score of benign clients Average suspicious score of malicious clients

100

200

300

400

Training iteration

0.030

0.025

0.020

0.015

0.010

0.005

0.000

500

âˆ’0.005

(a) Untargeted Model Poisoning Attack

Average suspicious score of benign clients Average suspicious score of malicious clients

100

200

300

400

Training iteration

(b) Scaling Attack

0.030

0.030

0.025

0.025

0.020

0.020

0.015

0.015

0.010

0.010

0.005

0.005

0.000

Average suspicious score of benign clients

Average suspicious score of malicious clients

0.000

Average suspicious score of benign clients

Average suspicious score of malicious clients

500

âˆ’0.005

100

200

300

400

500

âˆ’0.005

100

200

300

400

500

Training iteration

Training iteration

(c) Distributed Backdoor Attack

(d) A Little is Enough Attack

Figure 3: Dynamics of the clientsâ€™ suspicious scores when malicious clients perform attacks periodically, where MNIST, 0.5 degree of non-iid, and 28 malicious clients are used.

clients on CIFAR10 in Median and A Little is Enough Attack, but Median is robust against them when learning the global model.
Impact of the number of malicious clients and degree of noniid: Figure 1 and 2 show the impact of the number of malicious clients and the non-iid degree on FLDetector, respectively. First, we observe that the DACC of FLDetector starts to drop after the number of malicious clients is larger than some threshold or the non-iid degree is larger than some threshold, but the thresholds are attack-dependent. For instance, for the Untargeted Model Poisoning Attack, DACC of FLDetector starts to decrease after more than 30 clients are malicious, while it starts to decrease after 20 malicious clients for the A Little is Enough Attack. Second, the global models learnt with FLDetector deployed are more accurate than the global models learnt without FLDetector deployed for different number of malicious clients and non-iid degrees. Specifically, the TACCs of the global models learnt with FLDetector deployed are larger

than or comparable with those of the global models learnt without FLDetector deployed, while the ASRs of the global models learnt with FLDetector deployed are much smaller than those of the global models learnt without FLDetector. The reason is that FLDetector detects and removes (some) malicious clients.
Dynamics of the clientsâ€™ suspicious scores: Figure 3 shows the average suspicious scores of benign clients and malicious clients as a function of the training iteration ğ‘¡. To better show the dynamics of the suspicious scores, we assume the malicious clients perform the attacks in the first 50 iterations in every 100 iterations, starting from the 50th iteration. Note that FLDetector is ignorant of when the attack starts or ends. We observe the periodic patterns of the suspicious scores follow the attack patterns. Specifically, the average suspicious score of the malicious clients grows rapidly when the attack begins and drops to be around the same as that of the benign clients when the attack stops. In the iterations where there

FLDetector: Defending Federated Learning Against Model Poisoning Attacks via Detecting Malicious Clients

KDD â€™22, August 14â€“18, 2022, Washington, DC, USA

1.0

1.0

0.8

0.8

0.6

0.6

0.4

0.4

0.2

DACC

TACC

0.0

ASR

0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Î»

0.2
0.0 10

DACC TACC ASR

20

30

40

50

Iteration to start FLDetector

(a)

(b)

1.0

0.8

0.6

0.4

0.2

DACC

TACC

0.0

ASR

60

35

10

15

N

(c)

1.0

0.8

0.6

0.4

0.2

DACC

TACC

0.0

ASR

20

10

15

20

25

30

35

40

B

(d)

Figure 4: (a) Adaptive attack; (b) impact of the detection iteration; (c) impact of window size ğ‘ ; (d) impact of number of sampling ğµ, where CIFAR10, Median, 0.5 degree of non-iid, 28 malicious clients, and scaling attack are used.

are attacks, malicious and benign clients can be well separated based on the suspicious scores. In these experiments, FLDetector can detect malicious clients at around 60th iteration. Note that the average suspicious score of the benign clients decreases (or increases) in the iterations where there are attacks (or no attacks). This is because FLDetector normalizes the corresponding Euclidean distances when calculating suspicious scores. Adaptive attack and impact of the detection iteration: Figure 4(a) shows the performance when we adapt Scaling Attack to FLDetector. We observe that DACC drops as ğœ† decreases. However, ASR is still low because the local model updates from the malicious clients are less effective while trying to evade detection. Figure 4(b) shows the impact of the detection iteration. Although DACC drops slightly when FLDetector starts earlier due to the instability in the early iterations, FLDetector can still defend against Scaling Attack by removing a majority of the malicious clients. Impact of hyperparameters: Figure 4(c) and (d) explore the impact of hyperparameters ğ‘ and ğµ, respectively. We observe FLDetector is robust to these hyperparameters. DACC drops slightly when ğ‘ is too small. This is because the suspicious scores fluctuate in a small number of rounds. In experiments, we choose ğ‘ = 10 and ğµ =20 as the default setting considering the trade-off between detection accuracy and computation complexity.
6 CONCLUSION AND FUTURE WORK
In this paper, we propose FLDetector, a malicious-client detection method that checks the clientsâ€™ model-updates consistency. We quantify a clientâ€™s model-updates consistency using the Cauchy mean value theorem and an L-BFGS algorithm. Our extensive evaluation on three popular benchmark datasets, four state-of-the-art attacks, and four FL methods shows that FLDetector outperforms baseline detection methods in various scenarios. Interesting future research directions include extending our method to vertical federated learning, asynchronous federated learning, federated learning in other domains such as text and graphs, as well as efficient recovery of the global model from model poisoning attacks after removing the detected malicious clients.
ACKNOWLEDGEMENTS
We thank the reviewers for constructive comments. This work is supported by NSF under grant No. 2125977 and 2112562.

REFERENCES
[1] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. 2020. How to backdoor federated learning. In AISTATS.
[2] Gilad Baruch, Moran Baruch, and Yoav Goldberg. 2019. A Little Is Enough: Circumventing Defenses For Distributed Learning. In NeurIPS.
[3] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. 2019. Analyzing federated learning through an adversarial lens. In ICML.
[4] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.
2017. Machine learning with adversaries: Byzantine tolerant gradient descent. In NeurIPS. [5] Richard H Byrd, Jorge Nocedal, and Robert B Schnabel. 1994. Representations of quasi-Newton matrices and their use in limited memory methods. Mathematical Programming (1994). [6] Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub KoneÄny`,
H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. 2018. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097 (2018). [7] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. 2021. FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping. In NDSS. [8] Xiaoyu Cao and Neil Zhenqiang Gong. 2022. MPAF: Model Poisoning Attacks to Federated Learning based on Fake Clients. In CVPR Workshops. [9] Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. 2021. Provably Secure Federated Learning against Malicious Clients. In AAAI. [10] Yudong Chen, Lili Su, and Jiaming Xu. 2017. Distributed statistical machine learning in adversarial settings: Byzantine gradient descent. In SIGMETRICS. [11] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020. Local model poisoning attacks to Byzantine-robust federated learning. In USENIX Security. [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR. [13] A. Krizhevsky and G. Hinton. 2009. Learning multiple layers of features from
tiny images. [14] Serge Lang. 1968. A second course in calculus. Vol. 4197. Addison-Wesley Publish-
ing Company.
[15] Y. LeCun. 1998. The MNIST database of handwritten digits. http://yann. lecun.
com/exdb/mnist/.
[16] Suyi Li, Yong Cheng, Wei Wang, Yang Liu, and Tianjian Chen. 2020. Learning to detect malicious clients for robust federated learning. arXiv preprint arXiv:2002.00211 (2020).
[17] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep networks from decentralized data. In AISTATS. [18] Virat Shejwalkar and Amir Houmansadr. 2021. Manipulating the Byzantine:
Optimizing Model Poisoning Attacks and Defenses for Federated Learning. In NDSS. [19] Robert Tibshirani, Guenther Walther, and Trevor Hastie. 2001. Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology) (2001). [20] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. 2019. Dba: Distributed backdoor attacks against federated learning. In ICLR. [21] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine learning: Concept and applications. TIST (2019). [22] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. 2018. Byzantine-robust distributed learning: Towards optimal statistical rates. In ICML.

KDD â€™22, August 14â€“18, 2022, Washington, DC, USA

A PROOF OF THEOREM 1

Lemma 1. For any ğ‘¡ and any vector ğ‘§, the following inequality related to the estimated Hessian ğ»Ë† ğ‘¡ holds:

ğ‘‡
ğ‘§

ğ»Ë†

ğ‘¡

ğ‘§

â‰¤

(ğ‘

+

1)ğ¿

âˆ¥ğ‘§

âˆ¥

2
,

(7)

where N is the window size and L is from Assumption 1.

Proof. By following Equation 1.2 and 1.3 in [5], the Quasi-

Hessian update can be written as:

ğµğ‘¡ âˆ’ğ‘š+1

=

ğµğ‘¡ âˆ’ğ‘š

âˆ’

ğµğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡ğ‘‡âˆ’ğ‘šğµğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡ğ‘‡âˆ’ğ‘šğµğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š

+ Î”ğ‘”ğ‘¡âˆ’ğ‘š Î”ğ‘”ğ‘‡ğ‘¡âˆ’ğ‘š , Î”ğ‘”ğ‘‡ğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š

(8)

where the initialized matrix ğµğ‘¡âˆ’ğ‘ = Î”ğ‘”ğ‘‡ğ‘¡âˆ’ğ‘ Î”ğ‘¤ğ‘¡âˆ’ğ‘ /Î”ğ‘¤ğ‘¡ğ‘‡âˆ’ğ‘ Î”ğ‘¤ğ‘¡âˆ’ğ‘ I and ğ‘š âˆˆ {1, 2, Â· Â· Â· , ğ‘ }. The final estimated Hessian ğ»Ë† ğ‘¡ = ğµğ‘¡ .
Based on Equation 8, we derive an upper bound for ğ‘§ğ‘‡ ğ»Ë† ğ‘¡ ğ‘§:

ğ‘‡
ğ‘§

ğµğ‘¡ âˆ’ğ‘š+1ğ‘§

=

ğ‘‡
ğ‘§

ğµğ‘¡ âˆ’ğ‘š ğ‘§

âˆ’

ğ‘‡
ğ‘§

ğµğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡ğ‘‡âˆ’ğ‘šğµğ‘¡âˆ’ğ‘šğ‘§ Î”ğ‘¤ğ‘¡ğ‘‡âˆ’ğ‘šğµğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š

(9)

+ ğ‘§ğ‘‡ Î”ğ‘”ğ‘¡âˆ’ğ‘š Î”ğ‘”ğ‘‡ğ‘¡âˆ’ğ‘šğ‘§

(10)

Î”ğ‘”ğ‘‡ğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š

â‰¤

ğ‘‡
ğ‘§

ğµğ‘¡ âˆ’ğ‘š ğ‘§

+

ğ‘§ğ‘‡ Î”ğ‘”ğ‘¡âˆ’ğ‘š Î”ğ‘”ğ‘‡ğ‘¡âˆ’ğ‘šğ‘§ Î”ğ‘”ğ‘‡ğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š

(11)

=

ğ‘‡
ğ‘§

ğµğ‘¡ âˆ’ğ‘š ğ‘§

+

ğ‘§ğ‘‡

ğ»ğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡ğ‘‡âˆ’ğ‘šğ»ğ‘¡âˆ’ğ‘šğ‘§ Î”ğ‘¤ğ‘¡ğ‘‡âˆ’ğ‘šğ»ğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š

(12)

â‰¤

ğ‘‡
ğ‘§

ğµğ‘¡ âˆ’ğ‘š ğ‘§

+

ğ‘§ğ‘‡

ğ»ğ‘¡âˆ’ğ‘šğ‘§Î”ğ‘¤ğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡ğ‘‡âˆ’ğ‘šğ»ğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡ğ‘‡âˆ’ğ‘šğ»ğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š

(13)

=

ğ‘‡
ğ‘§

ğµğ‘¡ âˆ’ğ‘š ğ‘§

+

ğ‘‡
ğ‘§

ğ»ğ‘¡ âˆ’ğ‘š ğ‘§

(14)

â‰¤

ğ‘‡
ğ‘§

ğµğ‘¡ âˆ’ğ‘š ğ‘§

+

ğ¿

âˆ¥ğ‘§ âˆ¥2

(15)

Zaixi Zhang et al.

The first inequality uses the fact that ğ‘§ğ‘‡ ğµğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡ğ‘‡âˆ’ğ‘šğµğ‘¡âˆ’ğ‘šğ‘§ = (ğ‘§ğ‘‡ ğµğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š)2 â‰¥ 0 and Î”ğ‘¤ğ‘¡ğ‘‡âˆ’ğ‘šğµğ‘¡âˆ’ğ‘š Î”ğ‘¤ğ‘¡âˆ’ğ‘š â‰¥ 0 due to the pos-

itive definiteness of ğµğ‘¡âˆ’ğ‘š. The second inequality uses the Cauchy-

Schwarz inequality.

By applying the formula above recursively, we have ğ‘§ğ‘‡ ğ»Ë† ğ‘¡ ğ‘§ =

ğ‘§ğ‘‡ ğµğ‘¡ ğ‘§ â‰¤ (ğ‘ + 1)ğ¿âˆ¥ğ‘§ âˆ¥2.

â–¡

Next, we prove Theorem 1. Our idea is to bound the difference

ğ‘‘ğ‘– between predicted model updates and the received ones from benign clients in each iteration. For ğ‘– âˆˆ B and ğ‘— âˆˆ M, we have:

E ğ‘‘ ğ‘— âˆ’ E ğ‘‘ğ‘–

(16)

=

Eâˆ¥ğ‘”ğ‘¡ğ‘—âˆ’1

+

ğ»Ë† ğ‘¡

(ğ‘¤ğ‘¡

âˆ’ ğ‘¤ğ‘¡âˆ’1)

âˆ’

ğ‘¡
ğ‘”

âˆ¥

ğ‘—

âˆ’

E âˆ¥ğ‘”ğ‘–ğ‘¡ âˆ’1

+

ğ»Ë† ğ‘¡ (ğ‘¤ğ‘¡

âˆ’ ğ‘¤ğ‘¡âˆ’1)

âˆ’

ğ‘¡
ğ‘”

ğ‘–

âˆ¥

(17)

= Eğ·ğ‘— âˆ¼ğ· âˆ¥âˆ‡ğ‘“ (ğ· ğ‘— , ğ‘¤ğ‘¡âˆ’1) + âˆ‡ğ‘“ (ğ· ğ‘— , ğ‘¤ğ‘¡ ) + ğ»Ë† ğ‘¡ (ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1) âˆ¥ (18)

âˆ’ Eğ·ğ‘– âˆ¼ğ· âˆ¥âˆ‡ğ‘“ (ğ·ğ‘–, ğ‘¤ğ‘¡âˆ’1) âˆ’ âˆ‡ğ‘“ (ğ·ğ‘–, ğ‘¤ğ‘¡ ) + ğ»Ë† ğ‘¡ (ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1) âˆ¥

(19)

â‰¥ Eğ·ğ‘— âˆ¼ğ· 2âˆ¥âˆ‡ğ‘“ (ğ· ğ‘— , ğ‘¤ğ‘¡âˆ’1) âˆ¥ âˆ’ ğ¿âˆ¥ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1 âˆ¥ âˆ’ âˆ¥ğ»Ë† ğ‘¡ (ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1) âˆ¥

(20)

âˆ’ Eğ·ğ‘– âˆ¼ğ· (âˆ¥âˆ‡ğ‘“ (ğ·ğ‘–, ğ‘¤ğ‘¡âˆ’1) âˆ’ âˆ‡ğ‘“ (ğ·ğ‘–, ğ‘¤ğ‘¡ ) âˆ¥ + âˆ¥ğ»Ë† ğ‘¡ (ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1) âˆ¥) (21)

â‰¥ Eğ·ğ‘— âˆ¼ğ· 2âˆ¥âˆ‡ğ‘“ (ğ· ğ‘— , ğ‘¤ğ‘¡âˆ’1) âˆ¥ âˆ’ 2(ğ¿âˆ¥ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1 âˆ¥ + âˆ¥ğ»Ë† ğ‘¡ (ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1) âˆ¥) (22)

â‰¥ Eğ·ğ‘— âˆ¼ğ· 2âˆ¥âˆ‡ğ‘“ (ğ· ğ‘— , ğ‘¤ğ‘¡âˆ’1) âˆ¥ âˆ’ 2(ğ‘ + 2)ğ¿âˆ¥ (ğ‘¤ğ‘¡ âˆ’ ğ‘¤ğ‘¡âˆ’1) âˆ¥

(23)

= Eğ·ğ‘— âˆ¼ğ· 2âˆ¥âˆ‡ğ‘“ (ğ· ğ‘— , ğ‘¤ğ‘¡âˆ’1) âˆ¥ âˆ’ 2(ğ‘ + 2)ğ¿ğ›¼Eğ·ğ‘– âˆ¼ğ· âˆ¥âˆ‡ğ‘“ (ğ· ğ‘— , ğ‘¤ğ‘¡âˆ’1) âˆ¥

(24)

= (2 âˆ’ 2(ğ‘ + 2)ğ¿ğ›¼)Eğ·ğ‘— âˆ¼ğ· âˆ¥âˆ‡ğ‘“ (ğ· ğ‘— , ğ‘¤ğ‘¡âˆ’1) âˆ¥

(25)

â‰¥ 0,

(26)

where the first inequality uses the Triangle inequality, the sec-

ond inequality uses Assumption 1, and the third inequality uses

Lemma 1. According to the definition of suspicious scores (ğ‘ ğ‘¡ =

ğ‘–

1 ğ‘

âˆ‘ï¸ğ‘ âˆ’1
ğ‘Ÿ =0

ğ‘‘Ë†ğ‘–ğ‘¡

âˆ’ğ‘Ÿ

),

we

have

E

(ğ‘ ğ‘¡ )
ğ‘–

<

E

(ğ‘ ğ‘¡ ).
ğ‘—

FLDetector: Defending Federated Learning Against Model Poisoning Attacks via Detecting Malicious Clients

KDD â€™22, August 14â€“18, 2022, Washington, DC, USA

Table 4: DACC, FPR, and FNR of malicious-client detection for different attacks, detection methods, and aggregation rules. The best detection results are bold for each attack. MNIST dataset, CNN global model, and 28 malicious clients are used.

Attack

Detector

FedAvg

Krum

Trimmed-Mean

Median

DACC FPR FNR DACC FPR FNR DACC FPR FNR DACC FPR FNR

Untargeted

VAE

0.67 0.18 0.71 0.60 0.22 0.86 0.58 0.38 0.54 0.58 0.38 0.54

Model

FLD-Norm 0.68 0.17 0.71 0.08 0.89 1.00 0.28 1.00 0.00 0.15 0.79 1.00

Poisoning FLD-NoHVP 0.60 0.22 0.86 0.11 0.85 1.00 0.39 0.85 0.00 0.66 0.26 0.54

Attack

FLDetector 0.87 0.18 0.00 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00

Scaling Attack

VAE

0.78 0.31 0.00 0.97 0.00 0.11 0.76 0.00 0.86 0.75 0.00 0.89

FLD-Norm 0.97 0.00 0.11 0.97 0.00 0.11 0.92 0.11 0.00 1.00 0.00 0.00

FLD-NoHVP 0.62 0.21 0.82 0.59 0.40 0.43 0.90 0.10 0.11 0.83 0.21 0.07

FLDetector 0.81 0.22 0.11 0.98 0.00 0.07 1.00 0.00 0.00 1.00 0.00 0.00

VAE

0.89 0.15 0.00 0.97 0.00 0.11 0.79 0.00 0.75 0.81 0.06 0.54

Distributed

FLD-Norm 0.91 0.08 0.11 0.75 0.26 0.21 0.90 0.14 0.00 0.93 0.10 0.00

Backdoor

FLD-NoHVP 0.62 0.21 0.82 0.82 0.21 0.11 1.00 0.00 0.00 0.93 0.10 0.00

Attack

FLDetector 0.86 0.15 0.11 0.97 0.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00

A Little is Enough
Attack

VAE

0.80 0.28 0.00 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00

FLD-Norm 0.00 1.00 1.00 0.12 0.83 1.00 0.00 1.00 1.00 0.09 0.86 1.00

FLD-NoHVP 0.65 0.10 1.00 0.02 0.97 1.00 1.00 0.00 0.00 0.75 0.35 0.00

FLDetector 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00

Table 5: DACC, FPR, and FNR of malicious-client detection for different attacks, detection methods, and aggregation rules. The best detection results are bold for each attack. CIFAR10 dataset, ResNet20 global model, and 28 malicious clients are used.

Attack

Detector

FedAvg

Krum

Trimmed-Mean

Median

DACC FPR FNR DACC FPR FNR DACC FPR FNR DACC FPR FNR

Untargeted

VAE

0.28 1.00 0.00 0.61 0.42 0.32 0.52 0.33 0.86 0.46 0.40 0.89

Model

FLD-Norm 0.72 0.00 1.00 0.07 0.90 1.00 0.00 1.00 1.00 0.00 1.00 1.00

Poisoning FLD-NoHVP 0.53 0.40 0.64 0.85 0.21 0.00 0.48 0.51 0.54 0.98 0.00 0.07

Attack

FLDetector 0.93 0.10 0.00 0.97 0.04 0.00 1.00 0.00 0.00 1.00 0.00 0.00

Scaling Attack

VAE

0.24 0.71 0.89 0.50 0.31 1.00 0.48 0.38 0.89 0.75 0.00 0.89

FLD-Norm 0.96 0.00 0.14 0.98 0.00 0.07 0.96 0.01 0.11 1.00 0.00 0.00

FLD-NoHVP 0.86 0.14 0.14 1.00 0.00 0.00 1.00 0.00 0.00 0.97 0.00 0.11

FLDetector 0.88 0.11 0.14 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00

VAE

0.27 0.74 0.93 0.53 0.26 1.00 0.55 0.33 0.71 0.76 0.00 0.86

Distributed FLD-Norm 0.91 0.07 0.14 0.85 0.14 0.18 0.92 0.07 0.11 0.96 0.01 0.11

Backdoor

FLD-NoHVP 0.84 0.14 0.21 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00

Attack

FLDetector 0.89 0.11 0.11 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00

A Little is Enough
Attack

VAE

0.81 0.13 0.36 0.61 0.49 0.14 0.84 0.14 0.21 0.85 0.10 0.29

FLD-Norm 0.33 0.69 0.61 0.77 0.22 0.25 0.45 0.69 0.18 0.47 0.67 0.18

FLD-NoHVP 0.72 0.24 0.39 0.85 0.14 0.18 0.80 0.17 0.29 0.81 0.15 0.32 FLDetector 0.80 0.14 0.36 0.92 0.11 0.00 0.89 0.13 0.07 0.87 0.15 0.07

Table 6: TACC and ASR of the global models learnt by different FL methods on MNIST. The results for the targeted model poisoning attacks are in the form of â€œTACC / ASR (%)â€. 28% malicious clients are used. The ASR on FedAvg with FLDetector is still high because FedAvg is not Byzantine-robust and can be backdoored by even a single malicious client.

FL Method

Attack

No Attack w/o FLDetector w/ FLDetector

FedAvg

Untargeted Model Poisoning Attack 98.4

Scaling Attack

98.4

Distributed Backdoor Attack

98.4

A Little is Enough Attack

98.4

10.1 98.5/99.8 98.4/99.9 97.9/99.9

98.3 98.2/99.6 98.1/99.5 98.2/0.3

Krum

Untargeted Model Poisoning Attack 93.5

Scaling Attack

93.5

Distributed Backdoor Attack

93.5

A Little is Enough Attack

93.5

11.2 94.3/0.9 94.4/0.8 94.4/99.6

92.8 93.2/0.7 93.1/0.8 93.4/0.6

Untargeted Model Poisoning Attack 97.6

Scaling Attack

97.6

Trimmed-Mean

Distributed Backdoor Attack

97.6

A Little is Enough Attack

97.6

63.9 97.5/0.6 97.5/0.5 97.8/100.0

97.5 97.5/0.5 97.4/0.4 97.5/0.4

Median

Untargeted Model Poisoning Attack 97.6

Scaling Attack

97.6

Distributed Backdoor Attack

97.6

A Little is Enough Attack

97.6

69.5 97.6/0.5 97.4/0.5 97.8/100.0

97.4 97.6/0.5 97.5/0.4 97.9/0.3

