
Skip to main content
Cornell University
We gratefully acknowledge support from
the Simons Foundation and member institutions.
arxiv logo > cs > arXiv:2210.00584

Help | Advanced Search
Search
Computer Science > Cryptography and Security
(cs)
[Submitted on 2 Oct 2022 ( v1 ), last revised 4 Oct 2022 (this version, v2)]
Title: FLCert: Provably Secure Federated Learning against Poisoning Attacks
Authors: Xiaoyu Cao , Zaixi Zhang , Jinyuan Jia , Neil Zhenqiang Gong
Download a PDF of the paper titled FLCert: Provably Secure Federated Learning against Poisoning Attacks, by Xiaoyu Cao and 3 other authors
Download PDF

    Abstract: Due to its distributed nature, federated learning is vulnerable to poisoning attacks, in which malicious clients poison the training process via manipulating their local training data and/or local model updates sent to the cloud server, such that the poisoned global model misclassifies many indiscriminate test inputs or attacker-chosen ones. Existing defenses mainly leverage Byzantine-robust federated learning methods or detect malicious clients. However, these defenses do not have provable security guarantees against poisoning attacks and may be vulnerable to more advanced attacks. In this work, we aim to bridge the gap by proposing FLCert, an ensemble federated learning framework, that is provably secure against poisoning attacks with a bounded number of malicious clients. Our key idea is to divide the clients into groups, learn a global model for each group of clients using any existing federated learning method, and take a majority vote among the global models to classify a test input. Specifically, we consider two methods to group the clients and propose two variants of FLCert correspondingly, i.e., FLCert-P that randomly samples clients in each group, and FLCert-D that divides clients to disjoint groups deterministically. Our extensive experiments on multiple datasets show that the label predicted by our FLCert for a test input is provably unaffected by a bounded number of malicious clients, no matter what poisoning attacks they use. 

Comments: 	To appear in Transactions on Information Forensics and Security. arXiv admin note: text overlap with arXiv:2102.01854
Subjects: 	Cryptography and Security (cs.CR) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
Cite as: 	arXiv:2210.00584 [cs.CR]
  	(or arXiv:2210.00584v2 [cs.CR] for this version)
  	https://doi.org/10.48550/arXiv.2210.00584
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Xiaoyu Cao [ view email ]
[v1] Sun, 2 Oct 2022 17:50:04 UTC (9,610 KB)
[v2] Tue, 4 Oct 2022 02:10:46 UTC (9,610 KB)
Full-text links:
Download:

    Download a PDF of the paper titled FLCert: Provably Secure Federated Learning against Poisoning Attacks, by Xiaoyu Cao and 3 other authors
    PDF
    Other formats 

( license )
Current browse context:
cs.CR
< prev   |   next >
new | recent | 2210
Change to browse by:
cs
cs.AI
cs.LG
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

a export BibTeX citation Loading...
Bookmark
BibSonomy logo Mendeley logo Reddit logo ScienceWISE logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

