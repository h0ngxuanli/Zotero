
Skip to main content
Cornell University
We are hiring

We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:2212.10505

Help | Advanced Search
Search
Computer Science > Computation and Language
(cs)
[Submitted on 20 Dec 2022 ( v1 ), last revised 23 May 2023 (this version, v2)]
Title: DePlot: One-shot visual language reasoning by plot-to-table translation
Authors: Fangyu Liu , Julian Martin Eisenschlos , Francesco Piccinno , Syrine Krichene , Chenxi Pang , Kenton Lee , Mandar Joshi , Wenhu Chen , Nigel Collier , Yasemin Altun
Download a PDF of the paper titled DePlot: One-shot visual language reasoning by plot-to-table translation, by Fangyu Liu and 9 other authors
Download PDF

    Abstract: Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than >28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA. 

Comments: 	ACL 2023 (Findings)
Subjects: 	Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)
Cite as: 	arXiv:2212.10505 [cs.CL]
  	(or arXiv:2212.10505v2 [cs.CL] for this version)
  	https://doi.org/10.48550/arXiv.2212.10505
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Fangyu Liu [ view email ]
[v1] Tue, 20 Dec 2022 18:20:50 UTC (7,307 KB)
[v2] Tue, 23 May 2023 18:28:39 UTC (8,875 KB)
Full-text links:
Download:

    Download a PDF of the paper titled DePlot: One-shot visual language reasoning by plot-to-table translation, by Fangyu Liu and 9 other authors
    PDF
    Other formats 

Current browse context:
cs.CL
< prev   |   next >
new | recent | 2212
Change to browse by:
cs
cs.AI
cs.CV
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

1 blog link
( what is this? )
a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

