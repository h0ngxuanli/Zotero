Lingfei Wu · Peng Cui Jian Pei · Liang Zhao Eds.
Graph Neural Networks
Foundations, Frontiers, and Applications

Graph Neural Networks: Foundations, Frontiers, and Applications

Lingfei Wu • Peng Cui • Jian Pei • Liang Zhao
Editors
Graph Neural Networks: Foundations, Frontiers, and Applications
123

Editors Lingfei Wu JD Silicon Valley Research Center Mountain View, CA, USA
Jian Pei Simon Fraser University Burnaby, Canada

Peng Cui Tsinghua University Beijing, China
Liang Zhao Emory University Atlanta, USA

ISBN 978-981-16-6053-5

ISBN 978-981-16-6054-2 (eBook)

https://doi.org/10.1007/978-981-16-6054-2

© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022 This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether the whole or part of the material is concerned, specifically the rights of reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

This Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd. The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721, Singapore

Foreword
“The first comprehensive book covering the full spectrum of a young, fast-growing research field, graph neural networks (GNNs), written by authoritative authors!”
Jiawei Han (Michael Aiken Chair Professor at University of Illinois at UrbanaChampaign, ACM Fellow and IEEE Fellow) “This book presents a comprehensive and timely survey on graph representation learning. Edited and contributed by the best group of experts in this area, this book is a must-read for students, researchers and pratictioners who want to learn anything about Graph Neural Networks.”
Heung-Yeung ”Harry” Shum (Former Executive Vice President for Technology and Research at Microsoft Research, ACM Fellow, IEEE Fellow, FREng) “As the new frontier of deep learning, Graph Neural Networks offer great potential to combine probabilistic learning and symbolic reasoning, and bridge knowledgedriven and data-driven paradigms, nurturing the development of third-generation AI. This book provides a comprehensive and insightful introduction to GNN, ranging from foundations to frontiers, from algorithms to applications. It is a valuable resource for any scientist, engineer and student who wants to get into this exciting field.”
Bo Zhang (Member of Chinese Academy of Science, Professor at Tsinghua University) “Graph Neural Networks are one of the hottest areas of machine learning and this book is a wonderful in-depth resource covering a broad range of topics and applications of graph representation learning.”
Jure Leskovec (Associate Professor at Stanford University, and investigator at Chan Zuckerberg Biohub). “Graph Neural Networks are an emerging machine learning model that is already taking the scientific and industrial world by storm. The time is perfect to get in on the action – and this book is a great resource for newcomers and seasoned practitioners
v

vi

Foreword

alike! Its chapters are very carefully written by many of the thought leaders at the forefront of the area.”
Petar Velicˇkovic´ (Senior Research Scientist, DeepMind)

Preface
The field of graph neural networks (GNNs) has seen rapid and incredible strides over the recent years. Graph neural networks, also known as deep learning on graphs, graph representation learning, or geometric deep learning, have become one of the fastest-growing research topics in machine learning, especially deep learning. This wave of research at the intersection of graph theory and deep learning has also influenced other fields of science, including recommendation systems, computer vision, natural language processing, inductive logic programming, program synthesis, software mining, automated planning, cybersecurity, and intelligent transportation.
Although graph neural networks have achieved remarkable attention, it still faces many challenges when applying them into other domains, from the theoretical understanding of methods to the scalability and interpretability in a real system, and from the soundness of the methodology to the empirical performance in an application. However, as the field rapidly grows, it has been extremely challenging to gain a global perspective of the developments of GNNs. Therefore, we feel the urgency to bridge the above gap and have a comprehensive book on this fast-growing yet challenging topic, which can benefit a broad audience including advanced undergraduate and graduate students, postdoctoral researchers, lecturers, and industrial practitioners.
This book is intended to cover a broad range of topics in graph neural networks, from the foundations to the frontiers, and from the methodologies to the applications. Our book is dedicated to introducing the fundamental concepts and algorithms of GNNs, new research frontiers of GNNs, and broad and emerging applications with GNNs. Book Website and Resources The website and further resources of this book can be found at: https:// graph-neural-networks.github.io/. The website provides online preprints and lecture slides of all the chapters. It also provides pointers to useful material and resources that are publicly available and relevant to graph neural networks.
vii

viii
To the Instructors

Preface

The book can be used for a one-semester graduate course for graduate students. Though it is mainly written for students with a background in computer science, students with a basic understanding of probability, statistics, graph theory, linear algebra, and machine learning techniques such as deep learning will find it easily accessible. Some chapters can be skipped or assigned as homework assignments for reviewing purposes if students have knowledge of a chapter. For example, if students have taken a deep learning course, they can skip Chapter 1. The instructors can also choose to combine Chapters 1, 2, and 3 together as a background introduction course at the very beginning.
When the course focuses more on the foundation and theories of graph neural networks, the instructor can choose to focus more on Chapters 4-8 while using Chapters 19-27 to showcase the applications, motivations, and limitations. Please refer to the Editors’ Notes at the end of each chapter on how Chapters 4-8 and Chapters 19-27 are correlated. When the course focuses more on the research frontiers, Chapters 9-18 can be the pivot to organize the course. For example, an instructor can make it an advanced graduate course where the students are asked to search and present the most recent research papers in each different research frontier. They can also be asked to establish their course projects based on the applications described in Chapters 19-27 as well as the materials provided on our website.

To the Readers This book was designed to cover a wide range of topics in the field of graph neural network field, including background, theoretical foundations, methodologies, research frontiers, and applications. Therefore, it can be treated as a comprehensive handbook for a wide variety of readers such as students, researchers, and professionals. You should have some knowledge of the concepts and terminology associated with statistics, machine learning, and graph theory. Some backgrounds of the basics have been provided and referenced in the first eight chapters. You should better also have knowledge of deep learning and some programming experience for easily accessing the most of chapters of this book. In particular, you should be able to read pseudocode and understand graph structures.
The book is well modularized and each chapter can be learned in a standalone manner based on the individual interests and needs. For those readers who want to have a solid understanding of various techniques and theories of graph neural networks, you can start from Chapters 4-9. For those who further want to perform in-depth research and advance related fields, please read those chapters of interest among Chapters 9-18, which provide comprehensive knowledge in the most recent research issues, open problems, and research frontiers. For those who want to apply graph neural networks to benefit specific domains, or aim at finding interesting applications to validate specific graph neural networks techniques, please refer to Chapters 19-27.

Acknowledgements
Graph machine learning has attracted many gifted researchers to make their seminal contributions over the last few years. We are very fortunate to discuss the challenges and opportunities, and often work with many of them on a rich variety of research topics in this exciting field. We are deeply indebted to these collaborators and colleagues from JD.COM, IBM Research, Tsinghua University, Simon Fraser University, Emory University, and elsewhere, who encouraged us to create such a book comprehensively covering various topics of Graph Neural Networks in order to educate the interested beginners and foster the advancement of the field for both academic researchers and industrial practitioners.
This book would not have been possible without the contributions of many people. We would like to give many thanks to the people who offered feedback on checking the consistency of the math notations of the entire book as well as reference editing of this book. They are people from Emory University: Ling Chen, Xiaojie Guo, and Shiyu Wang, as well as people from Tsinghua University: Yue He, Ziwei Zhang, and Haoxin Liu. We would like to give our special thanks to Dr. Xiaojie Guo, who generously offered her help in providing numerous valuable feedback on many chapters.
We also want to thank those who allowed us to reproduce images, figures, or data from their publications.
Finally, we would like to thank our families for their love, patience and support during this very unusual time when we are writing and editing this book.
ix

Editor Biography
Dr. Lingfei Wu is a Principal Scientist at JD.COM Silicon Valley Research Center, leading a team of 30+ machine learning/natural language processing scientists and software engineers to build intelligent e-commerce personalization systems. He earned his Ph.D. degree in computer science from the College of William and Mary in 2016. Previously, he was a research staff member at IBM Thomas J. Watson Research Center and led a 10+ research scientist team for developing novel Graph Neural Networks methods and systems, which leads to the #1 AI Challenge Project in IBM Research and multiple IBM Awards including three-time Outstanding Technical Achievement Award. He has published more than 90 top-ranked conference and journal papers, and is a co-inventor of more than 40 filed US patents. Because of the high commercial value of his patents, he has received eight invention achievement awards and has been appointed as IBM Master Inventors, class of 2020. He was the recipients of the Best Paper Award and Best Student Paper Award of several conferences such as IEEE ICC’19, AAAI workshop on DLGMA’20 and KDD workshop on DLG’19. His research has been featured in numerous media outlets, including NatureNews, YahooNews, Venturebeat, TechTalks, SyncedReview, Leiphone, QbitAI, MIT News, IBM Research News, and SIAM News. He has co-organized 10+ conferences (KDD, AAAI, IEEE BigData) and is the founding co-chair for Workshops of Deep Learning on Graphs (with AAAI’21, AAAI’20, KDD’21, KDD’20, KDD’19, and IEEE BigData’19). He has currently served as Associate Editor for IEEE Transactions on Neural Networks and Learning Systems, ACM Transactions on Knowledge Discovery from Data and International Journal of Intelligent Systems, and regularly served as a SPC/PC member of the following major AI/ML/NLP conferences including KDD, IJCAI, AAAI, NIPS, ICML, ICLR, and ACL.
xi

xii

Editor Biography

Dr. Peng Cui is an Associate Professor with tenure at Department of Computer Science in Tsinghua University. He obtained his PhD degree from Tsinghua University in 2010. His research interests include data mining, machine learning and multimedia analysis, with expertise on network representation learning, causal inference and stable learning, social dynamics modeling, and user behavior modeling, etc. He is keen to promote the convergence and integration of causal inference and machine learning, addressing the fundamental issues of today’s AI technology, including explainability, stability and fairness issues. He is recognized as a Distinguished Scientist of ACM, Distinguished Member of CCF and Senior Member of IEEE. He has published more than 100 papers in prestigious conferences and journals in machine learning and data mining. He is one of the most cited authors in network embedding. A number of his proposed algorithms on network embedding generate substantial impact in academia and industry. His recent research won the IEEE Multimedia Best Department Paper Award, IEEE ICDM 2015 Best Student Paper Award, IEEE ICME 2014 Best Paper Award, ACM MM12 Grand Challenge Multimodal Award, MMM13 Best Paper Award, and were selected into the Best of KDD special issues in 2014 and 2016, respectively. He was PC co-chair of CIKM2019 and MMM2020, SPC or area chair of ICML, KDD, WWW, IJCAI, AAAI, etc., and Associate Editors of IEEE TKDE (2017-), IEEE TBD (2019-), ACM TIST(2018-), and ACM TOMM (2016-) etc. He received ACM China Rising Star Award in 2015, and CCF-IEEE CS Young Scientist Award in 2018.

Editor Biography

xiii

Dr. Jian Pei is a Professor in the School of Computing Science at Simon Fraser University. He is a well-known leading researcher in the general areas of data science, big data, data mining, and database systems. His expertise is on developing effective and efficient data analysis techniques for novel data intensive applications, and transferring his research results to products and business practice. He is recognized as a Fellow of the Royal Society of Canada (Canada’s national academy), the Canadian Academy of Engineering, the Association of Computing Machinery (ACM) and the Institute of Electrical and Electronics Engineers (IEEE). He is one of the most cited authors in data mining, database systems, and information retrieval. Since 2000, he has published one textbook, two monographs and over 300 research papers in refereed journals and conferences, which have been cited extensively by others. His research has generated remarkable impact substantially beyond academia. For example, his algorithms have been adopted by industry in production and popular open-source software suites. Jian Pei also demonstrated outstanding professional leadership in many academic organizations and activities. He was the editor-in-chief of the IEEE Transactions of Knowledge and Data Engineering (TKDE) in 2013-16, the chair of the Special Interest Group on Knowledge Discovery in Data (SIGKDD) of the Association for Computing Machinery (ACM) in 2017-2021, and a general co-chair or program committee co-chair of many premier conferences. He maintains a wide spectrum of industry relations with both global and local industry partners. He is an active consultant and coach for industry on enterprise data strategies, healthcare informatics, network security intelligence, computational finance, and smart retail. He received many prestigious awards, including the 2017 ACM SIGKDD Innovation Award, the 2015 ACM SIGKDD Service Award, the 2014 IEEE ICDM Research Contributions Award, the British Columbia Innovation Council 2005 Young Innovator Award, an NSERC 2008 Discovery Accelerator Supplements Award (100 awards cross the whole country), an IBM Faculty Award (2006), a KDD Best Application Paper Award (2008), an ICDE Influential Paper Award (2018), a PAKDD Best Paper Award (2014), a PAKDD Most Influential Paper Award (2009), and an IEEE Outstanding Paper Award (2007).

xiv

Editor Biography

Dr. Liang Zhao is an assistant professor at the Department of Compute Science at Emory University. Before that, he was an assistant professor in the Department of Information Science and Technology and the Department of Computer Science at George Mason University. He obtained his PhD degree in 2016 from Computer Science Department at Virginia Tech in the United States. His research interests include data mining, artificial intelligence, and machine learning, with special interests in spatiotemporal and network data mining, deep learning on graphs, nonconvex optimization, model parallelism, event prediction, and interpretable machine learning. He received AWS Machine Learning Research Award in 2020 from Amazon Company for his research on distributed graph neural networks. He won NSF Career Award in 2020 awarded by National Science Foundation for his research on deep learning for spatial networks, and Jeffress Trust Award in 2019 for his research on deep generative models for biomolecules, awarded by Jeffress Memorial Trust Foundation and Bank of America. He won the Best Paper Award in the 19th IEEE International Conference on Data Mining (ICDM 2019) for the paper of his lab on deep graph transformation. He has also won Best Paper Award Shortlist in the 27th Web Conference (WWW 2021) for deep generative models. He was selected as “Top 20 Rising Star in Data Mining” by Microsoft Search in 2016 for his research on spatiotemporal data mining. He has also won Outstanding Doctoral Student in the Department of Computer Science at Virginia Tech in 2017. He is awarded as CIFellow Mentor 2021 by the Computing Community Consortium for his research on deep learning for spatial data. He has published numerous research papers in top-tier conferences and journals such as KDD, TKDE, ICDM, ICLR, Proceedings of the IEEE, ACM Computing Surveys, TKDD, IJCAI, AAAI, and WWW. He has been serving as organizers such as publication chair, poster chair, and session chair for many top-tier conferences such as SIGSPATIAL, KDD, ICDM, and CIKM.

List of Contributors
Miltiadis Allamanis Microsoft Research, Cambridge, UK Yu Chen Facebook AI, Menlo Park, CA, USA Yunfei Chu Alibaba Group, Hangzhou, China Peng Cui Tsinghua University, Beijing, China Tyler Derr Vanderbilt University, Nashville, TN, USA Keyu Duan Texas A&M University, College Station, TX, USA Qizhang Feng Texas A&M University, College Station, TX, USA Stephan Gu¨nnemann Technical University of Munich, Mu¨nchen, Germany Xiaojie Guo JD.COM Silicon Valley Research Center, Mountain View, CA, USA Yu Hou Weill Cornell Medicine, New York City, New York, USA Xia Hu Texas A&M University, College Station, TX, USA Junzhou Huang University of Texas at Arlington, Arlington, TA, United States Shouling Ji
xv

xvi
Zhejiang University, Hangzhou, China Wei Jin Michigan State University, East Lansing, MI, USA Anowarul Kabir George Mason University, Fairfax, VA, USA Seyed Mehran Kazemi Borealis AI, Montreal, Canada. Jure Leskovec Stanford University, Stanford, CA, USA Juncheng Li Zhejiang University, Hangzhou, China Jiacheng Li Zhejiang University, Hangzhou, China Pan Li Purdue University, Lafayette, IN, USA Yanhua Li Worcester Polytechnic Institute, Worcester, MA, USA Renjie Liao University of Toronto, Toronto, Canada Xiang Ling Zhejiang University, Hangzhou, China Bang Liu University of Montreal, Montreal, Canada Ninghao Liu Texas A&M University, College Station, TX, USA Zirui Liu Texas A&M University, College Station, TX, USA Hehuan Ma University of Texas at Arlington, College Station, TX, USA Collin McMillan University of Notre Dame, Notre Dame, IN, USA Christopher Morris Polytechnique Montre´al, Montre´al, Canada Zongshen Mu Zhejiang University, Hangzhou, China Menghai Pan

List of Contributors

List of Contributors

xvii

Worcester Polytechnic Institute, Worcester, MA, USA Jian Pei Simon Fraser University, British Columbia, Canada Yu Rong Tencent AI Lab, Shenzhen, China Amarda Shehu George Mason University, Fairfax, VA, USA Kai Shen Zhejiang University, Hangzhou, China Chuan Shi Beijing University of Posts and Telecommunications, Beijing, China Le Song Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates Chang Su Weill Cornell Medicine, New York City, New York. USA Jian Tang Mila-Quebec AI Institute, HEC Montreal, Canada Siliang Tang Zhejiang University, Hangzhou, China Fei Wang Weill Cornell Medicine, New York City, New York, USA Shen Wang University of Illinois at Chicago, Chicago, IL, USA Shiyu Wang Emory University, Atlanta, GA, USA Xiao Wang Beijing University of Posts and Telecommunications, Beijing, China Yu Wang Vanderbilt University, Nashville, TN, USA Chunming Wu Zhejiang University, Hangzhou, China Lingfei Wu JD.COM Silicon Valley Research Center, Mountain View, CA, USA Hongxia Yang Alibaba Group, Hangzhou, China Jiangchao Yao

xviii
Alibaba Group, Hangzhou, China Philip S. Yu University of Illinois at Chicago, Chicago, IL, USA Muhan Zhang Peking University, Beijing, China Wenqiao Zhang Zhejiang University, Hangzhou, China Liang Zhao Emory University, Atlanta, GA, USA Chang Zhou Alibaba Group, Hangzhou, China Kaixiong Zhou Texas A&M University, TX, USA Xun Zhou University of Iowa, Iowa City, IA, USA

List of Contributors

Contents
Terminologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxxi 1 Basic concepts of Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxxi 2 Machine Learning on Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxxii 3 Graph Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxxii
Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxxv Part I Introduction 1 Representation Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
Liang Zhao, Lingfei Wu, Peng Cui and Jian Pei 1.1 Representation Learning: An Introduction . . . . . . . . . . . . . . . . . . . . . 3 1.2 Representation Learning in Different Areas . . . . . . . . . . . . . . . . . . . 5
1.2.1 Representation Learning for Image Processing . . . . . . . . . 5 1.2.2 Representation Learning for Speech Recognition . . . . . . . 8 1.2.3 Representation Learning for Natural Language Processing 10 1.2.4 Representation Learning for Networks . . . . . . . . . . . . . . . . 13 1.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2 Graph Representation Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Peng Cui, Lingfei Wu, Jian Pei, Liang Zhao and Xiao Wang 2.1 Graph Representation Learning: An Introduction . . . . . . . . . . . . . . . 17 2.2 Traditional Graph Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.3 Modern Graph Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.3.1 Structure-Property Preserving Graph Representation
Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.3.2 Graph Representation Learning with Side Information . . . 23 2.3.3 Advanced Information Preserving Graph
Representation Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.4 Graph Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
xix

xx

Contents

3 Graph Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao and Le Song 3.1 Graph Neural Networks: An Introduction . . . . . . . . . . . . . . . . . . . . . 28 3.2 Graph Neural Networks: Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.2.1 Graph Neural Networks: Foundations . . . . . . . . . . . . . . . . . 29 3.2.2 Graph Neural Networks: Frontiers . . . . . . . . . . . . . . . . . . . 31 3.2.3 Graph Neural Networks: Applications . . . . . . . . . . . . . . . . 33 3.2.4 Graph Neural Networks: Organization . . . . . . . . . . . . . . . . 35 3.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

Part II Foundations of Graph Neural Networks

4 Graph Neural Networks for Node Classification . . . . . . . . . . . . . . . . . . 41 Jian Tang and Renjie Liao 4.1 Background and Problem Definition . . . . . . . . . . . . . . . . . . . . . . . . . 41 4.2 Supervised Graph Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 42 4.2.1 General Framework of Graph Neural Networks . . . . . . . . 43 4.2.2 Graph Convolutional Networks . . . . . . . . . . . . . . . . . . . . . . 44 4.2.3 Graph Attention Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 46 4.2.4 Neural Message Passing Networks . . . . . . . . . . . . . . . . . . . 48 4.2.5 Continuous Graph Neural Networks . . . . . . . . . . . . . . . . . . 48 4.2.6 Multi-Scale Spectral Graph Convolutional Networks . . . . 51 4.3 Unsupervised Graph Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . 54 4.3.1 Variational Graph Auto-Encoders . . . . . . . . . . . . . . . . . . . . 54 4.3.2 Deep Graph Infomax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 4.4 Over-smoothing Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
5 The Expressive Power of Graph Neural Networks . . . . . . . . . . . . . . . . 63 Pan Li and Jure Leskovec 5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 5.2 Graph Representation Learning and Problem Formulation . . . . . . . 67 5.3 The Power of Message Passing Graph Neural Networks . . . . . . . . . 70 5.3.1 Preliminaries: Neural Networks for Sets . . . . . . . . . . . . . . . 70 5.3.2 Message Passing Graph Neural Networks . . . . . . . . . . . . . 71 5.3.3 The Expressive Power of MP-GNN . . . . . . . . . . . . . . . . . . 72 5.3.4 MP-GNN with the Power of the 1-WL Test . . . . . . . . . . . . 75 5.4 Graph Neural Networks Architectures that are more Powerful than 1-WL Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 5.4.1 Limitations of MP-GNN . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 5.4.2 Injecting Random Attributes . . . . . . . . . . . . . . . . . . . . . . . . 79 5.4.3 Injecting Deterministic Distance Attributes . . . . . . . . . . . . 86 5.4.4 Higher-order Graph Neural Networks . . . . . . . . . . . . . . . . . 92 5.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97

Contents

xxi

6 Graph Neural Networks: Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 Hehuan Ma, Yu Rong, and Junzhou Huang 6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 6.2 Preliminary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 6.3 Sampling Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 6.3.1 Node-wise Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 6.3.2 Layer-wise Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 6.3.3 Graph-wise Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 6.4 Applications of Large-scale Graph Neural Networks on Recommendation Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 6.4.1 Item-item Recommendation . . . . . . . . . . . . . . . . . . . . . . . . . 116 6.4.2 User-item Recommendation . . . . . . . . . . . . . . . . . . . . . . . . . 116 6.5 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

7 Interpretability in Graph Neural Networks . . . . . . . . . . . . . . . . . . . . . . 121 Ninghao Liu and Qizhang Feng and Xia Hu 7.1 Background: Interpretability in Deep Models . . . . . . . . . . . . . . . . . . 121 7.1.1 Definition of Interpretability and Interpretation . . . . . . . . . 122 7.1.2 The Value of Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . 123 7.1.3 Traditional Interpretation Methods . . . . . . . . . . . . . . . . . . . 124 7.1.4 Opportunities and Challenges . . . . . . . . . . . . . . . . . . . . . . . 127 7.2 Explanation Methods for Graph Neural Networks . . . . . . . . . . . . . . 128 7.2.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 7.2.2 Approximation-Based Explanation . . . . . . . . . . . . . . . . . . . 130 7.2.3 Relevance-Propagation Based Explanation . . . . . . . . . . . . 134 7.2.4 Perturbation-Based Approaches . . . . . . . . . . . . . . . . . . . . . . 135 7.2.5 Generative Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 7.3 Interpretable Modeling on Graph Neural Networks . . . . . . . . . . . . . 138 7.3.1 GNN-Based Attention Models . . . . . . . . . . . . . . . . . . . . . . . 138 7.3.2 Disentangled Representation Learning on Graphs . . . . . . . 141 7.4 Evaluation of Graph Neural Networks Explanations . . . . . . . . . . . . 143 7.4.1 Benchmark Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 7.4.2 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 7.5 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146

8 Graph Neural Networks: Adversarial Robustness . . . . . . . . . . . . . . . . 149 Stephan Gu¨nnemann 8.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 8.2 Limitations of Graph Neural Networks: Adversarial Examples . . . 152 8.2.1 Categorization of Adversarial Attacks . . . . . . . . . . . . . . . . 152 8.2.2 The Effect of Perturbations and Some Insights . . . . . . . . . 156 8.2.3 Discussion and Future Directions . . . . . . . . . . . . . . . . . . . . 159 8.3 Provable Robustness: Certificates for Graph Neural Networks . . . . 160 8.3.1 Model-Specific Certificates . . . . . . . . . . . . . . . . . . . . . . . . . 160 8.3.2 Model-Agnostic Certificates . . . . . . . . . . . . . . . . . . . . . . . . 163 8.3.3 Advanced Certification and Discussion . . . . . . . . . . . . . . . 165

xxii

Contents

8.4 Improving Robustness of Graph Neural Networks . . . . . . . . . . . . . . 165 8.4.1 Improving the Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 8.4.2 Improving the Training Procedure . . . . . . . . . . . . . . . . . . . . 167 8.4.3 Improving the Graph Neural Networks’ Architecture . . . . 170 8.4.4 Discussion and Future Directions . . . . . . . . . . . . . . . . . . . . 171
8.5 Proper Evaluation in the View of Robustness . . . . . . . . . . . . . . . . . . 172 8.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175

Part III Frontiers of Graph Neural Networks

9 Graph Neural Networks: Graph Classification . . . . . . . . . . . . . . . . . . . 179 Christopher Morris 9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179 9.2 Graph neural networks for graph classification: Classic works and modern architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180 9.2.1 Spatial approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 9.2.2 Spectral approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184 9.3 Pooling layers: Learning graph-level outputs from node-level outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 9.3.1 Attention-based pooling layers . . . . . . . . . . . . . . . . . . . . . . 187 9.3.2 Cluster-based pooling layers . . . . . . . . . . . . . . . . . . . . . . . . 187 9.3.3 Other pooling layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188 9.4 Limitations of graph neural networks and higher-order layers for graph classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 9.4.1 Overcoming limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 9.5 Applications of graph neural networks for graph classification . . . . 191 9.6 Benchmark Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192 9.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
10 Graph Neural Networks: Link Prediction . . . . . . . . . . . . . . . . . . . . . . . 195 Muhan Zhang 10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195 10.2 Traditional Link Prediction Methods . . . . . . . . . . . . . . . . . . . . . . . . . 197 10.2.1 Heuristic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197 10.2.2 Latent-Feature Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200 10.2.3 Content-Based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 10.3 GNN Methods for Link Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 10.3.1 Node-Based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 10.3.2 Subgraph-Based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 206 10.3.3 Comparing Node-Based Methods and Subgraph-Based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 10.4 Theory for Link Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 10.4.1 γ-Decaying Heuristic Theory . . . . . . . . . . . . . . . . . . . . . . . . 211 10.4.2 Labeling Trick . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217 10.5 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220 10.5.1 Accelerating Subgraph-Based Methods . . . . . . . . . . . . . . . 220

Contents

xxiii

10.5.2 Designing More Powerful Labeling Tricks . . . . . . . . . . . . . 221 10.5.3 Understanding When to Use One-Hot Features . . . . . . . . . 222 11 Graph Neural Networks: Graph Generation . . . . . . . . . . . . . . . . . . . . . 225 Renjie Liao 11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225 11.2 Classic Graph Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226 11.2.1 Erdo˝s–Re´nyi Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226 11.2.2 Stochastic Block Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 11.3 Deep Graph Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 11.3.1 Representing Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230 11.3.2 Variational Auto-Encoder Methods . . . . . . . . . . . . . . . . . . . 230 11.3.3 Deep Autoregressive Methods . . . . . . . . . . . . . . . . . . . . . . . 236 11.3.4 Generative Adversarial Methods . . . . . . . . . . . . . . . . . . . . . 244 11.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250 12 Graph Neural Networks: Graph Transformation . . . . . . . . . . . . . . . . . 251 Xiaojie Guo, Shiyu Wang, Liang Zhao 12.1 Problem Formulation of Graph Transformation . . . . . . . . . . . . . . . . 252 12.2 Node-level Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253 12.2.1 Definition of Node-level Transformation . . . . . . . . . . . . . . 253 12.2.2 Interaction Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253 12.2.3 Spatio-Temporal Convolution Recurrent Neural Networks254 12.3 Edge-level Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256 12.3.1 Definition of Edge-level Transformation . . . . . . . . . . . . . . 256 12.3.2 Graph Transformation Generative Adversarial Networks . 257 12.3.3 Multi-scale Graph Transformation Networks . . . . . . . . . . . 259 12.3.4 Graph Transformation Policy Networks . . . . . . . . . . . . . . . 260 12.4 Node-Edge Co-Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 12.4.1 Definition of Node-Edge Co-Transformation . . . . . . . . . . . 261 12.4.2 Editing-based Node-Edge Co-Transformation . . . . . . . . . . 266 12.5 Other Graph-based Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . 271 12.5.1 Sequence-to-Graph Transformation . . . . . . . . . . . . . . . . . . 271 12.5.2 Graph-to-Sequence Transformation . . . . . . . . . . . . . . . . . . 272 12.5.3 Context-to-Graph Transformation . . . . . . . . . . . . . . . . . . . . 273 12.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 13 Graph Neural Networks: Graph Matching . . . . . . . . . . . . . . . . . . . . . . 277 Xiang Ling, Lingfei Wu, Chunming Wu and Shouling Ji 13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278 13.2 Graph Matching Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279 13.2.1 Problem Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280 13.2.2 Deep Learning based Models . . . . . . . . . . . . . . . . . . . . . . . . 282 13.2.3 Graph Neural Network based Models . . . . . . . . . . . . . . . . . 284 13.3 Graph Similarity Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288 13.3.1 Problem Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288

xxiv

Contents

13.3.2 Graph-Graph Regression Tasks . . . . . . . . . . . . . . . . . . . . . . 290 13.3.3 Graph-Graph Classification Tasks . . . . . . . . . . . . . . . . . . . . 293 13.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295 14 Graph Neural Networks: Graph Structure Learning . . . . . . . . . . . . . . 297 Yu Chen and Lingfei Wu 14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297 14.2 Traditional Graph Structure Learning . . . . . . . . . . . . . . . . . . . . . . . . . 299 14.2.1 Unsupervised Graph Structure Learning . . . . . . . . . . . . . . . 299 14.2.2 Supervised Graph Structure Learning . . . . . . . . . . . . . . . . . 301 14.3 Graph Structure Learning for Graph Neural Networks . . . . . . . . . . . 303 14.3.1 Joint Graph Structure and Representation Learning . . . . . 304 14.3.2 Connections to Other Problems . . . . . . . . . . . . . . . . . . . . . . 317 14.4 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319 14.4.1 Robust Graph Structure Learning . . . . . . . . . . . . . . . . . . . . 319 14.4.2 Scalable Graph Structure Learning . . . . . . . . . . . . . . . . . . . 320 14.4.3 Graph Structure Learning for Heterogeneous Graphs . . . . 320 14.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320 15 Dynamic Graph Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323 Seyed Mehran Kazemi 15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323 15.2 Background and Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325 15.2.1 Graph Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325 15.2.2 Sequence Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327 15.2.3 Encoder-Decoder Framework and Model Training . . . . . . 330 15.3 Categories of Dynamic Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331 15.3.1 Discrete vs. Continues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331 15.3.2 Types of Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333 15.3.3 Prediction Problems, Interpolation, and Extrapolation . . . 334 15.4 Modeling Dynamic Graphs with Graph Neural Networks . . . . . . . . 335 15.4.1 Conversion to Static Graphs . . . . . . . . . . . . . . . . . . . . . . . . . 335 15.4.2 Graph Neural Networks for DTDGs . . . . . . . . . . . . . . . . . . 337 15.4.3 Graph Neural Networks for CTDGs . . . . . . . . . . . . . . . . . . 340 15.5 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 15.5.1 Skeleton-based Human Activity Recognition . . . . . . . . . . . 343 15.5.2 Traffic Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345 15.5.3 Temporal Knowledge Graph Completion . . . . . . . . . . . . . . 346 15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348 16 Heterogeneous Graph Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . 351 Chuan Shi 16.1 Introduction to HGNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351 16.1.1 Basic Concepts of Heterogeneous Graphs . . . . . . . . . . . . . 353 16.1.2 Challenges of HG Embedding . . . . . . . . . . . . . . . . . . . . . . . 354 16.1.3 Brief Overview of Current Development . . . . . . . . . . . . . . 355

Contents

xxv

16.2 Shallow Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356 16.2.1 Decomposition-based Methods . . . . . . . . . . . . . . . . . . . . . . 357 16.2.2 Random Walk-based Methods . . . . . . . . . . . . . . . . . . . . . . . 358
16.3 Deep Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360 16.3.1 Message Passing-based Methods (HGNNs) . . . . . . . . . . . . 360 16.3.2 Encoder-decoder-based Methods . . . . . . . . . . . . . . . . . . . . . 363 16.3.3 Adversarial-based Methods . . . . . . . . . . . . . . . . . . . . . . . . . 364
16.4 Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366 16.5 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
16.5.1 Structures and Properties Preservation . . . . . . . . . . . . . . . . 367 16.5.2 Deeper Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367 16.5.3 Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368 16.5.4 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369 17 Graph Neural Networks: AutoML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371 Kaixiong Zhou, Zirui Liu, Keyu Duan and Xia Hu 17.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372 17.1.1 Notations of AutoGNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373 17.1.2 Problem Definition of AutoGNN . . . . . . . . . . . . . . . . . . . . . 375 17.1.3 Challenges in AutoGNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375 17.2 Search Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376 17.2.1 Architecture Search Space . . . . . . . . . . . . . . . . . . . . . . . . . . 377 17.2.2 Training Hyperparameter Search Space . . . . . . . . . . . . . . . 380 17.2.3 Efficient Search Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381 17.3 Search Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382 17.3.1 Random Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382 17.3.2 Evolutionary Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382 17.3.3 Reinforcement Learning Based Search . . . . . . . . . . . . . . . . 383 17.3.4 Differentiable Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385 17.3.5 Efficient Performance Estimation . . . . . . . . . . . . . . . . . . . . 386 17.4 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387 18 Graph Neural Networks: Self-supervised Learning . . . . . . . . . . . . . . . 391 Yu Wang, Wei Jin, and Tyler Derr 18.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392 18.2 Self-supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393 18.3 Applying SSL to Graph Neural Networks: Categorizing Training Strategies, Loss Functions and Pretext Tasks . . . . . . . . . . . . . . . . . . . 395 18.3.1 Training Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396 18.3.2 Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399 18.3.3 Pretext Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402 18.4 Node-level SSL Pretext Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403 18.4.1 Structure-based Pretext Tasks . . . . . . . . . . . . . . . . . . . . . . . 403 18.4.2 Feature-based Pretext Tasks . . . . . . . . . . . . . . . . . . . . . . . . . 404 18.4.3 Hybrid Pretext Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406 18.5 Graph-level SSL Pretext Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408

xxvi

Contents

18.5.1 Structure-based Pretext Tasks . . . . . . . . . . . . . . . . . . . . . . . 408 18.5.2 Feature-based Pretext Tasks . . . . . . . . . . . . . . . . . . . . . . . . . 413 18.5.3 Hybrid Pretext Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414 18.6 Node-graph-level SSL Pretext Tasks . . . . . . . . . . . . . . . . . . . . . . . . . 417 18.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418 18.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419

Part IV Broad and Emerging Applications with Graph Neural Networks

19 Graph Neural Networks in Modern Recommender Systems . . . . . . . . 423 Yunfei Chu, Jiangchao Yao, Chang Zhou and Hongxia Yang 19.1 Graph Neural Networks for Recommender System in Practice . . . . 423 19.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423 19.1.2 Classic Approaches to Predict User-Item Preference . . . . 428 19.1.3 Item Recommendation in user-item Recommender Systems: a Bipartite Graph Perspective . . . . . . . . . . . . . . . 429 19.2 Case Study 1: Dynamic Graph Neural Networks Learning . . . . . . . 431 19.2.1 Dynamic Sequential Graph . . . . . . . . . . . . . . . . . . . . . . . . . 431 19.2.2 DSGL: Dynamic Sequential Graph Learning . . . . . . . . . . . 432 19.2.3 Model Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435 19.2.4 Experiments and Discussions . . . . . . . . . . . . . . . . . . . . . . . . 436 19.3 Case Study 2: Device-Cloud Collaborative Learning for Graph Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438 19.3.1 The proposed framework . . . . . . . . . . . . . . . . . . . . . . . . . . . 438 19.3.2 Experiments and Discussions . . . . . . . . . . . . . . . . . . . . . . . . 442 19.4 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
20 Graph Neural Networks in Computer Vision . . . . . . . . . . . . . . . . . . . . 447 Siliang Tang, Wenqiao Zhang, Zongshen Mu, Kai Shen, Juncheng Li, Jiacheng Li and Lingfei Wu 20.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448 20.2 Representing Vision as Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448 20.2.1 Visual Node representation . . . . . . . . . . . . . . . . . . . . . . . . . 448 20.2.2 Visual Edge representation . . . . . . . . . . . . . . . . . . . . . . . . . . 450 20.3 Case Study 1: Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451 20.3.1 Object Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451 20.3.2 Image Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453 20.4 Case Study 2: Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454 20.4.1 Video Action Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . 454 20.4.2 Temporal Action Localization . . . . . . . . . . . . . . . . . . . . . . . 456 20.5 Other Related Work: Cross-media . . . . . . . . . . . . . . . . . . . . . . . . . . . 457 20.5.1 Visual Caption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457 20.5.2 Visual Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . 458 20.5.3 Cross-Media Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459 20.6 Frontiers for Graph Neural Networks on Computer Vision . . . . . . . 460 20.6.1 Advanced Graph Neural Networks for Computer Vision . 460

Contents

xxvii

20.6.2 Broader Area of Graph Neural Networks on Computer Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
20.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462 21 Graph Neural Networks in Natural Language Processing . . . . . . . . . . 463
Bang Liu, Lingfei Wu 21.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463 21.2 Modeling Text as Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
21.2.1 Graph Representations in Natural Language Processing . . 466 21.2.2 Tackling Natural Language Processing Tasks from a
Graph Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468 21.3 Case Study 1: Graph-based Text Clustering and Matching . . . . . . . 470
21.3.1 Graph-based Clustering for Hot Events Discovery and Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
21.3.2 Long Document Matching with Graph Decomposition and Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473
21.4 Case Study 2: Graph-based Multi-Hop Reading Comprehension . . 475 21.5 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479 21.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480 22 Graph Neural Networks in Program Analysis . . . . . . . . . . . . . . . . . . . . 483 Miltiadis Allamanis 22.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483 22.2 Machine Learning in Program Analysis . . . . . . . . . . . . . . . . . . . . . . . 484 22.3 A Graph Represention of Programs . . . . . . . . . . . . . . . . . . . . . . . . . . 486 22.4 Graph Neural Networks for Program Graphs . . . . . . . . . . . . . . . . . . 489 22.5 Case Study 1: Detecting Variable Misuse Bugs . . . . . . . . . . . . . . . . . 491 22.6 Case Study 2: Predicting Types in Dynamically Typed Languages . 493 22.7 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495 23 Graph Neural Networks in Software Mining . . . . . . . . . . . . . . . . . . . . . 499 Collin McMillan 23.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499 23.2 Modeling Software as a Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500
23.2.1 Macro versus Micro Representations . . . . . . . . . . . . . . . . . 501 23.2.2 Combining the Macro- and Micro-level . . . . . . . . . . . . . . . 503 23.3 Relevant Software Mining Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503 23.4 Example Software Mining Task: Source Code Summarization . . . . 504 23.4.1 Primer GNN-based Code Summarization . . . . . . . . . . . . . . 505 23.4.2 Directions for Improvement . . . . . . . . . . . . . . . . . . . . . . . . . 510 23.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512 24 GNN-based Biomedical Knowledge Graph Mining in Drug Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517 Chang Su, Yu Hou, Fei Wang 24.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517

xxviii

Contents

24.2 Existing Biomedical Knowledge Graphs . . . . . . . . . . . . . . . . . . . . . . 518 24.3 Inference on Knowledge Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
24.3.1 Conventional KG inference techniques . . . . . . . . . . . . . . . . 523 24.3.2 GNN-based KG inference techniques . . . . . . . . . . . . . . . . . 524 24.4 KG-based hypothesis generation in computational drug development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528 24.4.1 A machine learning framework for KG-based drug
repurposing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529 24.4.2 Application of KG-based drug repurposing in COVID-19 530 24.5 Future directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 531 24.5.1 KG quality control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 532 24.5.2 Scalable inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533 24.5.3 Coupling KGs with other biomedical data . . . . . . . . . . . . . 533 25 Graph Neural Networks in Predicting Protein Function and Interactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541 Anowarul Kabir and Amarda Shehu 25.1 From Protein Interactions to Function: An Introduction . . . . . . . . . . 541 25.1.1 Enter Stage Left: Protein-Protein Interaction Networks . . 542 25.1.2 Problem Formulation(s), Assumptions, and Noise: A
Historical Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543 25.1.3 Shallow Machine Learning Models over the Years . . . . . . 543 25.1.4 Enter Stage Right: Graph Neural Networks . . . . . . . . . . . . 544 25.2 Highlighted Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547 25.2.1 Case Study 1: Prediction of Protein-Protein and
Protein-Drug Interactions: The Link Prediction Problem . 547 25.2.2 Case Study 2: Prediction of Protein Function and
Functionally-important Residues . . . . . . . . . . . . . . . . . . . . . 549 25.2.3 Case Study 3: From Representation Learning to
Multirelational Link Prediction in Biological Networks with Graph Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . 553 25.3 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555 26 Graph Neural Networks in Anomaly Detection . . . . . . . . . . . . . . . . . . . 557 Shen Wang, Philip S. Yu 26.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557 26.2 Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561 26.2.1 Data-specific issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561 26.2.2 Task-specific Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563 26.2.3 Model-specific Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563 26.3 Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564 26.3.1 Graph Construction and Transformation . . . . . . . . . . . . . . . 564 26.3.2 Graph Representation Learning . . . . . . . . . . . . . . . . . . . . . . 565 26.3.3 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567 26.4 Taxonomy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568 26.5 Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568

Contents

xxix

26.5.1 Case Study 1: Graph Embeddings for Malicious Accounts Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 569
26.5.2 Case Study 2: Hierarchical Attention Mechanism based Cash-out User Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570
26.5.3 Case Study 3: Attentional Heterogeneous Graph Neural Networks for Malicious Program Detection . . . . . . . . . . . . 572
26.5.4 Case Study 4: Graph Matching Framework to Learn the Program Representation and Similarity Metric via Graph Neural Networks for Unknown Malicious Program Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573
26.5.5 Case Study 5: Anomaly Detection in Dynamic Graph Using Attention-based Temporal GCN . . . . . . . . . . . . . . . . 575
26.5.6 Case Study 6: GCN-based Anti-Spam for Spam Review Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 576
26.6 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577 27 Graph Neural Networks in Urban Intelligence . . . . . . . . . . . . . . . . . . . 579
Yanhua Li, Xun Zhou, and Menghai Pan 27.1 Graph Neural Networks for Urban Intelligence . . . . . . . . . . . . . . . . . 580
27.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580 27.1.2 Application scenarios in urban intelligence . . . . . . . . . . . . 581 27.1.3 Representing urban systems as graphs . . . . . . . . . . . . . . . . 584 27.1.4 Case Study 1: Graph Neural Networksin urban
configuration and transportation . . . . . . . . . . . . . . . . . . . . . 586 27.1.5 Case Study 2: Graph Neural Networks in urban
anomaly and event detection . . . . . . . . . . . . . . . . . . . . . . . . 589 27.1.6 Case Study 3: Graph Neural Networks in urban human
behavior inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 590 27.1.7 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592

References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595

Terminologies
This chapter describes a list of definitions of terminologies related to graph neural networks used throughout this book.
1 Basic concepts of Graphs
• Graph: A graph is composed of a node set and an edge set, where nodes represent entities and edges represent the relationship between entities. The nodes and edges form the topology structure of the graph. Besides the graph structure, nodes, edges, and/or the whole graph can be associated with rich information represented as node/edge/graph features (also known as attributes or contents).
• Subgraph: A subgraph is a graph whose set of nodes and set of edges are all subsets of the original graph.
• Centrality: A centrality is a measurement of the importance of nodes in the graph. The basic assumption of centrality is that a node is thought to be important if many other important nodes also connect to it. Common centrality measurements include the degree centrality, the eigenvector centrality, the betweenness centrality, and the closeness centrality.
• Neighborhood: The neighborhood of a node generally refers to other nodes that are close to it. For example, the k-order neighborhood of a node, also called the k-step neighborhood, denotes a set of other nodes in which the shortest path distance between these nodes and the central node is no larger than k.
• Community Structure: A community refers to a group of nodes that are densely connected internally and less densely connected externally.
• Graph Sampling: Graph sampling is a technique to pick a subset of nodes and/ or edges from the original graph. Graph sampling can be applied to train machine learning models on large-scale graphs while preventing severe scalability issues.
xxxi

xxxii

Terminologies

• Heterogeneous Graphs: Graphs are called heterogeneous if the nodes and/or edges of the graph are from different types. A typical example of heteronomous graphs is knowledge graphs where the edges are composed of different types.
• Hypergraphs: Hypergraphs are generalizations of graphs in which an edge can join any number of nodes.
• Random Graph: Random graph generally aims to model the probability distributions over graphs that the observed graphs are generated from. The most basic and well-studied random graph model, known as the Erdos–Renyi model, assumes that the node set is fixed and each edge is identically and independently generated.
• Dynamic Graph: Dynamic graph refers to when at least one component of the graph data changes over time, e.g., adding or deleting nodes, adding or deleting edges, changing edges weights or changing node attributes, etc. If graphs are not dynamic, we refer to them as static graphs.

2 Machine Learning on Graphs
• Spectral Graph Theory: Spectral graph theory analyzes matrices associated with the graph such as its adjacency matrix or Laplacian matrix using tools of linear algebra such as studying the eigenvalues and eigenvectors of the matrix.
• Graph Signal Processing: Graph Signal Processing (GSP) aims to develop tools for processing signals defined on graphs. A graph signal refers to a finite collection of data samples with one sample at each node in the graph.
• Node-level Tasks: Node-level tasks refer to machine learning tasks associated with individual nodes in the graph. Typical examples of node-level tasks include node classification and node regression.
• Edge-level Tasks: Edge-level tasks refer to machine learning tasks associated with a pair of nodes in the graph. A typical example of an edge-level task in link prediction.
• Graph-level Tasks: Graph-level tasks refer to machine learning tasks associated with the whole graph. Typical examples of graph-level tasks include graph classification and graph property prediction.
• Transductive and Inductive Learning: Transductive learning refers to that the targeted instances such as nodes or edges are observed at the training time (though the labels of the targeted instances remain unknown) and inductive learning aims to learn the model which is generalizable to unobserved instances.

3 Graph Neural Networks
• Network embedding: The goal of network embedding is to represent each node in the graph as a low-dimensional vector so that useful information such as the

Terminologies

xxxiii

graph structures and some properties of the graph is preserved in the embedding vectors. Network embedding is also referred to as graph embedding and node representation learning. • Graph Neural Network: Graph neural network refers to any neural network working on the graph data. • Graph Convolutional Network: Graph convolutional network usually refers to a specific graph neural network proposed by Kipf and Welling Kipf and Welling (2017a). It is occasionally used as a synonym for graph neural network, i.e., referring to any neural network working on the graph data, in some literature. • Message-Passing: Message-passing is a framework of graph neural networks in which the key step is to pass messages between different nodes based on graph structures in each neural network layer. The most widely adopted formulation, usually denoted as message-passing neural networks, is to only pass messages between nodes that are directly connected Gilmer et al (2017). The message passing functions are also called graph filters and graph convolutions in some literature. • Readout: Readout refers to functions that summarize the information of individual nodes to form more high-level information such as forming a subgraph/supergraph or obtaining the representations of the entire graph. Readout is also called pooling and graph coarsening in some literature. • Graph Adversarial Attack: Graph adversarial attacks aim to generate worstcase perturbations by manipulating the graph structure and/or node features so that the performance of some models are downgraded. Graph adversarial attacks can be categorized based on the attacker’s goals, capabilities, and accessible knowledge. • Robustness certificates: Methods providing formal guarantees that the prediction of a GNN is not affected even when perturbations are performed based on a certain perturbation model.

Notations

This Chapter provides a concise reference that describes the notations used throughout this book. Numbers, Arrays, and Matrices

A scalar A vector A matrix An identity matrix The set of real numbers The set of complex numbers The set of integers The set of real n-length vectors The set of real m × n matrices The real interval including a and b The real interval including a but excluding b The element of the vector x with index i The element of matrix X’s indexed by Row i and Column j Graph Basics

x x X I
R
C
Z Rn Rm×n [a, b] [a, b) xi Xi, j

A graph Edge set Vertex set Adjacent matrix of a graph Laplacian matrix Diagonal degree matrix Isomorphism between graphs G and H H is a subgraph of graph G H is a proper subgraph of graph G Union of graphs H and G

G
E
V A L D G ∼= H H ⊆G
H ⊂G
G ∪H

xxxv

xxxvi
Intersection of graphs H and G Disjoint Union of graphs H and G Cartesian Product of graphs of graphs H and G The join of graphs H and G

Notations
G ∩H G +H G ×H G ∨H

Basic Operations

Transpose of matrix X Dot product of matrices X and Y Element-wise (Hadamard) product of matrices X and Y Determinant of X p-norm (also called ℓp norm) of x Union Intersection Subset Proper subset Inner prodct of vector x and y

X⊤ X ·Y or XY X ⊙Y det(X ) ∥x∥p ∪
∩
⊆
⊂ < x, y >

Functions

The function f with domain A and range B Derivative of y with respect to x Partial derivative of y with respect to x Gradient of y with respect to x Matrix derivatives of y with respect to matrix X The Hessian matrix of function f at input vector x Definite integral over the entire domain of x Definite integral with respect to x over the set S A function of x parametrized by θ Convolution between functions f and g

f :A→B
dy d∂xy
∇∂ xxy ∇X y ∇2 f (x)
f (x)dx fS(xf;(θx))dx f ∗g

Probablistic Theory

A probability distribution of a

p(a)

A conditional probabilistic distribution of b given a

p(b|a)

The random variables a and b are independent

a⊥b

Variables a and b are conditionally independent given c

a⊥b | c

Random variable a has a distribution p

a∼ p

The expectation of f (a) with respect to the variable a under distri- Ea∼p[ f (a)] bution p

Gaussian distribution over x with mean µ and covariance Σ

N (x; µ, Σ )

Part I
Introduction

Chapter 1
Representation Learning
Liang Zhao, Lingfei Wu, Peng Cui and Jian Pei

Abstract In this chapter, we first describe what representation learning is and why we need representation learning. Among the various ways of learning representations, this chapter focuses on deep learning methods: those that are formed by the composition of multiple non-linear transformations, with the goal of resulting in more abstract and ultimately more useful representations. We summarize the representation learning techniques in different domains, focusing on the unique challenges and models for different data types including images, natural languages, speech signals and networks. Last, we summarize this chapter.

1.1 Representation Learning: An Introduction
The effectiveness of machine learning techniques heavily relies on not only the design of the algorithms themselves, but also a good representation (feature set) of data. Ineffective data representations that lack some important information or contains incorrect or huge redundant information could lead to poor performance of the algorithm in dealing with different tasks. The goal of representation learning is to extract sufficient but minimal information from data. Traditionally, this can be achieved via human efforts based on the prior knowledge and domain expertise on the data and tasks, which is also named as feature engineering. In deploying ma-

Liang Zhao Department of Computer Science, Emory University, e-mail: liang.zhao@emory.edu Lingfei Wu JD.COM Silicon Valley Research Center, e-mail: lwu@email.wm.edu Peng Cui Department of Computer Science, Tsinghua University, e-mail: cuip@tsinghua.edu.cn Jian Pei Department of Computer Science, Simon Fraser University, e-mail: jpei@cs.sfu.ca

© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022

3

L. Wu et al. (eds.), Graph Neural Networks: Foundations, Frontiers, and Applications,

https://doi.org/10.1007/978-981-16-6054-2_1

4

Liang Zhao, Lingfei Wu, Peng Cui and Jian Pei

chine learning and many other artificial intelligence algorithms, historically a large portion of the human efforts goes into the design of prepossessing pipelines and data transformations. More specifically, feature engineering is a way to take advantage of human ingenuity and prior knowledge in the hope to extract and organize the discriminative information from the data for machine learning tasks. For example, political scientists may be asked to define a keyword list as the features of social-media text classifiers for detecting those texts on societal events. For speech transcription recognition, one may choose to extract features from raw sound waves by the operations including Fourier transformations. Although feature engineering is widely adopted over the years, its drawbacks are also salient, including: 1) Intensive labors from domain experts are usually needed. This is because feature engineering may require tight and extensive collaboration between model developers and domain experts. 2) Incomplete and biased feature extraction. Specifically, the capacity and discriminative power of the extracted features are limited by the knowledge of different domain experts. Moreover, in many domains that human beings have limited knowledge, what features to extract itself is an open questions to domain experts, such as cancer early prediction. In order to avoid these drawbacks, making learning algorithms less dependent on feature engineering has been a highly desired goal in machine learning and artificial intelligence domains, so that novel applications could be constructed faster and hopefully addressed more effectively.
The techniques of representation learning witness the development from the traditional representation learning techniques to more advanced ones. The traditional methods belong to “shallow” models and aim to learn transformations of data that make it easier to extract useful information when building classifiers or other predictors, such as Principal Component Analysis (PCA) (Wold et al, 1987), Gaussian Markov random field (GMRF) (Rue and Held, 2005), and Locality Preserving Projections (LPP) (He and Niyogi, 2004). Deep learning-based representation learning is formed by the composition of multiple non-linear transformations, with the goal of yielding more abstract and ultimately more useful representations. In the light of introducing more recent advancements and sticking to the major topic of this book, here we majorly focus on deep learning-based representation learning, which can be categorized into several types: (1) Supervised learning, where a large number of labeled data are needed for the training of the deep learning models. Given the welltrained networks, the output before the last fully-connected layers is always utilized as the final representation of the input data; (2) Unsupervised learning (including self-supervised learning), which facilitates the analysis of input data without corresponding labels and aims to learn the underlying inherent structure or distribution of data. The pre-tasks are utilized to explore the supervision information from large amounts of unlabelled data. Based on this constructed supervision information, the deep neural networks are trained to extract the meaningful representations for the future downstream tasks; (3) Transfer learning, which involves methods that utilize any knowledge resource (i.e., data, model, labels, etc.) to increase model learning and generalization for the target task. Transfer learning encompasses different scenarios including multi-task learning (MTL), model adaptation, knowledge transfer, co-variance shift, etc. There are also other important representation learning meth-

1 Representation Learning

5

ods such as reinforcement learning, few-shot learning, and disentangled representation learning.
It is important to define what is a good representation. As the definition by Bengio (2008), representation learning is about learning the (underlying) features of the data that make it easier to extract useful information when building classifiers or other predictors. Thus, the evaluation of a learned representation is closely related to its performance on the downstream tasks. For example, in the data generation task based on a generative model, a good representation is often the one that captures the posterior distribution of the underlying explanatory factors for the observed input. While for a prediction task, a good representation is the one that captures the minimal but sufficient information of input data to correctly predict the target label. Besides the evaluation from the perspective of the downstream tasks, there are also some general properties that the good representations may hold, such as the smoothness, the linearity, capturing multiple explanatory and casual factors, holding shared factors across different tasks and simple factor dependencies.

1.2 Representation Learning in Different Areas
In this section, we summarize the development of representation learning on four different representative areas: (1) image processing; (2) speech recognition; (3) Natural language processing; and (4) network analysis. For the representation learning in each research area, we consider some of the fundamental questions that have been driving research in this area. Specifically, what makes one representation better than another, and how should we compute its representation? Why is the representation learning important in that area? Also, what are appropriate objectives for learning good representations? We also introduce the relevant typical methods and their development from the perspective of three main categories: supervised representation learning, unsupervised learning and transfer learning, respectively.

1.2.1 Representation Learning for Image Processing
Image representation learning is a fundamental problem in understanding the semantics of various visual data, such as photographs, medical images, document scans, and video streams. Normally, the goal of image representation learning for image processing is to bridge the semantic gap between the pixel data and semantics of the images. The successful achievements of image representation learning have enpowered many real-world problems, including but not limited to image search, facial recognition, medical image analysis, photo manipulation and target detection.
In recent years, we have witnessed a fast advancement of image representation learning from handcrafted feature engineering to that from scratch through deep neural network models. Traditionally, the patterns of images are extracted with the

6

Liang Zhao, Lingfei Wu, Peng Cui and Jian Pei

help of hand-crafted features by human beings based on prior knowledge. For example, Huang et al (2000) extracted the character’s structure features from the strokes, then use them to recognize the handwritten characters. Rui (2005) adopted the morphology method to improve local feature of the characters, then use PCA to extract features of characters. However, all of these methods need to extract features from images manually and thus the prediction performances strongly rely on the prior knowledge. In the field of computer vision, manual feature extraction is very cumbersome and impractical because of the high dimensionality of feature vectors. Thus, representation learning of images which can automatically extract meaningful, hidden and complex patterns from high-dimension visual data is necessary. Deep learning-based representation learning for images is learned in an end-to-end fashion, which can perform much better than hand-crafted features in the target applications, as long as the training data is of sufficient quality and quantity.
Supervised Representation Learning for image processing. In the domain of image processing, supervised learning algorithm, such as Convolution Neural Network (CNN) and Deep Belief Network (DBN), are commonly applied in solving various tasks. One of the earliest deep-supervised-learning-based works was proposed in 2006 (Hinton et al, 2006), which is focused on the MNIST digit image classification problem, outperforming the state-of-the-art SVMs. Following this, deep convolutional neural networks (ConvNets) showed amazing performance which is greatly depends on their properties of shift in-variance, weights sharing and local pattern capturing. Different types of network architectures were developed to increase the capacity of network models, and larger and larger datasets were collected these days. Various networks including AlexNet (Krizhevsky et al, 2012), VGG (Simonyan and Zisserman, 2014b), GoogLeNet (Szegedy et al, 2015), ResNet (He et al, 2016a), and DenseNet (Huang et al, 2017a) and large scale datasets, such as ImageNet and OpenImage, have been proposed to train very deep convolutional neural networks. With the sophisticated architectures and large-scale datasets, the performance of convolutional neural networks keeps outperforming the state-of-the-arts in various computer vision tasks.
Unsupervised Representation Learning for image processing. Collection and annotation of large-scale datasets are time-consuming and expensive in both image datasets and video datasets. For example, ImageNet contains about 1.3 million labeled images covering 1,000 classes while each image is labeled by human workers with one class label. To alleviate the extensive human annotation labors, many unsupervised methods were proposed to learn visual features from large-scale unlabeled images or videos without using any human annotations. A popular solution is to propose various pretext tasks for models to solve, while the models can be trained by learning objective functions of the pretext tasks and the features are learned through this process. Various pretext tasks have been proposed for unsupervised learning, including colorizing gray-scale images (Zhang et al, 2016d) and image inpainting (Pathak et al, 2016). During the unsupervised training phase, a predefined pretext task is designed for the models to solve, and the pseudo labels for the pretext task are automatically generated based on some attributes of data. Then the models are trained according to the objective functions of the pretext tasks. When trained

1 Representation Learning

7

with pretext tasks, the shallower blocks of the deep neural network models focus on the low-level general features such as corners, edges, and textures, while the deeper blocks focus on the high-level task-specific features such as objects, scenes, and object parts. Therefore, the models trained with pretext tasks can learn kernels to capture low-level features and high-level features that are helpful for other downstream tasks. After the unsupervised training is finished, the learned visual features in this pre-trained models can be further transferred to downstream tasks (especially when only relatively small data is available) to improve performance and overcome over-fitting.
Transfer Learning for image processing. In real-world applications, due to the high cost of manual labeling, sufficient training data that belongs to the same feature space or distribution as the testing data may not always be accessible. Transfer learning mimics the human vision system by making use of sufficient amounts of prior knowledge in other related domains (i.e., source domains) when executing new tasks in the given domain (i.e., target domain). In transfer learning, both the training set and the test set can contribute to the target and source domains. In most cases, there is only one target domain for a transfer learning task, while either single or multiple source domains can exist. The techniques of transfer learning in images processing can be categorized into feature representation knowledge transfer and classifier-based knowledge transfer. Specifically, feature representation transfer methods map the target domain to the source domains by exploiting a set of extracted features, where the data divergence between the target domain and the source domains can be significantly reduced so that the performance of the task in the target domain is improved. For example, classifier-based knowledge-transfer methods usually share the common trait that the learned source domain models are utilized as prior knowledge, which are used to learn the target model together with the training samples. Instead of minimizing the cross-domain dissimilarity by updating instances’ representations, classifier-based knowledge-transfer methods aim to learn a new model that minimizes the generalization error in the target domain via the provided training set from both domains and the learned model.
Other Representation Learning for Image Processing. Other types of representation learning are also commonly observed for dealing with image processing, such as reinforcement learning, and semi-supervised learning. For example, reinforcement learning are commonly explored in the task of image captioning Liu et al (2018a); Ren et al (2017) and image editing Kosugi and Yamasaki (2020), where the learning process is formalized as a sequence of actions based on a policy network.

8

Liang Zhao, Lingfei Wu, Peng Cui and Jian Pei

1.2.2 Representation Learning for Speech Recognition

Nowadays, speech interfaces or systems have become widely developed and integrated into various real-life applications and devices. Services like Siri 1, Cortana 2, and Google Voice Search 3 have become a part of our daily life and are used by millions of users. The exploration in speech recognition and analysis has always been motivated by a desire to enable machines to participate in verbal human-machine interactions. The research goals of enabling machines to understand human speech, identify speakers, and detect human emotion have attracted researchers’ attention for more than sixty years across several distinct research areas, including but not limited to Automatic Speech Recognition (ASR), Speaker Recognition (SR), and Speaker Emotion Recognition (SER).
Analyzing and processing speech has been a key application of machine learning (ML) algorithms. Research on speech recognition has traditionally considered the task of designing hand-crafted acoustic features as a separate distinct problem from the task of designing efficient models to accomplish prediction and classification decisions. There are two main drawbacks of this approach: First, the feature engineering is cumbersome and requires human knowledge as introduced above; and second, the designed features might not be the best for the specific speech recognition tasks at hand. This has motivated the adoption of recent trends in the speech community towards the utilization of representation learning techniques, which can learn an intermediate representation of the input signal automatically that better fits into the task at hand and hence lead to improved performance. Among all these successes, deep learning-based speech representations play an important role. One of the major reasons for the utilization of representation learning techniques in speech technology is that speech data is fundamentally different from two-dimensional image data. Images can be analyzed as a whole or in patches, but speech has to be formatted sequentially to capture temporal dependency and patterns.
Supervised representation learning for speech recognition. In the domain of speech recognition and analyzing, supervised representation learning methods are widely employed, where feature representations are learned on datasets by leveraging label information. For example, restricted Boltzmann machines (RBMs) (Jaitly and Hinton, 2011; Dahl et al, 2010) and deep belief networks (DBNs) (Cairong et al, 2016; Ali et al, 2018) are commonly utilized in learning features from speech for different tasks, including ASR, speaker recognition, and SER. For example, in 2012, Microsoft has released a new version of their MAVIS (Microsoft Audio Video Indexing Service) speech system based on context-dependent deep neural networks (Seide et al, 2011). These authors managed to reduce the word error rate on four major benchmarks by about 30% (e.g., from 27.4% to 18.5% on RT03S) com-
1 Siri is an artificial intelligence assistant software that is built into Apple’s iOS system. 2 Microsoft Cortana is an intelligent personal assistant developed by Microsoft, known as ”the world’s first cross-platform intelligent personal assistant”. 3 Google Voice Search is a product of Google that allows you to use Google to search by speaking to a mobile phone or computer, that is, to use the legendary content on the device to be identified by the server, and then search for information based on the results of the recognition

1 Representation Learning

9

pared to the traditional models based on Gaussian mixtures. Convolutional neural networks are another popular supervised models that are widely utilized for feature learning from speech signals in tasks such as speech and speaker recognition (Palaz et al, 2015a,b) and SER Latif et al (2019); Tzirakis et al (2018). Moreover, it has been found that LSTMs (or GRUs) can help CNNs in learning more useful features from speech by learning both the local and long-term dependency (Dahl et al, 2010).
Unsupervised Representation Learning for speech recognition. Unsupervised representation learning from large unlabelled datasets is an active area of speech recognition. In the context of speech analysis, it is able to exploit the practically available unlimited amount of unlabelled corpora to learn good intermediate feature representations, which can then be used to improve the performance of a variety of downstream supervised learning speech recognition tasks or the speech signal synthetic tasks. In the tasks of ASR and SR, most of the works are based on Variational Auto-encoder (VAEs), where a generative model and an inference model are jointly learned, which allows them to capture latent representations from observed speech data (Chorowski et al, 2019; Hsu et al, 2019, 2017). For example, Hsu et al (2017) proposed a hierarchical VAE to capture interpretable and disentangled representations from speech without any supervision. Other auto-encoding architectures like Denoised Autoencoder(DAEs) are also found very promising in finding speech representations in an unsupervised way, especially for noisy speech recognition (Feng et al, 2014; Zhao et al, 2015). Beyond the aforementioned, recently, adversarial learning (AL) is emerging as a powerful tool in learning unsupervised representation for speech, such as generative adversarial nets (GANs). It involves at least a generator and a discriminator, where the former tries to generates as realistic as possible data to obfuscate the latter which also tries its best to deobfuscate. Hence both of the generator and discriminator can be trained and improved iteratively in an adversarial way, which result in more discriminative and robust features. Among these, GANs (Chang and Scherer, 2017; Donahue et al, 2018), adversarial autoencoders (AAEs) Sahu et al (2017) are becoming mostly popular in modeling speech not only in ASR but also SR and SER.
Transfer Learning for speech recognition. Transfer learning (TL) encompasses different approaches, including MTL, model adaptation, knowledge transfer, covariance shift, etc. In the domain of speech recognition, representation learning gained much interest in these approaches of TL including but not limited to domain adaptation, multi-task learning, and self-taught learning. In terms of Domain Adaption, speech is a typical example of heterogeneous data and thus, a mismatch always exists between the probability distributions of source and target domain data. To build more robust systems for speech-related applications in real-life, domain adaptation techniques are usually applied in the training pipeline of deep neural networks to learn representations which are able to explicitly minimize the difference between the distribution of data in the source and target domains (Sun et al, 2017; Swietojanski et al, 2016). In terms of MTL, representations learned can successfully increases the performance of speech recognition without requiring contextual speech data, since speech contains multi-dimensional information (message, speaker, gender, or emotion) that can be used as auxiliary tasks. For example, In the task of ASR, by us-

10

Liang Zhao, Lingfei Wu, Peng Cui and Jian Pei

ing MTL with different auxiliary tasks including gender, speaker adaptation, speech enhancement, it has been shown that the learned shared representations for different tasks can act as complementary information about the acoustic environment and give a lower word error rate (WER) (Parthasarathy and Busso, 2017; Xia and Liu, 2015).
Other Representation Learning for speech recognition. Other than the abovementioned three categories of representation learning for speech signals, there are also some other representation learning techniques commonly explored, such as semi-supervised learning and reinforcement learning. For example, in the speech recognition for ASR, semi-supervised learning is mainly used to circumvent the lack of sufficient training data. This can be achieved either by creating features fronts ends (Thomas et al, 2013), or by using multilingual acoustic representations (Cui et al, 2015), or by extracting an intermediate representation from large unpaired datasets (Karita et al, 2018). RL is also gaining interest in the area of speech recognition, and there have been multiple approaches to model different speech problems, including dialog modeling and optimization (Levin et al, 2000), speech recognition (Shen et al, 2019), and emotion recognition (Sangeetha and Jayasankar, 2019).

1.2.3 Representation Learning for Natural Language Processing
Besides speech recognition, there are many other Natural Language Processing (NLP) applications of representation learning, such as the text representation learning. For example, Google’s image search exploits huge quantities of data to map images and queries in the same space (Weston et al, 2010) based on NLP techniques. In general, there are two types of applications of representation learning in NLP. In one type, the semantic representation, such as the word embedding, is trained in a pre-training task (or directly designed by human experts) and is transferred to the model for the target task. It is trained by using language modeling objective and is taken as inputs for other down-stream NLP models. In the other type, the semantic representation lies within the hidden states of the deep learning model and directly aims for better performance of the target tasks in an end-to-end fashion. For example, many NLP tasks want to semantically compose sentence or document representation, such as tasks like sentiment classification, natural language inference, and relation extraction, which require sentence representation.
Conventional NLP tasks heavily rely on feature engineering, which requires careful design and considerable expertise. Recently, representation learning, especially deep learning-based representation learning is emerging as the most important technique for NLP. First, NLP is typically concerned with multiple levels of language entries, including but not limited to characters, words, phrases, sentences, paragraphs, and documents. Representation learning is able to represent the semantics of these multi-level language entries in a unified semantic space, and model complex semantic dependence among these language entries. Second, there are various NLP tasks that can be conducted on the same input. For example, given a sentence, we

1 Representation Learning

11

can perform multiple tasks such as word segmentation, named entity recognition, relation extraction, co-reference linking, and machine translation. In this case, it will be more efficient and robust to build a unified representation space of inputs for multiple tasks. Last, natural language texts may be collected from multiple domains, including but not limited to news articles, scientific articles, literary works, advertisement and online user-generated content such as product reviews and social media. Moreover, texts can also be collected from different languages, such as English, Chinese, Spanish, Japanese, etc. Compared to conventional NLP systems which have to design specific feature extraction algorithms for each domain according to its characteristics, representation learning enables us to build representations automatically from large-scale domain data and even add bridges among these languages from different domains. Given these advantages of representation learning for NLP in the feature engineering reduction and performance improvement, many researchers have developed efficient algorithms on representation learning, especially deep learning-based approaches, for NLP.
Supervised Representation Learning for NLP. Deep neural networks in the supervised learning setting for NLP emerge from distributed representation learning, then to CNN models, and finally to RNN models in recent years. At early stage, distributed representations are first developed in the context of statistical language modeling by Bengio (2008) in so-called neural net language models. The model is about learning a distributed representation for each word (i.e., word embedding). Following this, the need arose for an effective feature function that extracts higherlevel features from constituting words or n-grams. CNNs turned out to be the natural choice given their properties of excellent performance in computer vision and speech processing tasks. CNNs have the ability to extract salient n-gram features from the input sentence to create an informative latent semantic representation of the sentence for downstream tasks. This domain was pioneered by Collobert et al (2011) and Kalchbrenner et al (2014), which led to a huge proliferation of CNNbased networks in the succeeding literature. The neural net language model was also improved by adding recurrence to the hidden layers (Mikolov et al, 2011a) (i.e., RNN), allowing it to beat the state-of-the-art (smoothed n-gram models) not only in terms of perplexity (exponential of the average negative log-likelihood of predicting the right next word) but also in terms of WER in speech recognition. RNNs use the idea of processing sequential information. The term “recurrent” applies as they perform the same computation over each token of the sequence and each step is dependent on the previous computations and results. Generally, a fixed-size vector is produced to represent a sequence by feeding tokens one by one to a recurrent unit. In a way, RNNs have “memory” over previous computations and use this information in current processing. This template is naturally suited for many NLP tasks such as language modeling (Mikolov et al, 2010, 2011b), machine translation (Liu et al, 2014; Sutskever et al, 2014), and image captioning (Karpathy and Fei-Fei, 2015).
Unsupervised Representation Learning for NLP. Unsupervised learning (including self-supervised learning) has made a great success in NLP, for the plain text itself contains abundant knowledge and patterns about languages. For example, in most deep learning based NLP models, words in sentences are first mapped to their corre-

12

Liang Zhao, Lingfei Wu, Peng Cui and Jian Pei

sponding embeddings via the techniques, such as word2vec Mikolov et al (2013b), GloVe Pennington et al (2014), and BERT Devlin et al (2019), before sending to the networks. However, there are no human-annotated “labels” for learning those word embeddings. To acquire the training objective necessary for neural networks, it is necessary to generate “labels” intrinsically from the existing data. Language modeling is a typical unsupervised learning task, which can construct the probability distribution over sequences of words and does not require human annotations. Based on the distributional hypothesis, using the language modeling objective can lead to hidden representations that encode the semantics of words. Another typical unsupervised learning model in NLP is auto-encoder (AE), which consists of a reduction (encoding) phase and a reconstruction (decoding) phase. For example, recursive auto-encoders (which generalize recurrent networks with VAE) have been used to beat the state-of-the-art at the moment of its publication in full sentence paraphrase detection (Socher et al, 2011) by almost doubling the F1 score for paraphrase detection.
Transfer Learning for NLP. Over the recent years, the field of NLP has witnessed fast growth of transfer learning methods via sequential transfer learning models and architectures, which significantly improved upon the state-of-the-arts on a wide range of NLP tasks. In terms of domain adaption, the sequential transfer learning consists of two stages: a pretraining phase in which general representations are learned on a source task or domain followed by an adaptation phase during which the learned knowledge is applied to a target task or domain. The domain adaption in NLP is categorized into model-centric, data-centric, and hybrid approaches. Model-centric methods target the approaches to augmenting the feature space, as well as altering the loss function, the architecture, or the model parameters (Blitzer et al, 2006). Data-centric methods focus on the data aspect and involve pseudolabeling (or bootstrapping) where only small number of classes are shared between the source and target datasets (Abney, 2007). Lastly, hybrid-based methods are built by both data- and model-centric models. Similarly, great advances have also been made into the multi-task learning in NLP, where different NLP tasks can result in better representation of texts. For example, based on a convolutional architecture, Collobert et al (2011) developed the SENNA system that shares representations across the tasks of language modeling, part-of-speech tagging, chunking, named entity recognition, semantic role labeling, and syntactic parsing. SENNA approaches or sometimes even surpasses the state-of-the-art on these tasks while is simpler and much faster than traditional predictors. Moreover, learning word embeddings can be combined with learning image representations in a way that allow associating texts and images.
Other Representation Learning for NLP. In NLP tasks, when a problem gets more complicated, it requires more knowledge from domain experts to annotate training instances for fine-grained tasks and thus increases the cost of data labeling. Therefore, sometimes it requires the models or systems can be developed efficiently with (very) few labeled data. When each class has only one or a few labeled instances, the problem becomes a one/few-shot learning problem. The few-shot learning problem is derived from computer vision and has also been studied in NLP

1 Representation Learning

13

recently. For example, researchers have explored few-shot relation extractio (Han et al, 2018) where each relation has a few labeled instances, and low-resource machine translation (Zoph et al, 2016) where the size of the parallel corpus is limited.

1.2.4 Representation Learning for Networks
Beyond popular data like images, texts, and sounds, network data is another important data type that is becoming ubiquitous across a large scale of real-world applications ranging from cyber-networks (e.g., social networks, citation networks, telecommunication networks, etc.) to physical networks (e.g., transportation networks, biological networks, etc). Networks data can be formulated as graphs mathematically, where vertices and their relationships jointly characterize the network information. Networks and graphs are very powerful and flexible data formulation such that sometimes we could even consider other data types like images, and texts as special cases of it. For example, images can be considered as grids of nodes with RGB attributes which are special types of graphs, while texts can also be organized into sequential-, tree-, or graph-structured information. So in general, representation learning for networks is widely considered as a promising yet more challenging tasks that require the advancement and generalization of many techniques we developed for images, texts, and so forth. In addition to the intrinsic high complexity of network data, the efficiency of representation learning on networks is also an important issues considering the large-scale of many real-world networks, ranging from hundreds to millions or even billions of vertices. Analyzing information networks plays a crucial role in a variety of emerging applications across many disciplines. For example, in social networks, classifying users into meaningful social groups is useful for many important tasks, such as user search, targeted advertising and recommendations; in communication networks, detecting community structures can help better understand the rumor spreading process; in biological networks, inferring interactions between proteins can facilitate new treatments for diseases. Nevertheless, efficient and effective analysis of these networks heavily relies on good representations of the networks.
Traditional feature engineering on network data usually focuses on obtaining a number of predefined straightforward features in graph levels (e.g., the diameter, average path length, and clustering co-efficient), node levels (e.g., node degree and centrality), or subgraph levels (e.g., frequent subgraphs and graph motifs). Those limited number of hand-crafted, well-defined features, though describe several fundamental aspects of the graphs, discard the patterns that cannot be covered by them. Moreover, real-world network phenomena are usually highly complicated require sophisticated, unknown combinations among those predefined features or cannot be characterized by any of the existing features. In addition, traditional graph feature engineering usually involve expensive computations with super-linear or exponential complexity, which often makes many network analytic tasks computationally expensive and intractable over large-scale networks. For example, in dealing with

14

Liang Zhao, Lingfei Wu, Peng Cui and Jian Pei

the task of community detection, classical methods involve calculating the spectral decomposition of a matrix with at least quadratic time complexity with respect to the number of vertices. This computational overhead makes algorithms hard to scale to large-scale networks with millions of vertices.
More recently, network representation learning (NRL) has aroused a lot of research interest. NRL aims to learn latent, low-dimensional representations of network vertices, while preserving network topology structure, vertex content, and other side information. After new vertex representations are learned, network analytic tasks can be easily and efficiently carried out by applying conventional vectorbased machine learning algorithms to the new representation space. Earlier work related to network representation learning dates back to the early 2000s, when researchers proposed graph embedding algorithms as part of dimensionality reduction techniques. Given a set of independent and identically distributed (i.i.d.) data points as input, graph embedding algorithms first calculate the similarity between pairwise data points to construct an affinity graph, e.g., the k-nearest neighbor graph, and then embed the affinity graph into a new space having much lower dimensionality. However, graph embedding algorithms are designed on i.i.d. data mainly for dimensionality reduction purpose, which usually have at least quadratic time complexity with respect to the number of vertices.
Since 2008, significant research efforts have shifted to the development of effective and scalable representation learning techniques that are directly designed for complex information networks. Many network representation learning algorithms (Perozzi et al, 2014; Yang et al, 2015b; Zhang et al, 2016b; Manessi et al, 2020) have been proposed to embed existing networks, showing promising performance for various applications. These methods embed a network into a latent, low-dimensional space that preserves structure proximity and attribute affinity. The resulting compact, low-dimensional vector representations can be then taken as features to any vector-based machine learning algorithms. This paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space, such as node classification (Zhu et al, 2007), link prediction (Lu¨ and Zhou, 2011), clustering (Malliaros and Vazirgiannis, 2013), network synthesis (You et al, 2018b). The following chapters of this book will then provide a systematic and comprehensive introduction into network representation learning.

1.3 Summary
Representation learning is a very active and important field currently, which heavily influences the effectiveness of machine learning techniques. Representation learning is about learning the representations of the data that makes it easier to extract useful and discriminative information when building classifiers or other predictors. Among the various ways of learning representations, deep learning algorithms have increasingly been employed in many areas nowadays where the good representation can be learned in an efficient and automatic way based on large amount of complex

1 Representation Learning

15

and high dimensional data. The evaluation of a representation is closely related to its performance on the downstream tasks. Generally, there are also some general properties that the good representations may hold, such as the smoothness, the linearity, disentanglement, as well as capturing multiple explanatory and casual factors.
We have summarized the representation learning techniques in different domains, focusing on the unique challenges and models for different areas including the processing of images, natural language, and speech signals. For each area, there emerges many deep learning-based representation techniques from different categories, including supervised learning, unsupervised learning, transfer learning, disentangled representation learning, reinforcement learning, etc. We have also briefly mentioned about the representation learning on networks and its relations to that on images, texts, and speech, in order for the elaboration of it in the following chapters.

Chapter 2
Graph Representation Learning
Peng Cui, Lingfei Wu, Jian Pei, Liang Zhao and Xiao Wang

Abstract Graph representation learning aims at assigning nodes in a graph to lowdimensional representations and effectively preserving the graph structures. Recently, a significant amount of progress has been made toward this emerging graph analysis paradigm. In this chapter, we first summarize the motivation of graph representation learning. Afterwards and primarily, we provide a comprehensive overview of a large number of graph representation learning methods in a systematic manner, covering the traditional graph representation learning, modern graph representation learning, and graph neural networks.

2.1 Graph Representation Learning: An Introduction
Many complex systems take the form of graphs, such as social networks, biological networks, and information networks. It is well recognized that graph data is often sophisticated and thus is challenging to deal with. To process graph data effectively, the first critical challenge is to find effective graph data representation, that is, how to represent graphs concisely so that advanced analytic tasks, such as pattern discovery, analysis, and prediction, can be conducted efficiently in both time and space.
Liang Zhao Department of Computer Science, Emory University, e-mail: liang.zhao@emory.edu Lingfei Wu JD.COM Silicon Valley Research Center, e-mail: lwu@email.wm.edu Peng Cui Department of Computer Science, Tsinghua University, e-mail: cuip@tsinghua.edu.cn Jian Pei Department of Computer Science, Simon Fraser University, e-mail: jpei@cs.sfu.ca Xiao Wang Department of Computer Science, Beijing University of Posts and Telecommunications, e-mail: xiaowang@bupt.edu.cn

© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022

17

L. Wu et al. (eds.), Graph Neural Networks: Foundations, Frontiers, and Applications,

https://doi.org/10.1007/978-981-16-6054-2_2

18

Peng Cui, Lingfei Wu, Jian Pei, Liang Zhao and Xiao Wang

Traditionally, we usually represent a graph as G = (V , E ), where V is a node set and E is an edge set. For large graphs, such as those with billions of nodes, the traditional graph representation poses several challenges to graph processing and analysis.
(1) High computational complexity. These relationships encoded by the edge set E take most of the graph processing or analysis algorithms either iterative or combinatorial computation steps. For example, a popular way is to use the shortest or average path length between two nodes to represent their distance. To compute such a distance using the traditional graph representation, we have to enumerate many possible paths between two nodes, which is in nature a combinatorial problem. Such methods result in high computational complexity that prevents them from being applicable to large-scale real-world graphs.
(2) Low parallelizability. Parallel and distributed computing is de facto to process and analyze large-scale data. Graph data represented in the traditional way, however, casts severe difficulties to design and implementat of parallel and distributed algorithms. The bottleneck is that nodes in a graph are coupled to each other explicitly reflected by E. Thus, distributing different nodes in different shards or servers often causes demandingly high communication cost among servers, and holds back speed-up ratio.
(3) Inapplicability of machine learning methods. Recently, machine learning methods, especially deep learning, are very powerful in many areas. For graph data represented in the traditional way, however, most of the off-the-shelf machine learning methods may not be applicable. Those methods usually assume that data samples can be represented by independent vectors in a vector space, while the samples in graph data (i.e., the nodes) are dependant to each other to some degree determined by E. Although we can simply represent a node by its corresponding row vector in the adjacency matrix of the graph, the extremely high dimensionality of such a representation in a large graph with many nodes makes the in sequel graph processing and analysis difficult.
To tackle these challenges, substantial effort has been committed to develop novel graph representation learning, i.e., learning the dense and continuous lowdimensional vector representations for nodes, so that the noise or redundant information can be reduced and the intrinsic structure information can be preserved. In the learned representation space, the relationships among the nodes, which were originally represented by edges or other high-order topological measures in graphs, are captured by the distances between nodes in the vector space, and the structural characteristics of a node are encoded into its representation vector.
Basically, in order to make the representation space well supporting graph analysis tasks, there are two goals for graph representation learning. First, the original graph can be reconstructed from the learned representation space. It requires that, if there is an edge or relationship between two nodes, then the distance of these two nodes in the representation space should be relatively small. Second, the learned representation space can effectively support graph inference, such as predicting unseen links, identifying important nodes, and inferring node labels. It should be noted that a representation space with only the goal of graph reconstruction is not sufficient

2 Graph Representation Learning

19

for graph inference. After the representation is obtained, downstream tasks such as node classification , node clustering , graph visualization and link prediction can be dealt with based on these representations. Overall, there are three main categories of graph representation learning methods: traditional graph embedding, modern graph embedding, and graph neural networks, which will be introduced separately in the following three sections.

2.2 Traditional Graph Embedding
Traditional graph embedding methods are originally studied as dimension reduction techniques. A graph is usually constructed from a feature represented data set, like image data set. As mentioned before, graph embedding usually has two goals, i.e. reconstructing original graph structures and support graph inference. The objective functions of traditional graph embedding methods mainly target the goal of graph reconstruction.
Specifically, Tenenbaum et al (2000) first constructs a neighborhood graph G using connectivity algorithms such as K nearest neighbors (KNN). Then based on G, the shortest path between different data can be computed. Consequently, for all the N data entries in the data set, we have the matrix of graph distances. Finally, the classical multidimensional scaling (MDS) method is applied to the matrix to obtain the coordinate vectors. The representations learned by Isomap approximately preserve the geodesic distances of the entry pairs in the low-dimensional space. The key problem of Isomap is its high complexity due to the computing of pair-wise shortest pathes. Locally linear embedding (LLE) (Roweis and Saul, 2000) is proposed to eliminate the need to estimate the pairwise distances between widely separated entries. LLE assumes that each entry and its neighbors lie on or close to a locally linear patch of a mainfold. To characterize the local geometry, each entry can be reconstructed from its neighbors. Finally, in the low-dimensional space, LLE constructs a neighborhood-preserving mapping based on locally linear reconstruction. Laplacian eigenmaps (LE) (Belkin and Niyogi, 2002) also begins with constructing a graph using ε-neighborhoods or K nearest neighbors. Then the heat kernel (Berline et al, 2003) is utilized to choose the weight of two nodes in the graph. Finally, the node representations can be obtained by based on the Laplacian matrix regularization. Furthermore, the locality preserving projection (LPP) (Berline et al, 2003), a linear approximation of the nonlinear LE, is proposed.
These methods are extended in the rich literature of graph embedding by considering different characteristics of the constructed graphs (Fu and Ma, 2012). We can find that traditional graph embedding mostly works on graphs constructed from feature represented data sets, where the proximity among nodes encoded by the edge weights is well defined in the original feature space. While, in contrast, modern graph embedding, which will be introduced in the following, mostly works on naturally formed networks, such as social networks, biology networks, and e-commerce networks. In those networks, the proximities among nodes are not explicitly or di-

20

Peng Cui, Lingfei Wu, Jian Pei, Liang Zhao and Xiao Wang

rectly defined. For example, an edge between two nodes usually just implies there is a relationship between them, but cannot indicate the specific proximity. Also, even if there is no edge between two nodes, we cannot say the proximity between these two nodes is zero. The definition of node proximities depends on specific analytic tasks and application scenarios. Therefore, modern graph embedding usually incorporates rich information, such as network structures, properties, side information and advanced information, to facilitate different problems and applications. Modern graph embedding needs to target both of goals mentioned before. In view of this, traditional graph embedding can be regarded as a special case of modern graph embedding, and the recent research progress on modern graph embedding pays more attention to network inference.

2.3 Modern Graph Embedding
To well support network inference, modern graph embedding considers much richer information in a graph. According to the types of information that are preserved in graph representation learning, the existing methods can be categorized into three categories: (1) graph structures and properties preserving graph embedding, (2) graph representation learning with side information and (3) advanced information preserving graph representation learning. In technique view, different models are adopted to incorporate different types of information or address different goals. The commonly used models include matrix factorization, random walk, deep neural networks and their variations.

2.3.1 Structure-Property Preserving Graph Representation Learning
Among all the information encoded in a graph, graph structures and properties are two crucial factors that largely affect graph inference. Thus, one basic requirement of graph representation learning is to appropriately preserve graph structures and capture properties of graphs. Often, graph structures include first-order structures and higher-order structures, such as second-order structures and community structures. Graphs with different types have different properties. For example, directed graphs have the asymmetric transitivity property. The structural balance theory is widely applicable to signed graphs.
2.3.1.1 Structure Preserving Graph Representation Learning Graph structures can be categorized into different groups that present at different granularities. The commonly exploited graph structures in graph representation

2 Graph Representation Learning

21

learning include neighborhood structure, high-order node proximity and graph communities.
How to define the neighborhood structure in a graph is the first challenge. Based on the discovery that the distribution of nodes appearing in short random walks is similar to the distribution of words in natural language, DeepWalk (Perozzi et al, 2014) employs the random walks to capture the neighborhood structure. Then for each walk sequence generated by random walks, following Skip-Gram, DeepWalk aims to maximize the probability of the neighbors of a node in a walk sequence. Node2vec defines a flexible notion of a node’s graph neighborhood and designs a second order random walks strategy to sample the neighborhood nodes, which can smoothly interpolate between breadth-first sampling (BFS) and depth-first sampling (DFS). Besides the neighborhood structure, LINE (Tang et al, 2015b) is proposed for large scale network embedding, which can preserve the first and second order proximities. The first order proximity is the observed pairwise proximity between two nodes. The second order proximity is determined by the similarity of the “contexts” (neighbors) of two nodes. Both are important in measuring the relationships between two nodes. Essentially, LINE is based on the shallow model, consequently, the representation ability is limited. SDNE (Wang et al, 2016) proposes a deep model for network embedding, which also aims at capturing the first and second order proximites. SDNE uses the deep auto-encoder architecture with multiple non-linear layers to preserve the second order proximity. To preserve the first-order proximity, the idea of Laplacian eigenmaps (Belkin and Niyogi, 2002) is adopted. Wang et al (2017g) propose a modularized nonnegative matrix factorization (M-NMF) model for graph representation learning, which aims to preserve both the microscopic structure, i.e., the first-order and second-order proximities of nodes, and the mesoscopic community structure (Girvan and Newman, 2002). They adopt the NMF model (Fe´votte and Idier, 2011) to preserve the microscopic structure. Meanwhile, the community structure is detected by modularity maximization (Newman, 2006a). Then, they introduce an auxiliary community representation matrix to bridge the representations of nodes with the community structure. In this way, the learned representations of nodes are constrained by both the microscopic structure and community structure.
In summary, many network embedding methods aim to preserve the local structure of a node, including neighborhood structure, high-order proximity as well as community structure, in the latent low-dimensional space. Both linear and nonlinear models are attempted, demonstrating the large potential of deep models in network embedding.

2.3.1.2 Property Preserving Graph Representation Learning Currently, most of the existing property preserving graph representation learning methods focus on graph transitivity in all types of graphs and the structural balance property in signed graphs.

22

Peng Cui, Lingfei Wu, Jian Pei, Liang Zhao and Xiao Wang

We usually demonstrate that the transitivity usually exists in a graph. But meanwhile, we can find that preserving such a property is not challenging, because in a metric space, the distance between different data points naturally satisfies the triangle inequality. However, this is not always true in the real world. Ou et al (2015) aim to preserve the non-transitivity property via latent similarity components. The nontransitivity property declares that, for nodes v1, v2 and v3 in a graph where (v1; v2) and (v2; v3) are similar pairs, (v1; v3) may be a dissimilar pair. For example, in a social network, a student may connect with his classmates and his family, while his classmates and family are probably very different. The main idea is that they learn multiple node embeddings, and then compare different nodes based on multiple similarities, rather than one similarity. They observe that if two nodes have a large semantic similarity, at least one of the structure similarities is large, otherwise, all of the similarities are small. In a directed graph, it usually has the asymmetric transitivity property. Asymmetric transitivity indicates that, if there is a directed edge from node i to node j and a directed edge from j to v, there is likely a directed edge from i to v, but not from v to i. In order to measure this high-order proximity, HOPE (Ou et al, 2016) summarizes four measurements in a general formulation, and then utilizes a generalized SVD problem to factorize the high-order proximity (Paige and Saunders, 1981), such that the time complexity of HOPE is largely reduced, which means HOPE is scalable for large scale networks. In a signed graph with both of positive and negative edges, the social theories, such as structural balance theory (Cartwright and Harary, 1956; Cygan et al, 2012), which are very different from the unsigned graph. The structural balance theory demonstrates that users in a signed social network should be able to have their “friends” closer than their “foes”. To model the structural balance phenomenon, SiNE (Wang et al, 2017f) utilizes a deep learning model consisting of two deep graphs with non-linear functions.
The importance of maintaining network properties in network embedding space, especially the properties that largely affect the evolution and formation of networks, has been well recognized. The key challenge is how to address the disparity and heterogeneity of the original network space and the embedding vector space at property level. Generally, most of the structure and property preserving methods take high order proximities of nodes into account, which demonstrate the importance of preserving high order structures in network embedding. The difference is the strategy of obtaining the high order structures. Some methods implicitly preserve highorder structure by assuming a generative mechanism from a node to its neighbors, while some other methods realize this by explicitly approximating high-order proximities in the embedding space. As topology structures are the most notable characteristic of networks, structure-preserving network methods embody a large part of the literature. Comparatively, property preserving network embedding is a relatively new research topic and is only studied lightly. As network properties usually drive the formation and evolution of networks, it shows great potential for future research and applications.

2 Graph Representation Learning

23

2.3.2 Graph Representation Learning with Side Information

Besides graph structures, side information is another important information source for graph representation learning. Side information in the context of graph representation learning can be divided into two categories: node content and types of nodes and edges. Their difference is the way of integrating network structures and side information.
Graph Representation Learning with Node Content. In some types of graphs, like information networks, nodes are acompanied with rich information, such as node labels, attributes or even semantic descriptions. How to combine them with the network topology in graph representation learning arouses considerable research interests. Tu et al (2016) propose a semi-supervised graph embedding algorithm, MMDW, by leveraging labeling information of nodes. MMDW is also based on the DeepWalk-derived matrix factorization. MMDW adopts support vector machines (SVM) (Hearst et al, 1998) and incorporates the label information to find an optimal classifying boundary. Yang et al (2015b) propose TADW that takes the rich information (e.g., text) associated with nodes into account when they learn the low dimensional representations of nodes. Pan et al (2016) propose a coupled deep model that incorporates graph structures, node attributes and node labels into graph embedding. Although different methods adopt different strategies to integrate node content and network topology, they all assume that node content provides additional proximity information to constrain the representations of nodes.
Heterogeneous Graph Representation Learning. Different from graphs with node content, heterogeneous graphs consist of different types of nodes and links. How to unify the heterogeneous types of nodes and links in graph embedding is also an interesting and challenging problem. Jacob et al (2014) propose a heterogeneous social graph representation learning algorithm for classifying nodes. They learn the representations of all types of nodes in a common vector space, and perform the inference in this space. Chang et al (2015) propose a deep graph representation learning algorithm for heterogeneous graphs, whose nodes have various types(e.g., images and texts). The nonlinear embeddings of images and texts are learned by a CNN model and the fully connected layers, respectively. Huang and Mamoulis (2017) propose a meta path similarity preserving heterogeneous information graph representation learning algorithm. To model a particular relationship, a meta path (Sun et al, 2011) is a sequence of object types with edge types in between.
In the methods preserving side information, side information introduces additional proximity measures so that the relationships between nodes can be learned more comprehensively. Their difference is the way of integrating network structuress and side information. Many of them are naturally extensions from structure preserving network embedding methods.

24

Peng Cui, Lingfei Wu, Jian Pei, Liang Zhao and Xiao Wang

2.3.3 Advanced Information Preserving Graph Representation Learning

Different from side information, the advanced information refers to the supervised or pseudo supervised information in a specific task. The advanced information preserving network embedding usually consists of two parts. One is to preserve the network structure so as to learn the representations of nodes. The other is to establish the connection between the representations of nodes and the target task. The combination of advanced information and network embedding techniques enables representation learning for networks.
Information Diffusion. Information diffusion (Guille et al, 2013) is an ubiquitous phenomenon on the web, especially in social networks. Bourigault et al (2014) propose a graph representation learning algorithm for predicting information diffusion in social network. The goal of the proposed algorithm is to learn the representations of nodes in the latent space such that the diffusion kernel can best explain the cascades in the training set. The basic idea is to map the observed information diffusion process into a heat diffusion process modeled by a diffusion kernel in the continuous space. The kernel describes that the closer a node in the latent space is from the source node, the sooner it is infected by information from the source node. The cascade prediction problem here is defined as predicting the increment of cascade size after a given time interval (Li et al, 2017a). Li et al (2017a) argue that the previous work on cascade prediction all depends on the bag of hand-crafting features to represent the cascade and graph structures. Instead, they present an end-to-end deep learning model to solve this problem using the idea of graph embedding. The whole procedure is able to learn the representation of cascade graph in an end-to-end manner.
Anomaly Detection. Anomaly detection has been widely investigated in previous work (Akoglu et al, 2015). Anomaly detection in graphs aims to infer the structural inconsistencies, which means the anomalous nodes that connect to various diverse influential communities (Hu et al, 2016), (Burt, 2004). Hu et al (2016) propose a graph embedding based method for anomaly detection. They assume that the community memberships of two linked nodes should be similar. An anomaly node is one connecting to a set of different communities. Since the learned embedding of nodes captures the correlations between nodes and communities, based on the embedding, they propose a new measure to indicate the anomalousness level of a node. The larger the value of the measure, the higher the propensity for a node being an anomaly node.
Graph Alignment. The goal of graph alignment is to establish the correspondence between the nodes from two graphs, i.e., to predict the anchor links across two graphs. The same users who are shared by different social networks naturally form the anchor links, and these links bridge the different graphs. The anchor link prediction problem is, given a source graph,a target graph and a set of observed anchor links, to identify the hidden anchor links across the two graphs. Man et al (2016) propose a graph representation learning algorithm to solve this problem. The

2 Graph Representation Learning

25

learned representations can preserve the graph structures and respect the observed anchor links.
Advanced information preserving graph embedding usually consists of two parts. One is to preserve the graph structures so as to learn the representations of nodes. The other is to establish the connection between the representations of nodes and the target task. The first one is similar to structure and property preserving network embedding, while the second one usually needs to consider the domain knowledge of a specific task. The domain knowledge encoded by the advanced information makes it possible to develop end-to-end solutions for network applications. Compared with the hand-crafted network features, such as numerous network centrality measures, the combination of advanced information and network embedding techniques enables representation learning for networks. Many network applications may be benefitted from this new paradigm.

2.4 Graph Neural Networks
Over the past decade, deep learning has become the “crown jewel” of artificial intelligence and machine learning, showing superior performance in acoustics, images and natural language processing, etc. Although it is well known that graphs are ubiquitous in the real world, it is very challenging to utilize deep learning methods to analyze graph data. This problem is non-trivial because of the following challenges: (1) Irregular structures of graphs. Unlike images, audio, and text, which have a clear grid structure, graphs have irregular structures, making it hard to generalize some of the basic mathematical operations to graphs. For example, defining convolution and pooling operations, which are the fundamental operations in convolutional neural networks (CNNs), for graph data is not straightforward. (2) Heterogeneity and diversity of graphs. A graph itself can be complicated, containing diverse types and properties. These diverse types, properties, and tasks require different model architectures to tackle specific problems. (3) Large-scale graphs. In the big-data era, real graphs can easily have millions or billions of nodes and edges. How to design scalable models, preferably models that have a linear time complexity with respect to the graph size, is a key problem. (4) Incorporating interdisciplinary knowledge. Graphs are often connected to other disciplines, such as biology, chemistry, and social sciences. This interdisciplinary nature provides both opportunities and challenges: domain knowledge can be leveraged to solve specific problems but integrating domain knowledge can complicate model designs.
Currently, graph neural networks have attracted considerable research attention over the past several years. The adopted architectures and training strategies vary greatly, ranging from supervised to unsupervised and from convolutional to recursive, including graph recurrent neural networks (Graph RNNs), graph convolutional networks (GCNs), graph autoencoders (GAEs), graph reinforcement learning (Graph RL), and graph adversarial methods. Specifically, Graroperty h RNNs capture recursive and sequential patterns of graphs by modeling states at either the

26

Peng Cui, Lingfei Wu, Jian Pei, Liang Zhao and Xiao Wang

node-level or the graph-level; GCNs define convolution and readout operations on irregular graph structures to capture common local and global structural patterns; GAEs assume low-rank graph structures and adopt unsupervised methods for node representation learning; Graph RL defines graph-based actions and rewards to obtain feedbacks on graph tasks while following constraints; Graph adversarial methods adopt adversarial training techniques to enhance the generalization ability of graphbased models and test their robustness by adversarial attacks.
There are many ongoing or future research directions which are also worthy of further study, including new models for unstudied graph structures, compositionality of existing models, dynamic graphs, interpretability and robustness, etc. On the whole, deep learning on graphs is a promising and fast-developing research field that both offers exciting opportunities and presents many challenges. Studying deep learning on graphs constitutes a critical building block in modeling relational data, and it is an important step towards a future with better machine learning and artificial intelligence techniques.

2.5 Summary
In this chapter, we introduce the motivation of graph representation learning. Then in Section 2, we discuss the traditional graph embedding methods and the modern graph embedding methods are introduced in Section 3. Basically, the structure and property preserving graph representation learning is the foundation. If one cannot preserve well the graph structures and retain the important graph properties in the representation space, serious information will be lost, which hurts the analytic tasks in sequel. Based on the structures and property preserving graph representation learning, one may apply the off-the-shelf machine learning methods. If some side information is available, it can be incorporated into graph representation learning. Furthermore, the domain knowledge of some certain applications as advanced information can be considered. As shown in Section 4, utilizing deep learning methods on graphs is a promising and fast-developing research field that both offers exciting opportunities and presents many challenges. Studying deep learning on graphs constitutes a critical building block in modeling relational data, and it is an important step towards a future with better machine learning and artificial intelligence techniques.

Chapter 3
Graph Neural Networks
Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao and Le Song

Abstract Deep Learning has become one of the most dominant approaches in Artificial Intelligence research today. Although conventional deep learning techniques have achieved huge successes on Euclidean data such as images, or sequence data such as text, there are many applications that are naturally or best represented with a graph structure. This gap has driven a tide in research for deep learning on graphs, among them Graph Neural Networks (GNNs) are the most successful in coping with various learning tasks across a large number of application domains. In this chapter, we will systematically organize the existing research of GNNs along three axes: foundations, frontiers, and applications. We will introduce the fundamental aspects of GNNs ranging from the popular models and their expressive powers, to the scalability, interpretability and robustness of GNNs. Then, we will discuss various frontier research, ranging from graph classification and link prediction, to graph generation and transformation, graph matching and graph structure learning. Based on them, we further summarize the basic procedures which exploit full use of various GNNs for a large number of applications. Finally, we provide the organization of our book and summarize the roadmap of the various research topics of GNNs.

Lingfei Wu JD.COM Silicon Valley Research Center, e-mail: lwu@email.wm.edu Peng Cui Department of Computer Science, Tsinghua University, e-mail: cuip@tsinghua.edu.cn Jian Pei Department of Computer Science, Simon Fraser University, e-mail: jpei@cs.sfu.ca Liang Zhao Department of Computer Science, Emory University, e-mail: liang.zhao@emory.edu Le Song Mohamed bin Zayed University of Artificial Intelligence, e-mail: dasongle@gmail.com

© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022

27

L. Wu et al. (eds.), Graph Neural Networks: Foundations, Frontiers, and Applications,

https://doi.org/10.1007/978-981-16-6054-2_3

28

Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao and Le Song

3.1 Graph Neural Networks: An Introduction

Deep Learning has become one of the most dominant approaches in Artificial Intelligence research today. Conventional deep learning techniques, such as recurrent neural networks (Schuster and Paliwal, 1997) and convolutional neural networks (Krizhevsky et al, 2012) have achieved huge successes on Euclidean data such as images, or sequence data such as text and signals. However, in a rich variety of scientific fields, many important real-world objects and problems can be naturally or best expressed along with a complex structure, e.g., graph or manifold structure, such as social networks, recommendation systems, drug discovery and program analysis. On the one hand, these graph-structured data can encode complicated pairwise relationships for learning more informative representations; On the other hand, the structural and semantic information in original data (images or sequential texts) can be exploited to incorporate domain-specific knowledge for capturing more finegrained relationships among the data.
In recent years, deep learning on graphs has experienced a burgeoning interest from the research community (Cui et al, 2018; Wu et al, 2019e; Zhang et al, 2020e). Among them, Graph Neural Networks (GNNs) is the most successful learning framework in coping with various tasks across a large number of application domains. Newly proposed neural network architectures on graph-structured data (Kipf and Welling, 2017a; Petar et al, 2018; Hamilton et al, 2017b) have achieved remarkable performance in some well-known domains such as social networks and bioinformatics. They have also infiltrated other fields of scientific research, including recommendation systems (Wang et al, 2019j), computer vision (Yang et al, 2019g), natural language processing (Chen et al, 2020o), program analysis (Allamanis et al, 2018b), software mining (LeClair et al, 2020), drug discovery (Ma et al, 2018), anomaly detection (Markovitz et al, 2020), and urban intelligence (Yu et al, 2018a).
Despite these successes that existing research has achieved, GNNs still face many challenges when they are used to model highly-structured data that is time-evolving, multi-relational, and multi-modal. It is also very difficult to model mapping between graphs and other highly structured data, such as sequences, trees, and graphs. One challenge with graph-structured data is that it does not show as much spatial locality and structure as image or text data does. Thus, graph-structured data is not naturally suitable for highly regularized neural structures such as convolutional and recurrent neural networks.
More importantly, new application domains for GNNs that emerge from realworld problems introduce significantly challenges for GNNs. Graphs provide a powerful abstraction that can be used to encode arbitrary data types such as multidimensional data. For example, similarity graphs, kernel matrices, and collaborative filtering matrices can also be viewed as special cases of graph structures. Therefore, a successful modeling process of graphs is likely to subsume many applications that are often used in conjunction with specialized and hand-crafted methods.
In this chapter, we will systematically organize the existing research of GNNs along three axes: foundations of GNNs, frontiers of GNNs, and GNN based applications. First of all, we will introduce the fundamental aspects of GNNs ranging from

3 Graph Neural Networks

29

popular GNN methods and their expressive powers, to the scalability, interpretability, and robustness of GNNs. Next, we will discuss various frontier research which are built on GNNs, including graph classification, link prediction, graph generation and transformation, graph matching, graph structure learning, dynamic GNNs, heterogeneous GNNs, AutoML of GNNs and self-supervised GNNs. Based on them, we further summarize the basic procedures which exploit full use of various GNNs for a large number of applications. Finally, we provide the organization of our GNN book and summarize the roadmap of the various research topics of GNNs.

3.2 Graph Neural Networks: Overview
In this section, we summarize the development of graph neural networks along three important dimensions: (1) Foundations of GNNs; (2) Frontiers of GNNs; (3) GNNbased applications. We will first discuss the important research areas under the first two dimensions for GNNs and briefly illustrate the current progress and challenges for each research sub-domain. Then we will provide a general summarization on how to exploit the power of GNNs for a rich variety of applications.

3.2.1 Graph Neural Networks: Foundations
Conceptually, we can categorize the fundamental learning tasks of GNNs into five different directions: i) Graph Neural Networks Methods; ii) Theoretical understanding of Graph Neural Networks; iii) Scalability of Graph Neural Networks; iv) Interpretability of Graph Neural Networks; and v) Adversarial robustness of Graph Neural Networks. We will discuss these fundamental aspects of GNNs one by one in this subsection.
Graph Neural Network Methods. Graph Neural Networks are specifically designed neural architectures operated on graph-structure data. The goal of GNNs is to iteratively update the node representations by aggregating the representations of node neighbors and their own representation in the previous iteration. There are a variety of graph neural networks proposed in the literature (Kipf and Welling, 2017a; Petar et al, 2018; Hamilton et al, 2017b; Gilmer et al, 2017; Xu et al, 2019d; Velickovic et al, 2019; Kipf and Welling, 2016), which can be further categorized into supervised GNNs and unsupervised GNNs. Once the node representations are learnt, a fundamental task on graphs is node classification that tries to classify the nodes into a few predefined classes. Despite the huge successes that various GNNs have achieved, a severe issue on training deep graph neural networks has been observed to yield inferior results, namely, over-smoothing problem (Li et al, 2018b), where all the nodes have similar representations. Many recent works have been proposed with different remedies to overcome this over-smoothing issue.

30

Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao and Le Song

Theoretical understanding of Graph Neural Networks. Rapid algorithmic developments of GNNs have aroused a significant amount of interests in theoretical analysis on the expressive power of GNNs. In particular, much efforts have been made in order to characterize the expressive power of GNNs when compared with the traditional graph algorithms (e.g. graph kernel-based methods) and how to build more powerful GNNs so as to overcome several limitations in GNNs. Specifically, Xu et al (2019d) showed that current GNN methods are able to achieve the expressive power of the 1-dimensional Weisfeiler-Lehman test (Weisfeiler and Leman., 1968), a widely used method in traditional graph kernel community (Shervashidze et al, 2011b). Much recent research has further proposed a series of design strategies in order to further reach beyond the expressive power of the Weisfeiler-Lehman test by including attaching random attributes, distance attributes, and utilizing higher-order structures.
Scalability of Graph Neural Networks. The increasing popularity of GNNs have attracted many attempts to apply various GNN methods on real-world applications, where the graph sizes are often about having one hundred million nodes and one billion edges. Unfortunately, most of the GNN methods cannot directly be applied on these large-scale graph-structured data due to large memory requirements (Hu et al, 2020b). Specifically, this is because the majority of GNNs are required to store the whole adjacent matrices and the intermediate feature matrices in the memory, rendering the significant challenges for both computer memory consumption and computational costs. In order to address these issues, many recent works have been proposed with various sampling strategies such as node-wise sampling (Hamilton et al, 2017b; Chen et al, 2018d), layer-wise sampling (Chen and Bansal, 2018; Huang et al, 2018), and graph-wise sampling (Chiang et al, 2019; Zeng et al, 2020a).
Interpretability of Graph Neural Networks. Explainable artificial intelligence are becoming increasingly popular in providing interpretable results on machine learning process, especially due to the black-box issue of deep learning techniques. As a result, there is a surge of interests in improving the interpretability of GNNs. Generally speaking, explanation results on GNNs could be important nodes, important edges, or important features of nodes or edges. Technically, white-box approximation based methods (Baldassarre and Azizpour, 2019; Sanchez-Lengeling et al, 2020) utilize the information inside the model inlucidng gradients, intermediate features, and model parameters to provide the explanation. In contrast, the black-box approximation based methods (Huang et al, 2020c; Zhang et al, 2020a; Vu and Thai, 2020) abandon the utilization of internal information of complex models but instead leverage the intrinsically interpretable simple models (e.g. linear regression and decision trees) to fit the complex models. However, most of the existing works are time-consuming, which rendering the difficulty in coping with large-scale graph. To this end, many recent efforts have been made in order to develop more efficient approaches without compromising the explanation accuracy.
Adversarial robustness of Graph Neural Networks. Trustworthy machine learning has recently attracted a significant amount of attention since the existing studies have shown that deep learning models could be deliberately fooled, evaded, misled, and stolen (Goodfellow et al, 2015). Consequently, a line of research has exten-

3 Graph Neural Networks

31

sively studied the robustness of models in domains like computer vision and natural language processing, which has also influenced similar research on the robustness of GNNs. Technically, the standard approach (via adversarial examples) for studying the robustness of GNNs is to construct a small change of the input graph data and then to observe if it leads to a large change of the prediction results (i.e. node classification accuracy). There are a growing number of research works toward either adversarial attacks (Dai et al, 2018a; Wang and Gong, 2019; Wu et al, 2019b; Zu¨gner et al, 2018; Zu¨gner et al, 2020) or adversarial training (Xu et al, 2019c; Feng et al, 2019b; Chen et al, 2020i; Jin and Zhang, 2019). Many recent efforts have been made to provide both theoretical guarantees and new algorithmic developments in adversarial training and certified robustness.

3.2.2 Graph Neural Networks: Frontiers
Built on these aforementioned fundamental techniques of GNNs, there are various fast-growing recent research developments in coping with a variety of graph-related research problems. In this section, we will comprehensively introduce these research frontiers that are either long-standing graph learning problems with new GNN solutions or recently emerging learning problems with GNNs.
Graph Neural Networks: Graph Classification and Link Prediction. Since each layer in GNN models only produce the node-level representations, graph pooling layers are needed to further compute graph-level representation based on node-level representations. The graph-level representation, which summarizes the key characteristics of input graph-structure, is the critical component for the graph classification. Depending on the learning techniques of graph pooling layers, these methods can be generally categorized into four groups: simple flat-pooling (Duvenaud et al, 2015a; Mesquita et al, 2020), attention-based pooling (Lee et al, 2019d; Huang et al, 2019d), cluster-based pooling (Ying et al, 2018c), and other type of pooling (Zhang et al, 2018f; Bianchi et al, 2020; Morris et al, 2020b). Beside graph classification, another long-standing graph learning problem is link prediction task, which aims to predict missing or future links between any pair of nodes. Since GNNs can jointly learn from both graph structure and side information (e.g. node and edge features), it has shown great advantages over other conventional graph learning methods for link prediction. Regarding the learning types of link prediction, node-based methods (Kipf and Welling, 2016) and subgraph-based methods (Zhang and Chen, 2018a, 2020) are two popular groups of GNN based methods.
Graph Neural Networks: Graph Generation and Graph Transformation. Graph generation problem that builds probabilistic models over graphs is a classical research problem that lies at the intersection between the probability theory and the graph theory. Recent years have seen an increasing amount of interest in developing deep graph generative models that are built on modern deep learning on graphs techniques like GNNs. These deep models have proven to be a more successful approach in capturing the complex dependencies within the graph data and generating

32

Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao and Le Song

more realistic graphs. Encouraged by the great successes of Variational AutoEncoder (VAE) (Kingma and Welling, 2013) and Generative Adversarial Networks (Goodfellow et al, 2014a) (Goodfellow et al, 2014b), there are three representative GNN based learning paradigms for graph generation including GraphVAE approaches (Jin et al, 2018b; Simonovsky and Komodakis, 2018; Grover et al, 2019), GraphGAN approaches (De Cao and Kipf, 2018; You et al, 2018a) and Deep Autoregressive methods (Li et al, 2018d; You et al, 2018b; Liao et al, 2019a). Graph transformation problem can be formulated as a conditional graph generation problem, where its goal is to learn a translation mapping between the input source graph and the output target graph (Guo et al, 2018b). Such learning problem often arises in other domains such as machine translation problem in Natural Language Processing domain and image style transfer in computer Vision domain. Depending on what graph information is transformed, this problem can be generally grouped into four categories including node-level transformation (Battaglia et al, 2016; Yu et al, 2018a; Li et al, 2018e), edge-level transformation (Guo et al, 2018b; Zhu et al, 2017; Do et al, 2019), node-edge co-transformation (Maziarka et al, 2020a; Kaluza et al, 2018; Guo et al, 2019c), and graph-involved transformation (Bastings et al, 2017; Xu et al, 2018c; Li et al, 2020f).
Graph Neural Networks: Graph Matching and Graph Structure Learning. The problem of graph matching is to find the correspondence between two input graphs, which is an extensively studied problem in a variety of research fields. Conventionally, the graph matching problem is known to be NP-hard (Loiola et al, 2007), rendering this problem computationally infeasible for exact and optimum solutions for real-world large-scale problems. Due to the expressive power of GNNs, there is an increasing attention on developing various graph matching methods based on GNNs in order to improve the matching accuracy and efficiency (Zanfir and Sminchisescu, 2018; Rol´ınek et al, 2020; Li et al, 2019h; Ling et al, 2020). Graph matching problem aims to measure the similarity between two graph structures without changing them. In contrast, graph structure learning aims to produce an optimized graph structure by jointly learning implicit graph structure and graph node representation (Chen et al, 2020m; Franceschi et al, 2019; Velickovic et al, 2020). The learnt graph structure often can be treated as a shift compared to the intrinsic graph which is often noisy or incomplete. Graph structure learning can also be used when the initial graph is not provided while the data matrix shows correlation among data points.
Dynamic Graph Neural Networks and Heterogeneous Graph Neural Networks. In real-world applications, the graph nodes (entities) and the graph edges (relations) are often evolving over time, which naturally gives rise to dynamic graphs. Unfortunately, various GNNs cannot be directly applied to the dynamic graphs, where modeling the evolution of the graph is critical in making accurate predictions. A simple yet often effective approach is converting dynamic graphs into static graphs, leading to potential loss of information. Regarding the type of dynamic graphs, there are two major categories of GNN-based methods, including GNNs for discrete-time dynamic graphs (Seo et al, 2018; Manessi et al, 2020) and GNNs for continue-time dynamic graphs (Kazemi et al, 2019; Xu et al, 2020a). Independently, another pop-

3 Graph Neural Networks

33

ular graph type in real applications is heterogeneous graphs that consist of different types of graph nodes and edges. To fully exploit this information in heterogeneous graphs, different GNNs for homogeneous graphs are not applicable. As a result, a new line of research has been devoted to developing various heterogeneous graph neural networks including message passing based methods (Wang et al, 2019l; Fu et al, 2020; Hong et al, 2020b), encoder-decoder based methods (Tu et al, 2018; Zhang et al, 2019b), and adversarial based methods (Wang et al, 2018a; Hu et al, 2018a).
Graph Neural Networks: AutoML and Self-supervised Learning. Automated machine learning (AutoML) has recently drawn a significant amount of attention in both research and industrial communities, the goal of which is coping with the huge challenge of time-consuming manual tuning process, especially for complicated deep learning models. This wave of the research in AutoML also influences the research efforts in automatically identifying an optimized GNN model architecture and training hyperparameters. Most of the existing research focuses on either architecture search space (Gao et al, 2020b; Zhou et al, 2019a) or training hyperparameter search space (You et al, 2020a; Shi et al, 2020). Another important research direction of GNNs is to address the limitation of most of deep learning models that requires large amount of annotated data. As a result, self-supervised learning has been proposed which aims to design and leverage domain-specific pretext tasks on unlabeled data to pretrain a GNN model. In order to study the power of serfsupervised leanring in GNNs, there are quite a few works that systemmatically design and compare different self-supervised pretext tasks in GNNs (Hu et al, 2020c; Jin et al, 2020d; You et al, 2020c).

3.2.3 Graph Neural Networks: Applications
Due to the power of GNNs to model various data with complex structures, GNNs have been widely applied into many applications and domains, such as modern recommender systems, computer vision (CV), natural language processing (NLP), program analysis, software mining, bioinformatics, anomaly detection, and urban intelligence. Though GNNs are utilized to solve different tasks for different applications, they all consist of two important steps, namely graph construction and graph representation learning. Graph construction aims to first transform or represent the input data as graph-structured data. Based on the graphs, graph representation learning utilizes GNNs to learn the node or graph embeddings for the downstream tasks. In the following, we briefly introduce the techniques of these two steps regarding different applications.

34
3.2.3.1 Graph Construction

Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao and Le Song

Graph construction is important in capturing the dependency among the objects in the input data. Given the various formats of input data, different applications have different graph construction techniques, while some tasks need to pre-define the semantic meaning of nodes and edges to fully express the structural information of the input data.
Input Data with Explicit Graph Structures. Some applications naturally have the structure inside the data without pre-defined nodes and the edges/relationships among them. For example, the user-item interactions in a recommender systems naturally form a graph where user-item preference is regarded as the edges between the nodes of user and item. In the task of drug design, a molecule is also naturally represented as a graph, where each node denotes an atom and an edge denotes a bond that connects two atoms. In the task of protein function prediction and interaction, the graph can also easily fit into a protein, where each amino-acid refers to a node and each edge refers to the interaction among amino-acids.
Some graphs are constructed with the node and edge attributes. For example, in dealing with the transportation in the urban intelligence, the traffic networks can be formalized as an undirected graph to predict the traffic state. Specifically, the nodes are the traffic sensing locations, e.g., sensor stations, road segments, and the edges are the intersections or road segments connecting those traffic sensing locations. Some urban traffic network can be modeled as a directed graph with attributes to predict the traffic speed, where the nodes are the road segments, and the edges are the intersections. Road segment width, length, and direction are the attributes of the nodes, and the type of intersection, and whether there are traffic lights, toll gates are the attributes of edges.
Input Data with Implicit Graph Structures. For many tasks that do not naturally involve a structured data, graph construction becomes very challenging. It is important to choose the best representation so that the nodes and edges can capture all the important things. For example, in computer vision (CV) tasks, there are three kinds of graph construction. The first is to split the image or the frame of the video into regular grids, and each grid serves as a vertex of the visual graph. The second way is to first get the preprocessed structures which can be directly borrowed for vertex representation, such as the formulation of scene graphs. The last one is about utilizing semantic information to represent visual vertexes, such as assigning pixels with similar features to the same vertex. The edges in the visual images can capture two kinds of information. One is spatial information. For example, for static methods, generating scene graphs (Xu et al, 2017a) and human skeletons (Jain et al, 2016a) is natural to choose edges between nodes in the visual graph to represent their location connection. Another is temporal information. For example, to represent the video, the model not only builds spatial relations in a frame but also captures temporal connections among adjacent frames.
In the natural language processing (NLP) tasks, the graph construction from the text data can be categorized into five categories: text graphs, syntactic graphs, semantic graphs, knowledge graphs, and hybrid graphs. Text graphs normally re-

3 Graph Neural Networks

35

gard words, sentences, paragraphs, or documents as nodes and establish edges by word co-occurrence, location, or text similarities. Syntactic graphs (or trees) emphasize the syntactical dependencies between words in a sentence, such as dependency graph and constituency graph. Knowledge graphs (KGs) are graphs of data intended to accumulate and convey knowledge of the real world. Hybrid graphs contain multiple types of nodes and edges to integrate heterogeneous information. In the task of program analysis, the formulation over graph representations of programs includes syntax trees, control flow, data flow, program dependence, and call graphs, each providing different views of a program. At a high level, programs can be thought as a set of heterogeneous entities that are related through various kinds of relations. This view directly maps a program to a heterogeneous directed graph, with each entity being represented as a node and each relationship of type represented as an edge.

3.2.3.2 Graph Representation Learning After getting the graph expression of the input data, the next step is applying GNNs for learning the graph representations. Some works directly utilize the typical GNNs, such as GCN (Kipf and Welling, 2017a), GAT (Petar et al, 2018), GGNN (Li et al, 2016a) and GraphSage (Hamilton et al, 2017b), which can be generalized to different application tasks. While some special tasks needs an additional design on the GNN architecture to better handle the specific problem. For example, in the task of recommender systems, PinSage (Ying et al, 2018a) is proposed which takes the top-k counted nodes of a node as its receptive field and utilizes weighted aggregation for aggregation. PinSage can be scalable to the web-scale recommender systems with millions of users and items. KGCN (Wang et al, 2019d) aims to enhance the item representation by performing aggregations among its corresponding entity neighborhood in a knowledge graph. KGAT (Wang et al, 2019j) shares a generally similar idea with KGCN except for incorporating an auxiliary loss for knowledge graph reconstruction. For instance, in the NLP task of KB-alignment, Xu et al (2019e) formulated it as a graph matching problem, and proposed a graph attentionbased approach. It first matches all entities in two KGs, and then jointly models the local matching information to derive a graph-level matching vector. The detailed GNN techniques for each application can be found in the following chapters of this book.

3.2.4 Graph Neural Networks: Organization
The high-level organization of the book is demonstrated in Figure 1.3. The book is organized into four parts to best accommodate a variety of readers. Part I introduces basic concepts; Part II discusses the most established methods; Part III presents the most typical frontiers, and Part IV describes advances of methods and applications

36

Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao and Le Song

that tend to be important and promising for future research. Next, we briefly elaborate on each chapter.
• Part I: Introduction. These chapters provide the general introduction from the representation learning for different data types, to the graph representation learning. In addition, it introduces the basic ideas and typical variants of graph neural networks for the graph representation learning.
• Part II: Foundations. These chapters describe the foundations of the graph neural networks by introducing the properties of graph neural networks as well as several fundamental problems in this line. Specifically, this part introduces the fundamental problems in graphs: node classification, the expressive power of graph neural networks, the interpretability and scalability issues of graph neural network, and the adversarial robustness of the graph neural networks.
• Part III: Frontiers. In these chapters, some frontier or advanced problems in the domain of graph neural networks are proposed. Specifically, there are introductions about the techniques in graph classification, link prediction, graph generation, graph transformation, graph matching, graph structure learning. In addition, there are also introductions of several variants of GNNs for different types of graphs, such as GNNs for dynamic graphs, heterogeneous graphs. We also introduce the AutoML and self-supervised learning for GNNs.
• Part IV: Broad and Emerging Applications. These chapters introduce the broad and emerging applications with GNNs. Specifically, these GNNs-based applications covers modern recommender systems, tasks in computer vision and NLP, program analysis, software mining, biomedical knowledge graph mining for drug design, protein function prediction and interaction, anomaly detection, and urban intelligence.

3.3 Summary
Graph Neural Networks (GNNs) have been emerging rapidly to deal with the graphstructured data, which cannot be directly modeled by the conventional deep learning techniques that are designed for Euclidean data such as images and text. A wide range of applications can be naturally or best represented with graph structure and have been successfully handled by various graph neural networks.
In this chapter, we have systematically introduced the development and overview of GNNs, including the introduction of its foundations, frontiers, and applications. Specifically, we provide the fundamental aspects of GNNs ranging from the existing typical GNN methods and their expressive powers, to the scalability, interpretability and robustness of GNNs. These aspects motivate the research on better understanding and utilization of GNNs. Built on GNNs, recent research developments have seen a surge of interests in coping with graph-related research problems, which we called frontiers of GNNs. We have discussed various frontier research built on GNNs, ranging from graph classification and link prediction, to graph generation,

3 Graph Neural Networks

37

Fig. 3.1: The high-level organization of the book transformation, matching and graph structure learning. Due to the power of GNNs to model various data with complex structures, GNNs have been widely applied into many applications and domains, such as modern recommender systems, computer vision, natural language processing, program analysis, software mining, bioinformatics, anomaly detection, and urban intelligence. Most of these tasks consist of two important steps, namely graph construction and graph representation learning. Thus, we provide the introduction of the techniques of these two steps regarding different applications. The introduction part will end here and thus a summary of the organization of this book has been provided at the end of this chapter.

Part II
Foundations of Graph Neural Networks

Chapter 4
Graph Neural Networks for Node Classification
Jian Tang and Renjie Liao

Abstract Graph Neural Networks are neural architectures specifically designed for graph-structured data, which have been receiving increasing attention recently and applied to different domains and applications. In this chapter, we focus on a fundamental task on graphs: node classification. We will give a detailed definition of node classification and also introduce some classical approaches such as label propagation. Afterwards, we will introduce a few representative architectures of graph neural networks for node classification. We will further point out the main difficulty— the oversmoothing problem—of training deep graph neural networks and present some latest advancement along this direction such as continuous graph neural networks.

4.1 Background and Problem Definition
Graph-structured data (e.g., social networks, the World Wide Web, and proteinprotein interaction networks) are ubiquitous in real-world, covering a variety of applications. A fundamental task on graphs is node classification, which tries to classify the nodes into a few predefined categories. For example, in social networks, we want to predict the political bias of each user; in protein-protein interaction networks, we are interested in predicting the function role of each protein; in the World Wide Web, we may have to classify web pages into different semantic categories. To make effective prediction, a critical problem is to have very effective node representations, which largely determine the performance of node classification.
Graph neural networks are neural network architectures specifically designed for learning representations of graph-structured data including learning node represen-
Jian Tang Mila-Quebec AI Institute, HEC Montreal, e-mail: jian.tang@hec.ca Renjie Liao University of Toronto, e-mail: rjliao@cs.toronto.edu

© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022

41

L. Wu et al. (eds.), Graph Neural Networks: Foundations, Frontiers, and Applications,

https://doi.org/10.1007/978-981-16-6054-2_4

42

Jian Tang and Renjie Liao

tations of big graphs (e.g., social networks and the World Wide Web) and learning representations of entire graphs (e.g., molecular graphs). In this chapter, we will focus on learning node representations for large-scale graphs and will introduce learning the whole-graph representations in other chapters. A variety of graph neural networks have been proposed (Kipf and Welling, 2017b; Velicˇkovic´ et al, 2018; Gilmer et al, 2017; Xhonneux et al, 2020; Liao et al, 2019b; Kipf and Welling, 2016; Velicˇkovic´ et al, 2019). In this chapter, we will comprehensively revisit existing graph neural networks for node classification including supervised approaches (Sec. 4.2), unsupervised approaches (Sec. 4.3), and a common problem of graph neural networks for node classification—over-smoothing (Sec. 4.4).

Problem Definition. Let us first formally define the problem of learning node representations for node classification with graph neural networks. Let G = (V , E ) denotes a graph, where V is the set of nodes and E is the set of edges. A ∈ RN×N represents the adjacency matrix, where N is the total number of nodes, and X ∈ RN×C represents the node attribute matrix, where C is the number of features for each node. The goal of graph neural networks is to learn effective node representations (denoted as H ∈ RN×F , F is the dimension of node representations) by combining the graph structure information and the node attributes, which are further used for node classification.

Table 4.1: Notations used throughout this chapter.

Concept

Notation

Graph

G = (V , E )

Adjacency matrix

A ∈ RN×N

Node attributes

X ∈ RN×C

Total number of GNN layers

K

Node representations at the k-th layer Hk ∈ RN×F , k ∈ {1, 2, · · · , K}

4.2 Supervised Graph Neural Networks
In this section, we revisit several representative methods of graph neural networks for node classification. We will focus on the supervised methods and introduce the unsupervised methods in the next section. We will start by introducing a general framework of graph neural networks and then introduce different variants under this framework.

4 Graph Neural Networks for Node Classification

43

4.2.1 General Framework of Graph Neural Networks

The essential idea of graph neural networks is to iteratively update the node representations by combining the representations of their neighbors and their own representations. In this section, we introduce a general framework of graph neural networks in (Xu et al, 2019d). Starting from the initial node representation H0 = X, in each layer we have two important functions:
• AGGREGATE, which tries to aggregate the information from the neighbors of each node;
• COMBINE, which tries to update the node representations by combining the aggregated information from neighbors with the current node representations. Mathematically, we can define the general framework of graph neural networks
as follows: Initialization: H0 = X For k = 1, 2, · · · , K,

akv = AGGREGATEk{Huk−1 : u ∈ N(v)} Hvk = COMBINEk{Hvk−1, akv},

(4.1) (4.2)

where N(v) is the set of neighbors for the v-th node. The node representations HK in the last layer can be treated as the final node representations.
Once we have the node representations, they can be used for downstream tasks. Take the node classification as an example, the label of node v (denoted as yˆv) can be predicted through a Softmax function, i.e.,

yˆv = Softmax(W Hv⊤),

(4.3)

where W ∈ R|L |×F , |L | is the number of labels in the output space.

Given a set of labeled nodes, the whole model can be trained by minimizing the

following loss function:

∑ O

=

1 nl

nl
loss(yˆi, yi),
i=1

(4.4)

where yi is the ground truth label of node i, nl is the number of labeled nodes, loss(·, ·) is a loss function such as cross-entropy loss function. The whole neural networks can be optimized by minimizing the objective function O with backpropagation.
Above we present a general framework of graph neural networks. Next, we will introduce a few most representative instantiations or variants of graph neural networks in the literature.

44
4.2.2 Graph Convolutional Networks

Jian Tang and Renjie Liao

We will start from the graph convolutional networks (GCN) (Kipf and Welling, 2017b), which is now the most popular graph neural network architecture due to its simplicity and effectiveness in a variety of tasks and applications. Specifically, the node representations in each layer is updated according to the following propagation rule:

H k+1

=

σ

(D˜ −

1 2

A˜D˜ −

1 2

HkW k).

(4.5)

A˜ = A + I is the adjacency matrix of the given undirected graph G with selfconnections, which allows to incorporate the node features itself when updating the node representations. I ∈ RN×N is the identity matrix. D˜ is a diagonal matrix with D˜ ii = ∑ j A˜i j. σ (·) is an activation function such as ReLU and Tanh. The ReLU active function is widely used, which is defined as ReLU(x) = max(0, x). W k ∈ RF×F′ (F,F′ are the dimensions of node representations in the k-th, (k+1)-th layer respectively) is a laywise linear transformation matrix, which will be trained during the optimization.
We can further dissect equation equation 4.5 and understand the AGGREGATE and COMBINE function defined in GCN. For a node i, the node updating equation can be reformulated as below:

Hik = σ ( ∑ j∈{N(i)∪i}

A˜i j D˜ iiD˜

j

j

H

kj −1W

k)

(4.6)

Hik = σ ( ∑ j∈N(i)

Ai j D˜ iiD˜

j

j

Hkj −1W

k

+

1 D˜ i

Hik−1W

k)

(4.7)

In the Equation equation 4.7, we can see that the AGGREGATE function is defined as the weighted average of the neighbor node representations. The weight of the neighbor j is determined by the weight of the edge between i and j (i.e. Ai j normalized by the degrees of the two nodes). The COMBINE function is defined as the summation of the aggregated messages and the node representation itself, in which the node representation is normalized by its own degree.

Connections with Spectral Graph Convolutions. Next, we discuss the connections between GCNs and traditional spectral filters defined on graphs (Defferrard et al, 2016). The spectral convolutions on graphs can be defined as a multiplication of a node-wise signal x ∈ RN with a convolutional filter gθ = diag(θ ) (θ ∈ RN is the parameter of the filter) in the Fourier domain. Mathematically,

gθ ⋆ x = UgθUT x.

(4.8)

4 Graph Neural Networks for Node Classification

45

U represents the matrix of the eigenvectors of the normalized graph Laplacian ma-

trix

L

=

IN

−

D−

1 2

AD−

1 2

.

L

= UΛUT,

Λ

is

a

diagonal

matrix

of

eigenvalues,

and

UT x is the graph Fourier transform of the input signal x. In practice, gθ can be un-

derstood as a function of the eigenvalues of the normalized graph Laplacian matrix

L (i.e. gθ (Λ )). In practice, directly calculating Eqn. equation 4.8 is very computationally expensive, which is quadratic to the number of nodes N. According to

(Hammond et al, 2011), this problem can be circumvented by approximating the

Kfutnhcotirodnerg: θ (Λ ) with a truncated expansion of Chebyshev polynomials Tk(x) up to

∑ gθ′ (Λ ) = K θk′Tk(Λ˜ ), k=0

(4.9)

wofhCerheeΛb˜y=sheλvm2acxoΛef−ficIi,eanntsd.

λmax is the largest eigenvalue of L. Tk(x) are Chebyshev polynomials

θ ′ ∈ RK is the vector which are recursively

defined as Tk(x) = 2xTk−1(x) − Tk−2(x), with T0(x) = 1 and T1(x) = x. By combining Eqn. equation 4.9 and Eqn. equation 4.8, the convolution of a signal x with a filter

gθ′ can be reformulated as below:

K
∑ gθ′ ⋆ x = θk′Tk(L˜ )x, k=0

(4.10)

where on the

L˜

=

2 λmax

L

−

information

I. From this equation, we can see that each node only depends within the Kth-order neighborhood. The overall complexity of

evaluating Eqn. equation 4.10 is O(|E |) (i.e. linear to the number of edges in the

original graph G ), which is very efficient.

To define a neural network based on graph convolutions, one can stack multiple

convolution layers defined according to Eqn. equation 4.10 with each layer followed

by a nonlinear transformation. At each layer, instead of being limited to the explicit

parametrization by the Chebyshev polynomials defined in Eqn. equation 4.10, the

authors of GCNs proposed to limit the number of convolutions to K = 1 at each

layer. By doing this, at each layer, it only defines a linear function over the graph

Laplacian matrix L. However, by stacking multiple such layers, we are still capable

of covering a rich class of convolution filter functions on graphs. Intuitively, such a

model is capable of alleviating the problem of overfitting local neighborhood struc-

tures for graphs whose node degree distribution has a high variance such as social

networks, the World Wide Web, and citation networks.

At each layer, we can further approximate λmax ≈ 2, which could be accommo-

dated by the neural network parameters during training. Based on al these simplifi-

cations, we have

gθ

′

⋆

x

≈

θ0′ x

+

θ1′ x(L

−

IN

)x

=

θ0′ x

−

θ1′ D−

1 2

AD−

1 2

,

(4.11)

where θ0′ and θ1′ are too free parameters, which could be shared over the entire graph. In practice, we can further reduce the number of parameters, which allows to

46

Jian Tang and Renjie Liao

reduce overfitting and meanwhile minimize the number of operations per layer. As a result, the following expression can be further obtained:

gθ

⋆

x

≈

θ

(I

+

D−

1 2

AD−

1 2

)x,

(4.12)

where

θ

=

θ0′

=

−θ1′ .

One

potential

issue

is

the

matrix

IN

+

D−

1 2

AD−

1 2

,

whose

eigenvalues lie in the interval of [0, 2]. In a deep graph convolutional neural network,

repeated application of the above function will likely lead to exploding or vanish-

ing gradients, yielding numerical instabilites. As a result, we can further renormal-

ize

this

matrix

by

converting

I

+

D−

1 2

AD−

1 2

to

D˜ −

1 2

A˜D˜ −

1 2

,

where

A˜ = A + I,

and

D˜ ii = ∑ j A˜i j.

In the above, we only consider the case that there is only one feature channel

and one filter. This can be easily generalized to an input signal with C channels

X ∈ RN×C and F filters (or number of hidden units) as follows:

H

=

D˜ −

1 2

A˜D˜ −

1 2

XW,

(4.13)

where W ∈ RC×F is a matrix of filter parameters. H is the convolved signal matrix.

4.2.3 Graph Attention Networks

In GCNs, for a target node i, the importance of a neighbor j is determined by the weight of their edge Ai j (normalized by their node degrees). However, in practice, the input graph may be noisy. The edge weights may not be able to reflect the true strength between two nodes. As a result, a more principled approach would be to automatically learn the importance of each neighbor. Graph Attention Networks (a.k.a. GAT(Velicˇkovic´ et al, 2018)) is built on this idea and try to learn the importance of each neighbor based on the Attention mechanism (Bahdanau et al, 2015; Vaswani et al, 2017). Attention mechanism has been wide used in a variety of tasks in natural language understanding (e.g. machine translation and question answering) and computer vision (e.g. visual question answering and image captioning). Next, we will introduce how attention is used in graph neural networks.

Graph Attention Layer. The graph attention layer defines how to transfer the hidden node representations at layer k − 1 (denoted as Hk−1 ∈ RN×F ) to the new node representations Hk ∈ RN×F′ . In order to guarantee sufficient expressive power to transform the lower-level node representations to higher-level node representations, a shared linear transformation is applied to every node, denoted as W ∈ RF×F′ . Afterwards, self-attention is defined on the nodes, which measures the attention coefficients for any pair of nodes through a shared attentional mechanism a : RF′ × RF′ → R

ei j = a(W Hik−1,W Hkj −1).

(4.14)

4 Graph Neural Networks for Node Classification

47

ei j indicates the relationship strength between node i and j. Note in this subsection we use Hik−1 to represent a column-wise vector instead of a row-wise vector. For each node, we can theoretically allow it to attend to every other node on the graph, which however will ignore the graph structural information. A more reasonable solution would be only to attend to the neighbors for each node. In practice, the first-order neighbors are only used (including the node itself). And to make the coefficients comparable across different nodes, the attention coefficients are usually normalized with the softmax function:

αi j

=

Softmax j({ei j})

=

exp(ei j) ∑l∈N(i) exp(eil

)

.

(4.15)

We can see that for a node i, αi j essentially defines a multinomial distribution over the neighbors, which can also be interpreted as the transition probability from node i to each of its neighbors.
In the work by Velicˇkovic´ et al (2018), the attention mechanism a is defined as a single-layer feedforward neural network including a linear transformation with the weight vector W2 ∈ R1×2F′ ) and a LeakyReLU nonlinear activation function (with negative input slope α = 0.2). More specifically, we can calculate the attention coefficients with the following architecture:

αi j

=

exp(LeakyReLU(W2[W Hik−1||W ∑l∈N(i) exp(LeakyReLU(W2[W Hik−1

Hkj −1])) ||W Hlk−1

]))

,

(4.16)

where || represents the operation of concatenating two vectors. The new node representation is a linear combination of the neighboring node representations with the weights determined by the attention coefficients (with a potential nonlinear transformation), i.e.

∑ Hik = σ

αi

jW

H

k−1 j

.

j∈N(i)

(4.17)

Multi-head Attention. In practice, instead of only using one single attention mechanism, multi-head at-
tention can be used, each of which determines a different similarity function over the nodes. For each attention head, we can independently obtain a new node representation according to Eqn. equation 4.17. The final node representation will be a concatenation of the node representations learned by different attention heads. Mathematically, we have

T

Hik =

∑ σ

αitjW

t

H

k−1 j

,

t=1

j∈N(i)

(4.18)

48

Jian Tang and Renjie Liao

where T is the total number of attention heads, αitj is the attention coefficient calculated from the t-th attention head, W t is the linear transformation matrix of the t-th attention head.
One thing that mentioned in the paper by Velicˇkovic´ et al (2018) is that in the final layer, when trying to combine the node representations from different attention heads, instead of using the operation concatenation, other pooling techniques could be used, e.g. simply taking the average node representations from different attention heads.

Hik = σ

∑ ∑ 1
T

T t=1

αitjW t Hkj −1
j∈N(i)

.

(4.19)

4.2.4 Neural Message Passing Networks

Another very popular graph neural network architecture is the Neural Message Passing Network (MPNN) (Gilmer et al, 2017), which is originally proposed for learning molecular graph representations. However, MPNN is actually very general, provides a general framework of graph neural networks, and could be used for the task of node classification as well. The essential idea of MPNN is formulating existing graph neural networks as a general framework of neural message passing among nodes. In MPNNs, there are two important functions including Message and Updating function:

∑ mki =

Mk

(Hik−1

,

H

k−1 j

,

ei

j

),

i∈N( j)

(4.20)

Hik = Uk(Hik−1, mki ).

(4.21)

Mk(·, ·, ·) defines the message between node i and j in the k-th layer, which depends on the two node representations and the information of their edge. Uk is the node updating function in the k-th layer which combines the aggregated messages from the neighbors and the node representation itself. We can see that the MPNN framework is very similar to the general framework we introduced in Section 4.2.1. The AGGREGATE function defined here is simply a summation of all the messages from the neighbors. The COMBINE function is the same as the node Updating function.

4.2.5 Continuous Graph Neural Networks
The above graph neural networks iteratively update the node representations with different kinds of graph convolutional layers. Essentially, these approaches model

4 Graph Neural Networks for Node Classification

49

the discrete dynamics of node representations with GNNs. Xhonneux et al (2020) proposed the continuous graph neural networks (CGNNs), which generalizes existing graph neural networks with discrete dynamics to continuous settings, i.e., trying to model the continuous dynamics of node representations. The key idea is how to characterize the continuous dynamics of node representations, i.e. the derivatives of node representation w.r.t. time. The CGNN model is inspired by the diffusion-based models on graphs such as PageRank and epidemic models on social networks. The derivatives of the node representations are defined as a combination of the node representation itself, the representations of its neighbors, and the initial status of the nodes. Specifically, two different variants of node dynamics are introduced. The first model assumes that different dimensions of node presentations (a.k.a. feature channels) are independent; the second model is more flexible, which allows different feature channels to interact with each other. Next, we give a detailed introduction to each of the two models. Note: in this part, instead of using the original adjacency matrix A, we use the following regularized matrix for characterizing the graph structure:

A

:=

α 2

I

+

D−

1 2

AD−

1 2

,

(4.22)

where α ∈ (0, 1) is a hyperparameter. D is the degree matrix of the original adjacency matrix A. With the new regularized adjacency matrix A, the eigenvalues of A will lie in the interval [0, α], which will make Ak converges to 0 when we increase the power of k.

Model 1: Independent Feature Channels. As different nodes in a graph are interconnected, a natural solution to model the dynamic of each feature channel should be taking the graph structure into consideration, which allows the information to propagate across different nodes. We are motivated by existing diffusion-based methods on graphs such as PageRank (Page et al, 1999) and label propagation (Zhou et al, 2004), which defines the discrete propagation of node representations (or signals on nodes) with the following step-wise propagation equations:

Hk+1 = AHk + H0,

(4.23)

where H0 = X or the output of an encoder on the input feature X. Intuitively, at each step, the new node representation is a linear combination of its neighboring node representations as well as the initial node features. Such a mechanism allows to model the information propagation on the graph without forgetting the initial node features. We can unroll Eqn. equation 4.23 and explicitly derive the node representations at the k-th step:

k

∑ Hk =

Ai H0 = (A − I)−1(Ak+1 − I)H0.

i=0

(4.24)

As the above equation effectively models the discrete dynamics of node representations, the CGNN model further extended it to the continuous setting, which

50

Jian Tang and Renjie Liao

replaces the discrete time step k to a continuous variable t ∈ R+0 . Specifically, it has been shown that Eqn. equation 4.24 is a discretization of the following ordinary differential equation (ODE):

dHt dt

= log AHt + X,

(4.25)

with the initial value H0 = (log A)−1(A − I)X, where X is the initial node features or the output of an encoder applied to it. We do not provide the proof here. More details can be referred to the original paper (Xhonneux et al, 2020). In Eqn. equation 4.25, as log A is intractable to compute in practice, it is approximated with the first-order of the Taylor expansion, i.e. log A ≈ A − I. By integrating all these information, we have the following ODE equation:

dHt dt

= (A − I)Ht

+X,

(4.26)

with the initial value H0 = X, which is the first variant of the CGNN model. The CGNN model is actually very intuitive, which has a nice connection with
traditional epidemic model, which aims at studying the dynamics of infection in a population. For the epidemic model, it usually assumes that the infection of people will be affected by three different factors including the infection from neighbors, the natural recovery, and the natural characteristics of people. If we treat Ht as the number of people infected at time t, then these three factors can be naturally modeled by the three terms in Eqn. equation 4.26: AHt for the infection from neighbors, −Ht for the natural recovery, and the last one X for the natural characteristics of people.

Model 2: Modeling the Interaction of Feature Channels. The above model assumes different node feature channels are independent with each other, which is a very strong assumption and limits the capacity of the model. Inspired by the success of a linear variant of graph neural networks (i.e., Simple GCN (Wu et al, 2019a)), a more powerful discrete node dynamic model is proposed, which allows different feature channels to interact with each other as,

Hk+1 = AHkW + H0,

(4.27)

where W ∈ RF×F is a weight matrix used to model the interactions between different feature channels. Similarly, we can also extend the above discrete dynamics into continuous case, yielding the following equation:

dHt dt

= (A − I)Ht

+ Ht (W

− I) + X,

(4.28)

with the initial value being H0 = X. This is the second variant of CGNN with trainable weights. Similar form of ODEs defined in Eqn. equation 4.28 has been studied in the literature of control theory, which is known as Sylvester differential equation (Locatelli and Sieniutycz, 2002). The two matrices A − I and W − I characterize

4 Graph Neural Networks for Node Classification

51

the natural solution of the system while X is the information provided to the system to drive the system into the desired state.

Discussion. The proposed continuous graph neural networks (CGNN) has multiple nice properties: (1) Recent work has shown that if we increase the number of layers K in the discrete graph neural networks, the learned node representations tend to have the problem of over-smoothing (will introduce in detail later) and hence lose the power of expressiveness. On the contrary, the continuous graph neural networks are able to train very deep graph neural networks and are experimentally robust to arbitrarily chosen integration time; (2) For some of the tasks on graphs, it is critical to model the long-range dependency between nodes, which requires training deep GNNs. Existing discrete GNNs fail to train very deep GNNs due to the oversmoothing problem. The CGNNs are able to effectively model the long-range dependency between nodes thanks to the stability w.r.t. time. (3) The hyperparameter α is very important, which controls the rate of diffusion. Specifically, it controls the rate at which high-order powers of regularized matrix A vanishes. In the work proposed by (Xhonneux et al, 2020), the authors proposed to learn a different value of α for each node, which hence allows to choose the best diffusion rates for different nodes.

4.2.6 Multi-Scale Spectral Graph Convolutional Networks

Recall the one-layer graph convolution operator used in GCNs (Kipf and Welling,

2017b)

H

=

LHW

,

where

L

=

D−

1 2

A˜D−

1 2

.

Here

we

drop

the

superscript

of

the

layer

index to avoid the clash with the notation of the matrix power. There are two main

issues with this simple graph convolution formulation. First, one such graph convo-

lutional layer would only propagate information from any node to its nearest neigh-

bors, i.e., neighboring nodes that are one-hop away. If one would like to propagate

information to M-hop away neighbors, one has to either stack M graph convolutional

layers or compute the graph convolution with M-th power of the graph Laplacian,

i.e., H = σ (LMHW ). When M is large, the solution of stacking layers would make

the whole GCN model very deep, thus causing problems in learning like the van-

ishing gradient. This is similar to what people experienced in training very deep

feedforward neural networks. For the matrix power solution, naively computing the

M-th power of the graph Laplacian is also very costly (e.g., the time complexity is

O(N3(M−1)) for graphs with N nodes). Second, there are no learnable parameters

in GCNs associated with the graph Laplacian L (corresponding to the connectiv-

ities/structures). The only learnable parameter W is a linear transform applied to

every node simultaneously which is not aware of the structures. Note that we typ-

ically associate learnable weights on edges while applying the convolution applied

to regular graphs like grids (e.g., applying 2D convolution to images). This would

greatly improve the expressiveness of the model. However, it is not clear that how

52

Jian Tang and Renjie Liao

one can add learnable parameters to the graph Laplacian L since its size varies from graph to graph.

Algorithm 1 : Lanczos Algorithm

1: Input: S, x, M, ε

2: Initialization: β0 = 0, q0 = 0, and q1 = x/∥x∥

3: For j = 1, 2, . . . , K:

4:

z = Sq j

5: 6:

γ z

j==zq−⊤j γzj

q

j

−

β

j−1 q

j−1

7:

β j = ∥z∥2

8:

If β j < ε, quit

9:

q j+1 = z/β j

10:

11: Q = [q1, q2, · · · , qM]

12: Construct T following Eq. (4.29)

13: Eigen decomposition T = BRB⊤

14: Return V = QB and R. =0

{"#, %#}' = )*+,-./())

Long Range Spectral Filtering e.g., I = {20, 50, , … }
'
)>< = K N<("#OP , "#OQ , … , "#O|R|)%#%#S
#LM
7< = = )><?@< ∀B ∈ [|F|]

Long Range Spectral Filtering e.g., I = {20, 50, , … }
'
)>< = K N<("#OP , "#OQ , … , "#O|R|)%#%#S
#LM
7< = = )><?@< ∀B ∈ [|F|]

concat 7

concat 7

... 2 = 345645(7)

Short Range Spectral Filtering e.g., S = 1, 2, …
7< = = )TU?@< ∀B ∈ [|V|]
Layer 1

Short Range Spectral Filtering e.g., S = 1, 2, …
7< = = )TU?@< ∀B ∈ [|V|]
Layer 2

Fig. 4.1: The inference procedure of Lanczos Networks. The approximated top eigenvalues {rk} and eigenvectors {vk} are computed by the Lanczos algorithm. Note that this step is only needed once per graph. The long range/scale (top blocks) graph convolutions are efficiently computed by the low-rank approximation of the graph Laplacian. One can control the ranges (i.e., the exponent of eigenvalues) as hyperparameters. Learnable spectral filters are applied to the approximated top eigenvalues {rk}. The short range/scale (bottom blocks) graph convolution is the same as GCNs. Adapted from Figure 1 of (Liao et al, 2019b).

To overcome these two problems, authors propose Lanczos Networks in (Liao et al, 2019b). Given the graph Laplacian matrix L1 and node features X, one first
1 Here we assume a symmetric graph Laplacian matrix. If it is non-symmetric (e.g., for directed graphs), one can resort to the Arnoldi algorithm.

4 Graph Neural Networks for Node Classification

53

uses the M-step Lanczos algorithm (Lanczos, 1950) (listed in Alg. 1) to compute an orthogonal matrix Q and a symmetric tridiagonal matrix T , such that Q⊤LQ = T . We denote Q = [q1, · · · , qM] where column vector qi is the i-th Lanczos vector. Note that M could be much smaller than the number of nodes N. T is illustrated as below,





γ1 β1

T

=

 β1

... ...

... ...

βM−1  .

(4.29)

βM−1 γM

After obtaining the tridiagonal matrix T , we can compute the Ritz values and Ritz vectors which approximate the top eigenvalues and eigenvectors of L by diagonalizing the matrix T as T = BRB⊤, where the K × K diagonal matrix R contains the Ritz values and B ∈ RK×K is an orthogonal matrix. Here top means ranking the eigenvalues by their magnitudes in a descending order. This can be implemented via the general eigendecomposition or some fast decomposition methods specialized for tridiagonal matrices. Now we have a low rank approximation of the graph Laplacian matrix L ≈ V RV ⊤, where V = QB. Denoting the column vectors of V as {v1, · · · , vM}, we can compute multi-scale graph convolution as

H = Lˆ HW

M

∑ Lˆ =

fθ (rmI1 , rmI2 , · · · , rmIu )vmv⊤m ,

m=1

(4.30)

where {I1, · · · , Iu} is the set of scale/range parameters which determine how many hops (or how far) one would like to propagate the information over the graph. For example, one could easily set {I1 = 50, I2 = 100} (u = 2 in this case) to consider the situations of propagating 50 and 100 steps respectively. Note that one only needs to compute the scalar power rather than the original matrix power. The overall complexity of the Lanczos algorithm in our context is O(MN2) which makes the whole algorithm much more efficient than naively computing the matrix power. Moreover, fθ is a learnable spectral filter parameterized by θ and can be applied to graphs with varying sizes since we decouple the graph size and the input size of fθ . fθ directly acts on the graph Laplacian and greatly improves the expressiveness of the model.
Although Lanczos algorithm provides an efficient way to approximately compute arbitrary powers of the graph Laplacian, it is still a low-rank approximation which may lose certain information (e.g., the high frequency one). To alleviate the problem, one can further do vanilla graph convolution with small scale parameters like H = LSHW where S could be small integers like 2 or 3. The resultant representation can be concatenated with the one obtained from the longer scale/range graph convolution in Eq. (4.30). Relying on the above design, one could add nonlinearities and stack multiple such layers to build a deep graph convolutional network (namely Lanczos Networks) just like GCNs. The overall inference procedure of Lanczos Networks is shown in Fig. 4.1. This method demonstrates strong empirical

54

Jian Tang and Renjie Liao

performances on a wide variety of tasks/benchmarks including molecular property prediction in quantum chemistry and document classification in citation networks. It just requires slight modifications to the implementation of the original GCNs. Nevertheless, if the input graph is extremely large (e.g., some large social network), the Lanczos algorithm itself would be a computational bottleneck. How to improve this model in such a problem context would be an open question.
Here we only introduce a few representative architectures of graph neural networks for node classification. There are also many other well-known architectures including gated graph neural networks (Li et al, 2016b)—which is mainly designed for output sequences—and GraphSAGE (Hamilton et al, 2017b)—which is mainly designed for inductive setting of node classification.

4.3 Unsupervised Graph Neural Networks
In this section, we review a few representative GNN-based methods for unsupervised learning on graph-structured data, including variational graph auto-encoders (Kipf and Welling, 2016) and deep graph infomax (Velicˇkovic´ et al, 2019).

4.3.1 Variational Graph Auto-Encoders
Following variational auto-encoders (VAEs) (Kingma and Welling, 2014; Rezende et al, 2014) , variational graph auto-encoders (VGAEs) (Kipf and Welling, 2016) provide a framework for unsupervised learning on graph-structured data. In the following, we first review the model and then discuss its advantages and disadvantages.
4.3.1.1 Problem Setup Suppose we are given an undirected graph G = (V , E ) with N nodes. Each node is associated with a node feature/attribute vector. We compactly denote all node features as a matrix X ∈ RN×C. The adjacency matrix of the graph is A. We assume self-loops are added to the orignal graph G so that the diagonal entries of A are 1. This is a convention in graph convolutional networks (GCNs) (Kipf and Welling, 2017b) and makes the model consider a node’s old representation while updating its new representation. We also assume each node is associated with a latent variable (the collection of all latent variables is again compactly denoted as a matrix Z ∈ RN×F ). We are interested in inferring the latent variables of nodes in the graph and decoding the edges.

4 Graph Neural Networks for Node Classification

55

4.3.1.2 Model

Similar to VAEs, the VGAE model consists of an encoder qφ (Z|A, X), a decoder pθ (A|Z), and a prior p(Z).
Encoder The goal of the encoder is to learn a distribution of latent variables associated with each node conditioning on the node features X and the adjacency matrix A. We could instantiate qφ (Z|A, X) as a graph neural network where the learnable parameters are φ . In particular, VGAE assumes an node-independent encoder as below,

N
qφ (Z|X, A) = ∏ qφ (zi|X, A) i=1
qφ (zi|X, A) = N (zi|µi, diag(σi2)) µ, σ = GCNφ (X, A)

(4.31)
(4.32) (4.33)

where zi, µi, and σi are the i-th rows of the matrices Z, µ, and σ respectively. Basically, we assume a multivariate Normal distribution with the diagonal covariance as the variational approximated distribution of the latent vector per node (i.e., zi). The mean and diagonal covariance are predict by the encoder network, i.e., a GCN as described in Section 4.2.2. For example, the original paper uses a two-layer GCN as follows,

µ = A˜HWµ σ = A˜HWσ H = ReLU(A˜XW0),

(4.34) (4.35) (4.36)

where

A˜

=

D−

1 2

AD−

1 2

is

the

symmetrically

normalized

adjacency

matrix

and

D

is

the degree matrix. Learnable parameters are thus φ = [Wµ,Wσ,W0]. Decoder Given sampled latent variables, the decoder aims at predicting the con-

nectivities among nodes. The original paper adopts a simple dot-product based pre-

dictor as below,

NN
p(A|Z) = ∏ ∏ p(Ai j|zi, z j) i=1 j=1
p(Ai j|zi, z j) = σ(z⊤i z j),

(4.37) (4.38)

where Ai j denotes the (i, j)-th element and σ(·) is the logistic sigmoid function. This decoder again assumes conditional independence among all possible edges for tractability. Note that there are no learnable parameters associated with this decoder. The only way to improve the performance of the decoder is to learn good latent representations.
Prior The prior distributions over the latent variables are simply set to independent zero-mean Gaussians with unit variances,

56

Jian Tang and Renjie Liao

N
p(Z) = ∏ N (zi|0, I). i=1

(4.39)

This prior is fixed throughout the learning as what typical VAEs do. Objective & Learning To learn the encoder and the decoder, one typically max-
imize the evidence lower bound (ELBO) as in VAEs,

LELBO = Eqφ (Z|X,A) [log p(A|Z)] − KL(qφ (Z|X , A)∥p(Z)),

(4.40)

where KL(q∥p) is the Kullback-Leibler divergence between distributions q and p. Note that we can not directly maximize the log likelihood since the introduction of latent variables Z induces a high-dimensional integral which is intractable. We instead maximize the ELBO in Eq. (4.40) which is a lower bound of the log likelihood. However, the first expectation term is again intractable. One often resorts to the Monte Carlo estimation by sampling a few Z from the encoder qφ (Z|X, A) and evaluating the term using the samples. To maximize the objective, one can perform stochastic gradient descent along with the reparameterization trick (Kingma and Welling, 2014). Note that the reparameterization trick is necessary since we need to back-propagate through the sampling in the aforementioned Monte Carlo estimation term to compute the gradient w.r.t. the parameters of the encoder.

4.3.1.3 Discussion The VGAE model is popular in the literature mainly due to its simplicity and good empirical performances. For example, since there are no learnable parameters for the prior and the decoder, the model is quite light-weight and the learning process is fast. Moreover, the VGAE model is versatile in way that once we learned a good encoder, i.e., good latent representations, we can use them for predicting edges (, link prediction), node attributes, and so on. On the other side, VGAE model is still limited in the following ways. First, it can not serve as a good generative model for graphs as what VAEs do for images since the decoder is not learnable. One could simply design some learnable decoder. However, it is not clear that the goal of learning good latent representations and generating graphs with good qualities are always well-aligned. More exploration along this direction would be fruitful. Second, the independence assumption is exploited for both the encoder and the decoder which might be very limited. More structural dependence (e.g., auto-regressive) would be desirable to improve the model capacity. Third, as discussed in the original paper, the prior may be potentially a poor choice. At last, for link prediction in practice, one may need to add the weighting of edges vs. non-edges in the decoder term and carefully tune it since graphs may be very sparse.

4 Graph Neural Networks for Node Classification

57

4.3.2 Deep Graph Infomax

Following Mutual Information Neural Estimation (MINE) (Belghazi et al, 2018) and Deep Infomax (Hjelm et al, 2018), Deep Graph Infomax (Velicˇkovic´ et al, 2019) is an unsupervised learning framework that learns graph representations via the principle of mutual information maximization.

4.3.2.1 Problem Setup Following the original paper, we will explain the model under the single-graph setup, i.e., the node feature matrix X and the graph adjacency matrix A of a single graph are provided as input. Extensions to other problem setups like transductive and inductive learning settings will be discussed in Section 4.3.2.3. The goal is to learn the node representations in an unsupervised way. After node representations are learned, one can apply some simple linear (logistic regression) classifier on top of the representations to perform supervised tasks like node classification.

4.3.2.2 Model

(X, A)

(H, A)

xi

E

C

xj

E

hi

D

+

R

s

hj

D

−

(X, A)

(H, A)

Fig. 4.2: The overall process of Deep Graph Infomax. The top path shows how the positive sample is processed, whereas the bottom shows process corresponding to the negative sample. Note that the graph representation is shared for both positive and negative samples. Subgraphs of positive and negative samples do not necessarily need to be different. Adapted from Figure 1 of (Velicˇkovic´ et al, 2019).

The main idea of the model is to maximize the local mutual information between a node representation (capturing local graph information) and the graph representation (capturing global graph information). By doing so, the learned node representation should capture the global graph information as much as possible. Let us denote the graph encoder as ε which could be any GNN discussed before, e.g., a two-layer GCN. We can obtain all node representations as H = ε(X, A) where the

58

Jian Tang and Renjie Liao

representation hi of any node i should contain some local information near node i. Specifically, k-layer GCN should be able to leverage node information that is k-hop away. To get the global graph information, one could use a readout layer/function to process all node representations, i.e., s = R(H), where the readout function R could be some learnable pooling function or simply an average operator.
Objective Given the local node representation hi and the global graph representation s, the natural next-step is to compute their mutual information. Recall the definition of mutual information is as follows,

MI(h, s) =

p(h, s) log

p(h, s) p(h) p(s)

dhds.

(4.41)

However, maximizing the local mutual information alone is not enough to learn useful representations as shown in (Hjelm et al, 2018). To develop a more practical objective, authors in (Velicˇkovic´ et al, 2019) instead use a noise-contrastive type objective following Deep Infomax (Hjelm et al, 2018),

∑ ∑ 1
L = N+M

N E(X,A) [log D (hi, s)] + M E(X˜ ,A˜) log 1 − D (h˜ j, s)

i=1

j=1

. (4.42)

where D is a binary classifier which takes both the node representation hi and the

graph representation s as input and predicts whether the pair (hi, s) comes from the

joint ative

distribution p(h, s) (positive class) or class). We denote h˜ j as the j-th node

the product of representation

marginals p(hi)p(s) (negfrom the negative sample.

The numbers of positive and negative samples are N and M respectively. We will

explain how to draw positive and negative samples shortly. The overall objective is

thus the negative binary cross-entropy for training a probabilistic classifier. Note that

this objective is the same type of distance as used in generative adversarial networks

(GANs) (Goodfellow et al, 2014b) which is shown to be proportional to the Jensen-

Shannon divergence (Goodfellow et al, 2014b; Nowozin et al, 2016). As verified by

(Hjelm et al, 2018), maximizing the Jensen-Shannon divergence based mutual in-

formation estimator behaves similarly (i.e., they have an approximately monotonic

relationship) to directly maximizing the mutual information. Therefore, maximizing

the objective in Eq. (4.42) is expected to maximize the mutual information. More-

over, the freedom of choosing negative samples makes the method more likely to

learn useful representations than maximizing the vanilla mutual information.

Negative Sampling To generate the positive samples, one can directly sample a

few nodes from the graph to construct the generate them via corrupting the original

pgarairpsh(hdai,tsa),.dFeonrontienggataisve(Xs˜a,mA˜)pl=esC, o(nXe,cAa)n.

In practice, one can choose various forms of this corruption function C . For ex-

ample, authors in (Velicˇkovic´ et al, 2019) suggest to keep the adjacency matrix to

be the same and corrupt the node feature X by row-wise shuffling. Other possibili-

ties of the corruption function include randomly sampling subgraphs and applying

Dropout (Srivastava et al, 2014) to node features.

4 Graph Neural Networks for Node Classification

59

Once positive and negative samples were collected, one can learn the representations via maximizing the objective in Eq. (4.42). We summarize the training process of Deep Graph Infomax as follows:

1. Sample negative examples via the corruption function (X˜ , A˜) ∼ C (X, A).

2. 3.

Compute Compute

node node

representations representations

of of

positive negative

samples samples

H H˜

= =

{{hh˜11,,

··· ···

,,hh˜NM}}==εε((XX˜,,AA˜))..

4. Compute graph representation via the readout function s = R(H).

5. Update parameters of ε, D, and R via gradient ascent to maximize Eq. (4.42).

4.3.2.3 Discussion Deep Graph Infomax is an efficient unsupervised representation learning method for graph-structured data. The implementation of the encoder, the readout, and the binary cross-entropy type of loss are all straightforward. The mini-batch training does not necessarily need to store the whole graph since the readout can be applied to a set of subgraphs as well. Therefore, the method is memory-efficient. Also, the processing of positive and negative samples can be done in parallel. Moreover, authors prove that minimizing the cross-entropy type of classification error can be used to maximize the mutual information under certain conditions, e.g., the readout function is injective and input feature comes from a finite set. However, the choice of the corruption function seems to be crucial to ensure satisfying empirical performances. There seems no such a universally good corruption function. One needs to do trial–and-error to obtain a proper one depending on the task/dataset.

4.4 Over-smoothing Problem

Training deep graph neural networks by stacking multiple layers of graph neural networks usually yields inferior results, which is a common problem observed in many different graph neural network architectures. This is mainly due to the problem of over-smoothing, which is first explicitly studied in (Li et al, 2018b). (Li et al, 2018b) showed that the graph convolutional network (Kipf and Welling, 2017b) is a special case of Laplacian smoothing:

Y = (1 − γI)X + γA˜rwX,

(4.43)

where A˜rw = D˜ −1A˜, which defines the transitional probabilities between nodes on

graphs. The GCN corresponds to a special case of Laplacian smoothing with γ = 1

and

the

symmetric

matrix

A˜ sym

=

D˜ −

1 2

A˜D˜ −

1 2

is

used.

The

Laplacian

smoothing

will

push nodes belonging to the same clusters to take similar representations, which

are beneficial for downstream tasks such as node classification. However, when the

GCNs go deep, the node representations suffer from the problem of over-smoothing,

i.e., all the nodes will have similar representations. As a result, the performance on

60

Jian Tang and Renjie Liao

downstream tasks suffer as well. This phenomenon has later been pointed out by a few other later work as well such as (Zhao and Akoglu, 2019; Li et al, 2018b; Xu et al, 2018a; Li et al, 2019c; Rong et al, 2020b).

PairNorm (Zhao and Akoglu, 2019). Next, we will present a method called PairNorm for alleviating the problem of over-smoothing when GNNs go deep. The essential idea of PairNorm is to keep the total pairwise squared distance (TPSD) of node representations unchanged, which is the same as that of the original node feature X. Let H˜ be the output of the node representations by the graph convolution, which will be the input of PairNorm, and Hˆ is the output of PairNorm. The goal of PairNorm is to normalize the H˜ such that after normalization TPSD(Hˆ ) = TPSD(X). In other words,

∑ ||Hˆi − Hˆ j||2 + ∑ ||Hˆi − Hˆ j||2 = ∑ ||Xi − Xj||2 + ∑ ||Xi − Xj||2.

(i, j)∈E

(i, j)̸∈E

(i, j)∈E

(i, j)̸∈E

(4.44)

In practice, instead of measuring the TPSD of original node features X, (Zhao

and Akoglu, 2019) proposed to maintain a constant TPSD value C across different

graph convolutional layers. The value C will be a hyperparameter of the PairNorm layer, which can be tuned for each data set. To normalize H˜ into Hˆ with a constant

TPSD, we must first calculate the TPSD(H˜ ). However, this is very computationally

expensive, which is quadratic to the number of nodes N. We notice that the TPSD

can be reformulated as:

∑ ∑ ∑ TPSD(H˜ ) =

||H˜i − H˜ j||2 = 2N2

(i, j)∈[N]

1 N

N ||H˜i||22
i=1

−

||

1 N

N H˜i||22
i=1

(4.45)

We can further simply the above equation by substracting the row-wise mean

from each H˜i. representation.

In A

other words, nice property

H˜ic of

c=enHt˜eir−ingN1

N
∑i=1

H˜i,

the node

which denotes the centered representation is that it will

not change As a result,

the we

TPSD have

and

meanwhile

push

the

second

term

||

1 N

N
∑i=1

H˜i||22

to

zero.

TPSD(H˜ ) = TPSD(H˜ c) = 2N||H˜ c||2F .

(4.46)

To summarize, the proposed PairNorm can be divded into two steps: center-andscale,

∑ H˜ic

=

H˜i

−

1 N

N
H˜i
i=1

Hˆi = s ·

H˜ic

= s√N ·

1 N

N
∑i=1

||H˜ic||22

H˜ic ||H˜ c||2F

(Center) (Scale),

(4.47) (4.48)

4 Graph Neural Networks for Node Classification
where s is a hyperparameter determining C. At the end, we have

TPSD(Hˆ ) = 2N||Hˆ ||2F = 2N ∑ ||s · i

1 N

H˜ic

N
∑i=1

||H˜ic||22

||22

=

2N2s2

which is a constant across different graph convolutional layers.

61
(4.49)

4.5 Summary
In this chapter, we give a comprehensive introduction to different architectures of graph neural networks for node classification. These neural networks can be generally classified into two categories including supervised and unsupervised approaches. For supervised approaches, the main difference among different architectures lie in how to propagate messages between nodes, how to aggregate the messages from neighbors, and how to combine the aggregated messages from neighbors with the node representation itself. For the unsupervised approaches, the main difference comes from designing the objective function. We also discuss a common problem of training deep graph neural networks—over-smoothing, and introduce a method to tackle it. In the future, promising directions on graph neural networks include theoretical analysis for understanding the behaviors of graph neural networks, and applying them to a variety of fields and domains such as recommender systems, knowledge graphs, drug and material discovery, computer vision, and natural language understanding.

Editor’s Notes: Node classification task is one of the most important tasks in Graph Neural Networks. The node representation learning techniques introduced in this chapter are the corner stone for all other tasks for the rest of the book, including graph classification task (Chapter 9), link prediction (Chapter 10), graph generation task (Chapter 11), and so on. Familiar with the learning methodologies and design principles of node representation learning is the key to deeply understanding other fundamental research directions like Theoretical analysis (Chapter 5), Scalability (Chapter 6), Explainability (Chapter 7), and Adversarial Robustness (Chapter 8).

Chapter 5
The Expressive Power of Graph Neural Networks
Pan Li and Jure Leskovec

Abstract The success of neural networks is based on their strong expressive power that allows them to approximate complex non-linear mappings from features to predictions. Since the universal approximation theorem by (Cybenko, 1989), many studies have proved that feed-forward neural networks can approximate any function of interest. However, these results have not been applied to graph neural networks (GNNs) due to the inductive bias imposed by additional constraints on the GNN parameter space. New theoretical studies are needed to better understand these constraints and characterize the expressive power of GNNs. In this chapter, we will review the recent progress on the expressive power of GNNs in graph representation learning. We will start by introducing the most widely-used GNN framework— message passing— and analyze its power and limitations. We will next introduce some recently proposed techniques to overcome these limitations, such as injecting random attributes, injecting deterministic distance attributes, and building higher-order GNNs. We will present the key insights of these techniques and highlight their advantages and disadvantages.

5.1 Introduction
Machine learning problems can be abstracted as learning a mapping f ∗ from some feature space to some target space. The solution to this problem is typically given by a model fθ that intends to approximate f ∗ via optimizing some parameter θ . In practice, the ground truth f ∗ is a priori typically unknown. Therefore, one may expect the model fθ to approximate a rather broad range of f ∗. An estimate of
Pan Li Department of Computer Science, Purdue University, e-mail: panli@purdue.edu Jure Leskovec Department of Computer Science, Stanford University, e-mail: jure@cs.stanford.edu

© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022

63

L. Wu et al. (eds.), Graph Neural Networks: Foundations, Frontiers, and Applications,

https://doi.org/10.1007/978-981-16-6054-2_5

64

Pan Li and Jure Leskovec

how broad such a range could be, called the model’s expressive power, provides an

important measure of the model potential. It is desirable to have models with a more

expressive power that may learn more complex mapping functions.

Neural networks (NNs) are well known for their great expressive power. Specifi-

cally, Cybenko (1989) first proved that any continuous function defined over a com-

pact space could be uniformly approximated by neural networks with sigmoid acti-

vation functions and only one hidden layer. Later, this result got generalized to any

squashing activation functions by (Hornik et al, 1989).

However, these seminal findings are in-

sufficient to explain the current unprece-

dented success of NNs in practice because

their strong expressive power only demonstrates that the model fθ is able to approximate f ∗ but does not guarantee that the model obtained via training fˆ indeed approximates f ∗. Fig. 5.1 illustrates a wellknown curve of Amount of Data vs. Performance of machine learning models (Ng,

Neural networks
Traditional machine learning:
SVM, GBDT

2011). NN-based methods may only out-

perform traditional methods given sufficient data. One important reason is that Fig. 5.1: Amount of Data vs. Perfor-

NNs as machine learning models are still mance of different models.

governed by the fundamental tradeoff be-

tween the data amount and model complex-

ity (Fig. 5.2). Although NNs could be rather expressive, they are likely to overfit the

training examples when paired with more parameters. Therefore, it is necessary for

practice to build NNs that can maintain strong expressive power while constraints

are imposed on their parameters. At the same time, a good theoretical understanding

of the expressive power of NNs with constraints on their parameters is needed.

Optimal model complexity

Testing error

Naively improving the expressive power by increasing model complexity

Training error

Improving the expressive power by injecting inductive bias into the model while keeping model complexity
Without inductive bias With inductive bias

Model complexity

Fig. 5.2: Training and testing errors with and without inductive bias can dramatically affect the expressive power of models.

In practice, constraints on parameters are typically obtained from our prior knowledge of the data; these are referred to as inductive biases. Some significant

5 The Expressive Power of Graph Neural Networks

65

Features Targets
Features Targets

Translation invariance … …
Translation variance … …

RNNs/CNNs share parameters …
RNNs/CNNs do not fit this case

Fig. 5.3: Illustration of 1-dimensional translation invariance/variance. RNNs/CNNs use translation invariance to share parameters.

results about the expressive power of NNs with inductive bias have been shown recently. Yarotsky (2017); Liang and Srikant (2017) have proved that deep neural networks (DNNs), by stacking multiple hidden layers, can achieve good enough approximation with significantly fewer parameters than shallow NNs. The architecture of DNNs leverages the fact that data has typically a hierarchical structure. DNNs are agnostic to the type of data, while dedicated neural network architectures have been developed to support specific types of data. Recurrent neural networks (RNNs) (Hochreiter and Schmidhuber, 1997) or convolution neural networks (CNNs) (LeCun et al, 1989) were proposed to process time series and images, respectively. In these two types of data, effective patterns typically hold translation invariance in time and in space, respectively. To match this invariance, RNNs and CNNs adopt the inductive bias that their parameters have shared across time and space (Fig. 5.3). The parameter-sharing mechanism works as a constraint on the parameters and limits the expressive power of RNNs and CNNs. However, RNNs and CNNs have been shown to have sufficient expressive power to learn translation invariant functions (Siegelmann and Sontag, 1995; Cohen and Shashua, 2016; Khrulkov et al, 2018), which leads to the great practical success of RNNs and CNNs in processing time series and images.
Recently, many studies have focused on a new type of NNs, termed graph neural networks (GNNs) (Scarselli et al, 2008; Bruna et al, 2014; Kipf and Welling, 2017a; Bronstein et al, 2017; Gilmer et al, 2017; Hamilton et al, 2017b; Battaglia et al, 2018). These aim to capture the inductive bias of graphs/networks, another important type of data. Graphs are commonly used to model complex relations and interactions between multiple elements and have been widely used in machine learning applications, such as community detection, recommendation systems, molecule property prediction, and medicine design (Fortunato, 2010; Fouss et al, 2007; Pires et al, 2015). Compared to time series and images, which are well-structured and represented by tables or grids, graphs are irregular and thus introduce new challenges. A fundamental assumption behind machine learning on graphs is that the targets for prediction should be invariant to the order of nodes of the graph. To match this assumption, GNNs hold a general inductive bias termed permutation invariance. In particular, the output given by GNNs should be independent of how the node indices of a graph are assigned and thus in which order are they processed. GNNs require

66

Pan Li and Jure Leskovec

Permutation Invariance Feathers

GNNs are built to match permutation invariance

Targets

GNN

= GNN

Fig. 5.4: This illustrates how GNNs are designed to maintain permutation invariance.

their parameters to be independent from the node ordering and are shared across the entire graph (Fig. 5.4). Because of this new parameter sharing mechanism in GNNs, new theoretical tools are needed to characterize their expressive power.
Analyzing the expressive power of GNNs is challenging, as this problem is closely related to some long-standing problems in graph theory. To understand this connection, consider the following example of how a GNN would predict whether a graph structure corresponds to a valid molecule. The GNN should be able to identify whether this graph structure is the same, similar, or very different from the graph structures that are known to correspond to valid molecules. Measuring whether two graphs have the same structure involves addressing the graph isomorphism problem, in which no P solutions have yet been found (Helfgott et al, 2017). In addition, measuring whether two graphs have a similar structure requires contending with the graph edit distance problem, which is even harder to address than the graph isomorphism problem (Lewis et al, 1983).
Great progress has been made recently on characterizing the expressive power of GNNs, especially on how to match their power with traditional graph algorithms and how to build more powerful GNNs that overcome the limitation of those algorithms. We will delve more into these recent efforts further along in this chapter. In particular, compared to previous introductions (Hamilton, 2020; Sato, 2020), we will focus on recent key insights and techniques that yield more powerful GNNs. Specifically, we will introduce standard message-passing GNNs that are able to achieve the limit of the 1-dimensional Weisfeiler-Lehman test (Weisfeiler and Leman, 1968), a widely-used algorithm to test for graph isomorphism. We will also discuss a number of strategies to overcome the limitations of the Weisfeiler-Lehman test — including attaching random attributes, attaching deterministic distance attributes, and leveraging higher-order structures.
In Section 5.2, we will formulate the graph representation learning problems that GNNs target. In Section 5.3, we will review the most widely used GNN framework, the message passing neural network, describing the limitations of its expressive power and discussing its efficient implementations. In Section 5.4, we will introduce a number of methods that make GNNs more powerful than the message passing neural network. In Section 5.5, we will conclude this chapter by discussing further research directions.

5 The Expressive Power of Graph Neural Networks

67

Observed examples

𝑓

𝑋

𝑌

(a)

ℱ′

ℱ) ′

𝑓∗

𝑓(′

ℳ

(b)

ℱ′

-- the space of all

potential mappings

ℱ) ′

-- the space of mappings

that may represented by NNs

ℳ -- the space of mappings

that fit the observed examples

𝑓∗-- the precise mapping function

𝑓(′ -- the learnt model via NNs

ℱ) ℱ′
𝑓∗
ℱ
(c)

ℱ) ℱ′

𝑓∗

𝑓(

ℳ

ℱ

(d)

ℱ

-- the space of all

potential mappings that satisfy

permutation invariance

ℱ)

-- the space of mappings

that may represented by GNNs

𝑓( -- the learnt model via GNNs

Fig. 5.5: An illustration of the expressive power of NNs and GNNs and their affects on the performance of learned models. a) Machine learning problems aim to learn the mapping from the feature space to the target space based on several observed examples. b) The expressive power of NNs refers to the gap between the two spaces F and Fˆ ′. Although NNs are expressive (Fˆ ′ is dense in F ), the learned model f ′ based on NNs may differ significantly from f ∗ due to their overfit of the limited observed data. c) Suppose f ∗ is known to be permutation invariant, as it works on graph-structured data. Then, the space of potential mapping functions is reduced from F ′ to a much smaller space F that only includes permutation invariant functions. If we adopt GNNs, the space of mapping functions that can be approximated simultaneously reduces to Fˆ . The gap between F and Fˆ characterizes the expressive power of GNNs. d) Although GNNs are less expressive than general NNs (Fˆ ⊂ Fˆ ′), the learned model based on GNNs f is a much better approximator of f ∗ as opposed to the one based on NNs fˆ′. Therefore, for graph-structured data, our understanding of the expressive power of GNNs, i.e., the gap between F and Fˆ , is much more relevant than that of NNs.

5.2 Graph Representation Learning and Problem Formulation
In this section, we will set up the formal definition of graph representation learning problems, their fundamental assumption, and their inductive bias. We will also discuss relationships between different notions of graph representation learning problems frequently studied in recent literature.
First, we will start by defining graph-structured data. Definition 5.1. (Graph-structured data) Let G = (V , E , X) denote an attributed graph, where V is the node set, E is the edge set, and X ∈ R|V |×F are the node attributes. Each row of X, Xv ∈ RF refers to the attributes on the node v ∈ V . In practice, graphs are usually sparse, i.e., |E | ≪ |V |2. We introduce A ∈ {0, 1}|V |×|V | to denote the adjacency matrix of G such that Auv = 1 iff (u, v) ∈ E. Combining the

68 Node classification… 𝑆

Graph classification…

Pan Li and Jure Leskovec 𝑆

𝒢

𝒢

𝑆

𝒢

Link prediction…

𝑓 𝒢, 𝑆 should capture the
informative fingerprint of the graph 𝒢 to represent S for certain
applications (characterized by a ground-truth mapping 𝑓∗ 𝒢, 𝑆 .

Fig. 5.6: Graph representation learning problems frequently discussed in literature.

adjacency matrix and node attributes, we may also denote G = (A, X). Moreover, if G is unattributed with no node attributes, we can assume that all elements in X are constant. Later, we also use V [G ] to denote the entire node set of a particular graph G.
The goal of graph representation learning is to learn a model by taking graphstructured data as input and then mapping it so that certain prediction targets are matched. Different graph representation learning problems may apply to a varying number of nodes in a graph. For example, for node classification, a prediction is made for each node, for each link/relation prediction on a pair of nodes, and for each graph classification or graph property prediction on the entire node set V . We can unify all these problems as graph representation learning.

Definition 5.2. (Graph representation learning) The feature space is defined as

X := Γ × S , where Γ is the space of graph-structured data and S includes all

the node subsets of interest, given a graph G ∈ Γ . Then, a point in X can be de-

noted as (G , S), where S is a subset of nodes that are in G . Later, we call (G , S) as

a graph representation learning (GRL) example. Each GRL example (G , S) ∈ X is

associated with a target y in the target space Y . Suppose the ground-truth associa-

tion function between features and targets is denoted by f ∗ : X → Y , f ∗(G , S) = y.

GpalfievusenΨnctai=osne{tf(oGbf˜a(tirs)ae,idSn˜(ioin)n,gy˜Ξe(ix)s)au}mcki=ph1lte,hsaaΞtgrf=aips{hc(Glroe(spie)r,etSso(ein)f,t∗ya(otii)no)nΨ}ki=l.e1aarnnidnag

set of testing examproblem is to learn

The above definition is general in the sense that in a GRL example (G , S) ∈ X , G provides both raw and structural features on which some prediction for a node subset S of interest is to be made. Below, we will further list a few frequently-investigated learning problems that may be formulated as graph representation learning problems.

Remark 5.1. (Graph classification problem / Graph-level prediction) The node set S of interest is the entire node set V [G ] by default. The space of graph-structured data

5 The Expressive Power of Graph Neural Networks

69

Γ typically contains multiple graphs. The target space Y contains labels of different graphs. Later, for graph-level prediction, we will use G to denote a GRL example instead of (G , S) for notational simplicity.

Remark 5.2. (Node classification problem / Node-level prediction) In a GRL example (G , S), the S corresponds to one single node of interest. The corresponding G can be defined in different ways. On the one hand, only the nodes close to S provide effective features. In this case, G may be set as the induced local subgraph around S. Different G ’s for different S’s may come from a single graph. On the other hand, two nodes that are far apart on one graph still hold mutual impact and can be used as a feature to make a prediction on another graph. In that case, G needs to include a large portion of a graph or even the entire graph.

Remark 5.3. (Link prediction problem / Node-pair-level prediction) In a GRL example (G , S), S corresponds to a pair of nodes of interest. Similar to the node classification problem, G for each example may be an induced subgraph around S or the entire graph. The target space Y contains 0-1 labels that indicate whether there is a probable link between two nodes. Y may also be generalized to include labels that reflect the types of links to be predicted.

Next, we will introduce the fundamental assumption used in most graph representation learning problems. Definition 5.3. (Isomorphism) Consider two GRL examples (G (1), S(1)), (G (2), S(2)) ∈ X . Suppose G (1) = (A(1), X (1)) and G (2) = (A(2), X (2)). If there exists a bijective mapping π : V [G (1)] → V [G (2)], i ∈ {1, 2}, such that Au(1v) = Aπ(2()u)π(v), Xu(1) = Xπ(2(u)) and π also gives a bijective mapping between S(1) and S(2), we call that (G (1), S(1)) and (G (2), S(2)) are isomorphic, denoted as (G (1), S(1)) ∼= (G (2), S(2)). When the particular bijective mapping π should be highlighted, we use notation (G (1), S(1)) ∼=π (G (2), S(2)). If there is no such a π, we call that they are non-isomorphic, denoted as (G (1), S(1)) ̸∼= (G (2), S(2)).
Assumption 1 (Fundamental assumption in graph representation learning) Consider a graph representation learning problem with a feature space X and its corresponding target space Y . Pick any two GRL examples (G (1), S(1)), (G (2), S(2)) ∈ X . The fundamental assumption says that if (G (1), S(1)) ∼= (G (2), S(2)), their corresponding targets in Y are the same.
Due to this fundamental assumption, it is natural to introduce the corresponding permutation invariance as inductive bias that all models of graph representation learning should satisfy.

Definition 5.4. (Permutation invariance) A model f satisfies permutation invariance if for any (G (1), S(1)) ∼= (G (2), S(2)), f (G (1), S(1)) = f (G (2), S(2)).

Now we may define the expressive power of a model for graph representation learning problems.

70

Pan Li and Jure Leskovec

Definition 5.5. (Expressive power) Consider a feature space X of a graph representation learning problem and a model f defined on X . Define another space X ( f ) as a subspace of the quotient space X / ∼= such that for two GRL examples (G (1), S(1)), (G (2), S(2)) ∈ X ( f ), f (G (1), S(1)) ̸= f (G (2), S(2)). Then, the size of X ( f ) characterizes the expressive power of f . For two models, f (1) and f (2), if X ( f (1)) ⊃ X ( f (2)), we say that f (1) is more expressive than f (2).

Remark 5.4. Note that the expressive power in Def. 5.5, characterized by how a model can distinguish non-isomorphic GRL examples, does not exactly match the traditional expressive power used for NNs in the sense of functional approximation. Actually, Def. 5.5 is strictly weaker because distinguishing any non-isomorphic GRL examples does not necessarily indicate that we can approximate any function f ∗ defined over X . However, if a model f cannot distinguish two non-isomorphic features, f is definitely unable to approximate function f ∗ that maps these two examples to two different targets. Some recent studies have been able to prove some equivalence between distinguishing non-isomorphic features and permutation invariant function approximations under weak assumptions and applying involved techniques (Chen et al, 2019f; Azizian and Lelarge, 2020). Interested readers may check these references for more details.

It is trivial to provide the expressive power of a model f for graph representation learning if f does not satisfy permutation invariance. Without such a constraint, NNs can approximate all continuous functions (Cybenko, 1989), which include the continuous functions that distinguish any non-isomorphic GRL examples. Therefore, the key question we are to discuss in the chapter is: “How to build the most expressive permutation invariant models, GNNs in particular, for graph representation learning problems?”

5.3 The Power of Message Passing Graph Neural Networks 5.3.1 Preliminaries: Neural Networks for Sets
We will start by reviewing the NNs with sets (multisets) as their input,since a set can be viewed as a simplified-version of a graph where all edges are removed. By definition, the order of elements of a set does not impact the output; models that encode sets naturally provide an important building block for encoding the graphs. We term this approach invariant pooling. Definition 5.6. (Multiset) A multiset is a set where its elements can be repetitive, meaning that they are present multiple times. In this chapter, we assume by default that all the sets are multisets and thus allow repetitive elements. In situations where this is not the case, we will indicate otherwise.

5 The Expressive Power of Graph Neural Networks

71

Definition 5.7. (Invariant pooling) Given a multiset of vectors S = {a1, a2, ..., ak} where ai ∈ RF and F is an arbitrary constant, an invariant pooling refers to a mapping, denoted as q(S), that is invariant to the order of elements in S.

Some widely-used invariant pooling operations include: sum pooling q(S) =

k
∑i=1

ai,

for all j

m∈ e[1an, Fp]o. oZlainhgeeqr(eSt)a=l (21k0∑1ki7=)1sahioawndthmataaxnypoinovlianrgia[nqt(pSo)]ojli=ngms oaxf ia∈[s1e,Ft]S{aciaj}n

be approximated approximated by

bfuylqly(Sc)on=nφec(t∑edki=N1 ψN(sa, pi)r)o,vwidheedrethφaat nadi,

ψ are functions that may be i ∈ [k] comes from a count-

able universe. This statement can be generalized to the case where S is a multiset

(Xu et al, 2019d).

5.3.2 Message Passing Graph Neural Networks

Message passing is the most widely-used framework to build GNNs (Gilmer et al, 2017). Given a graph G = (V , E , X), the message passing framework encodes each node v ∈ V with a vector representation hv and keeps updating this node representation by iteratively collecting representations of its neighbors and applying neural network layers to perform a non-linear transformation of those collections: 1. Initialize node vector representations as node attributes: h(v0) ← Xv, ∀v ∈ V . 2. Update each node representation based on message passing over the graph
structure. In l-th layer, l = 1, 2, ..., L, perform the following steps:

Message: Aggregation:
Update:

m(vlu) ← MSG(h(vl−1), h(ul−1)), ∀(u, v) ∈ E , a(vl) ← AGG({m(vlu)|u ∈ Nv}), ∀v ∈ V , h(vl) ← UPT(h(vl−1), a(vl)), ∀v ∈ V .

(5.1) (5.2) (5.3)

where Nv is the set of neighbors of v.

The operations MSG, AGG, and UPT MP-GNN to learn the node embedding of the node A: ℎ( )

can be implemented via neural networks. Typically, MSG is implemented by a feedforward NN, e.g., MSG(p, q) = σ (pW1 + qW2), where W1 and W2 are learnable weights, and σ (·) is an element-wise nonlinear activation. UPT can be chosen in a similar way as MSG. AGG differs as its input is a multiset of vectors and thus the or-

E D

A

F

C

B

𝑈𝑃𝑇(… ) 𝐴𝐺𝐺(… ) … 𝑀𝑆𝐺(… ) One neural layer

ℎ(

)
A

ℎ( ) B

CE

ℎ( ) A ℎ( ) C F A B D A F ℎ( )

der of these vectors should not affect the Fig. 5.7: The computing flow of MP-

output. AGG is typically implemented as an GNN to obtain a node representation.

invariant pooling (Def. 5.7). Each layer k

can have different parameters from other layers. We will denote the GNNs that fol-

low this message passing framework as MP-GNN.

72

Pan Li and Jure Leskovec

MP-GNN produces representations of all the nodes, {h(vL)|v ∈ V }. Each node representation is essentially determined by a subtree rooted at this node (Fig. 5.7). Given a specific graph representation learning problem, for example, classifying a set of nodes S ⊆ V , we may use the representations of relevant nodes in S to make the prediction:

yˆS = READOUT({h(vL)|v ∈ S}).

(5.4)

where the READOUT operation is often implemented via another invariant pooling when |S| > 1 plus a feed-forward NN to predict the target. Combining Eqs.equation 11.45equation 5.4, MP-GNN builds a GNN model for graph representation learning:

yˆS = fMP−GNN (G , S).

(5.5)

We can show the permutation invariance of MP-GNN by induction over the iteration index l.
Theorem 5.1. (Invariance of MP-GNN) fMP−GNN(·, ·) satisfies permutation invariance (Def. 5.4) as long as the AGG and READOUT operations are invariant pooling operations (Def. 5.7).
Proof. This can be proved trivially by induction.
MP-GNN by default leverages the inductive bias that the nodes in the graph directly affect each other only via their connected edges. The mutual effect between nodes that are not connected by an edge can be captured via paths that connect these nodes via message passing. Indeed, such inductive bias may not match the assumptions in a specific application, and MP-GNN may find it hard to capture mutual effect between two far-away nodes. However, the message-passing framework has several benefits for model implementation and practical deployment. First, it directly works on the original graph structure and no pre-processing is needed. Second, graphs in practice are typically sparse (|E | ≪ |V |2) and thus MP-GNN is able to scale to very large but sparse graphs. Third, each of the three operations MSG, AGG, and UPT can be computed in parallel across all nodes and edges, which is beneficial for parallel computing platforms such as GPUs and map-reduce systems.
Because it is natural and easy to be implemented in practice, most GNN architectures essentially follow the MP-GNN framework by adopting specific MSG, AGG, and UPT operations. Representative approaches include InteractionNet (Battaglia et al, 2016), structure2vec (Dai et al, 2016), GCN (Kipf and Welling, 2017a), GraphSAGE (Hamilton et al, 2017b), GAT (Velicˇkovic´ et al, 2018), GIN (Xu et al, 2019d), and many others (Kearnes et al, 2016; Zhang et al, 2018g).

5.3.3 The Expressive Power of MP-GNN
In this section, we will introduce the expressive power of MP-GNN , following the results proposed in Xu et al (2019d); Morris et al (2019).

