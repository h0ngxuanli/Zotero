OOD Link Prediction Generalization Capabilities of Message-Passing GNNs in Larger Test Graphs

arXiv:2205.15117v5 [cs.LG] 10 Oct 2022

Yangze Zhou Department of Statistics
Purdue University West Lafayette, IN 47903 zhou950@purdue.edu

Gitta Kutyniok Department of Mathematics Ludwig-Maximilians-Universitat München
Munich, Germany kutyniok@math.lmu.de

Bruno Ribeiro Department of Computer Science
Purdue University West Lafayette, IN 47903 ribeiro@cs.purdue.edu

Abstract
This work provides the ﬁrst theoretical study on the ability of graph Message Passing Neural Networks (gMPNNs) —such as Graph Neural Networks (GNNs)— to perform inductive out-of-distribution (OOD) link prediction tasks, where deployment (test) graph sizes are larger than training graphs. We ﬁrst prove nonasymptotic bounds showing that link predictors based on permutation-equivariant (structural) node embeddings obtained by gMPNNs can converge to a random guess as test graphs get larger. We then propose a theoretically-sound gMPNN that outputs structural pairwise (2-node) embeddings and prove non-asymptotic bounds showing that, as test graphs grow, these embeddings converge to embeddings of a continuous function that retains its ability to predict links OOD. Empirical results on random graphs show agreement with our theoretical results.
1 Introduction
Link prediction is the task of predicting whether two nodes likely have a missing link [1, 12, 30, 37, 66]. Link prediction tasks arise in many settings, ranging from predicting edges on bipartite graphs between users and products or content in recommender systems [6, 11, 31, 32, 39, 62], to knowledge graph reconstruction [4, 14, 20, 54, 66, 67], to predicting protein-protein interactions [57].
In recent years, there has been growing interest in applying neural network models to inductive link prediction tasks. Inductive link prediction considers methods trained on a graph Gtr and deployed at test time on another graph Gte. It also encompasses the task of training the method on a smaller induced subgraph Gtr of a larger graph Gte, then deploying it on the entire graph. In particular, our work focuses on graph message-passing Neural Networks (gMPNNs) [21, 60] or, more precisely, the widely used Graph Neural Network (GNN) framework [8, 9, 13, 22, 24, 28, 61, 64, 69].
Our work asks the following questions: Are link prediction methods able to cope with the task of inductive out-of-distribution (OOD) link prediction, where (unseen) test graphs are signiﬁcantly larger than training graphs? How can these OOD link prediction tasks be theoretically deﬁned? Can we obtain non-asymptotic bounds on the generalization capabilities of these methods?
The majority of today’s link prediction methods are based on a similar principle. Consider an attributed graph G = (V, E), with node set V = {1, ..., N }, edge set E ⊆ V × V , and node features
36th Conference on Neural Information Processing Systems (NeurIPS 2022).

F ∈ RN×F0 , F0 ≥ 1. Then, given a pair of nodes i, j ∈ V , after T ≥ 1 iterations over G, these methods produce associated node embeddings (representation vectors) Θ•i , Θ•j ∈ RFT , FT ≥ 1, which are then used in a link function η• : RFT × RFT → [0, 1] such that η•(Θ•i , Θ•j ) predicts the probability that i and j have a missing link in G. In our notation we will denote all node embeddings and associated functions with the superscript “ •”. Henceforth we denote gMPNNs that output structural node embeddings as gMPNNs•.
Node embeddings. The ﬁrst part of our work considers a subset of these methods, where the output node embeddings are permutation equivariant (a.k.a. structural node embeddings [65]). Informally, a sequence of node embeddings Θ• ∈ RN×FT given by an embedding method is permutationequivariant if for any arbitrary graph G and any permutation π ∈ SN of the node indices, where SN is the symmetric group, the resulting isomorphic graph G = (π ◦ V, π ◦ E, π ◦ F ) gets permuted node embeddings Θ• = π ◦ Θ•, where π ◦ M deﬁnes the action of π on M (we will provide a formal deﬁnition in Section 2). We leave the study of OOD link prediction with positional node embeddings (a.k.a. permutation-sensitive node embeddings [65]) to future work.
The application of GNNs to link prediction tasks is made difﬁcult by the fact that, by construction, permutation-equivariant GNNs give the same embeddings Θ•i , Θ•j to any isomorphic nodes i, j in G, as noted by You et al. [82] and Srinivasan and Ribeiro [65]. Isomorphic nodes are nodes that are structurally indistinguishable in G (even when considering node features) except by their (assumed arbitrary) node indices i, j ∈ V . That is, if a graph has isomorphic pairs, permutationequivariant GNN link prediction can fail. The recent link prediction literature has signiﬁcantly relied on isomorphic nodes for theoretical results (e.g., Zhang et al. [86, Theorem 2] uses isomorphic nodes to prove that, uniformly, graphs are likely to have many isomorphic nodes and hence are not amenable to accurate link prediction). However, isomorphic nodes are rare in both real-world graphs (see Figure 3 in the Appendix) and in large random graphs (Proposition 1).
An important open question is whether equivariant GNN would be able to predict links in asymmetric graphs. That is, the concerns of [65, 82] may not be of practical importance. Our work also answers this question: We see that for in-distribution link prediction tasks (where graph test sizes are the same as training sizes), permutation-equivariant GNNs are able to predict links by tapping into the graph asymmetries. However, we show theoretically and empirically that tapping into asymmetries can fail OOD even when it works in-distribution.
Pairwise embeddings. Taking a different route, Srinivasan and Ribeiro [65] provides an existence proof that the link prediction task between i and j can always be performed by a pairwise embedding Θ•ij•(G), i.e., for any pair of nodes i, j in a graph G, there exists a pairwise embedding Θ•ij•(G) and a link function η•• : RFT → [0, 1] such that η••(Θ•ij•) approximates the probability that i and j have a hidden link. In our notation we will denote all pairwise (joint 2-node) embeddings and associated functions with the superscript “••”. Unfortunately, as the test graph grows, we were unable to prove existing pairwise embedding methods [50, 72, 84, 86, 87] are able to perform OOD link prediction tasks. Hence, we propose a novel family of gMPNNs for pairwise embeddings, denoted gMPNNs•• henceforth. The second part of our work considers the OOD generalization capability of these gMPNNs••.
Contributions. In this work we study inductive OOD link prediction tasks for larger test graphs using permutation-equivariant node and pairwise embeddings, Θ• and Θ••, respectively. Our work makes the following contributions:
1. We provide a theoretical framework deﬁning OOD inductive link prediction tasks, where test graphs are signiﬁcantly larger than training graphs.
2. We show that structural node embeddings from message-passing GNNs can fail in OOD link prediction tasks if the test graph (from the same graph family) is signiﬁcantly larger than the training graph. Our work ﬁlls an important gap in the literature, where Bevilacqua et al. [7] studied the OOD capabilities of GNNs for graph classiﬁcation using random graph models. Our work studies the OOD capabilities of GNNs for inductive link prediction in a similar setting.
3. We propose a new family of structural pairwise embeddings, denoted gMPNNs••, that can provably perform the above OOD task.
4. We provide non-asymptotic bounds on the convergence of pairwise gMPNNs embeddings. Extensive empirical experiments using stochastic block models (SBMs [63]) validate our theoretical
2

results. Our work focuses on providing a theoretical understanding of the challenges of OOD link prediction tasks rather than propose real-world link prediction tasks and compare baselines. However, we believe that our work lays the theoretical foundation (and challenges) for future application-focused works.
2 Preliminaries
Given an attributed graph G = (V, E), with node set V = {1, ..., N }, edge set E ⊆ V ×V , adjacency matrix A ∈ {0, 1}N×N , where Aij = 1{(i,j)∈E}, and node features F ∈ RN×F0 , F0 > 0. Let Pπ ∈ BN be a permutation matrix associated with permutation π ∈ SN (where SN is the symmetric group), where BN denotes the Birkhoff polytope of N × N doubly-stochastic matrices. Doublysctochastic matrices are non-negative square matrices whose rows and columns sum to one. The matrix Pπ deﬁnes the action of permutation π on these matrices, e.g., π ◦ A = PπAPπT . We denote a pair of nodes i, j ∈ V as isomorphic in G if exists π ∈ SN such that πi = j, A = PπAPπT , and F = PπF . Node features can be deﬁned by the graph signal f : V → RF0 as a function that maps a node to an F0-dimensional feature in RF0 . Then the signal of the graph F can be represented by a matrix F = [f1, ..., fN ]T ∈ RN×F0 , where fi ∈ RF0 are the features of node i ∈ V .
Random graph model for G. Denote the metric-measure space by (X , d, µ), where X is a set, d is a metric, and µ is a probability Borel measure. A graphon is deﬁned as a mapping W : X × X → [0, 1] [15, 75]. In what follows we deﬁne how the graph G is sampled from the graphon models. The signal deﬁnition follows Maskey et al. [46, Definition 2.3] and the edge samples follow Airoldi et al. [3], Lawrence and Hyvärinen [34].

Deﬁnition 1 (Random graph model). We deﬁne (W, f ) as a

Graph Model


random graph model for G on (X , d, µ) with the graphon W : Graph X × X → [0, 1] and the metric-space signal f : X → RF0 . f ∈ Size L∞(X ) is an essentially bounded measurable function with the

essential supremum norm. We obtain (G, F ) by ﬁrst sampling N

i.i.d. random points X1, ..., XN from X with probability density

µ, as the nodes of G. Then the edge (i, j) between nodes i and j is

sampled with probability W (Xi, Xj), i.e., the adjacency matrix
A = (Ai,j)i,j of G is deﬁned as Ai,j = 1(Zi,j < W (Xi, Wj))
for i, j = 1, ..., N , where {Zi,j}Ni,j=1 are sampled i.i.d. from Uniform(0, 1). The graph signal F = [f1, ..., fN ]T ∈ RN×F0

Figure 1: Templated causal DAG of G. Hidden and observed variables are shaded white and gray,

is deﬁned as fi = f (Xi). We say (G, F ) ∼ (W, f ). Further, respectively.

we restrict our attention to graphons W such that there exists a

constant dmin satisfying the graphon degree dW (x) := X W (x, y)dµ(y) ≥ dmin > 0, ∀x ∈ X .

In an abuse of notation we identify node i ∈ V with the sampled value Xi ∼ µ, ∀i ∈ {1, ..., N }, since generally µ is such that P (Xi = Xj) = 0 almost everywhere for i = j (e.g., µ is uniform). The causal DAG of the data generation process of G is given in Figure 1. Our goal is to produce
predictors that survive the distribution shift implied by a change in the distribution of graph sizes N during test time. Furthermore, we note that all proofs are relegated to the Appendix due to space
constraints.

2.1 Inductive structural node representations with graph message-passing neural networks
Henceforth use the terms node embeddings and node representations interchangeably. Graph messagepassing Neural Network (gMPNN•) is deﬁned by realizing a message-passing Neural Network (MPNN) on a graph. We now restate the Maskey et al. [46, Deﬁnition 2.1] of MPNN.
Deﬁnition 2 (MPNN [46]). Let T ∈ N denote the number of layers. For t = 1, ..., T , let Φ(t) : R2Ft−1 → RHt−1 and Ψ(t) : RFt−1+Ht−1 → RFt be functions, where Ft ∈ N is called the feature dimension of layer t. The corresponding MPNN Θ is deﬁne by the sequence of message functions (Φ(t))Tt=1 and update functions (Ψ(t))Tt=1, i.e. Θ = ((Φ(t))Tt=1, (Ψ(t))Tt=1).

We now introduce the gMPNN• with T message-passing layers. For each node i ∈ V , fi•(t) at layer t ∈ {1, . . . , T } is deﬁned recursively using (a) its own representation at layer t − 1 (fi•(t−1)) and (b) an aggregated representation of its neighbors m(it). Unlike Maskey et al. [46, Deﬁnition 2.2] considering mean aggregation, we consider here the (N-normalized)sum representation as follows:

3

Deﬁnition 3 (gMPNN•). (Adaptation of [46, Deﬁnition 2.2] to N-normalized GNNs) Let (G, F ) be a graph with graph signals as in Deﬁnition 1 and Θ be a MPNN as in Deﬁnition 2. For layer t = 1, ..., T , deﬁne Θ•A(t) as maps from the input graph G and graph signals F (0) = F ∈ RN×F0 to the features in the t-th neural layer by

Θ•A(t) : RN×F0 → RN×Ft , F → F (t) = (fi•(t))Ni=1

where F (t) is deﬁned by the (N -normalized) sum aggregation procedure, ∀i ∈ V , for Θ•A(t),

m(it)

:=

1 N

N

Ai,j Φ(t)(fi•(t−1), fj•(t−1)),

j=1

and fi•(t) := Ψ(t)(fi•(t−1), m(it)).

Given a gMPNN•, Θ•A(T ), with T ≥ 1 layers as in Deﬁnition 3, their outputs are Θ•A(T )(F ) ∈ RN×FT for the (N -normalized) sum aggregation, and are henceforth denoted as node embedding outputs of the gMPNN•. We denote Θ•A(T )(F )i as the node embedding for node i ∈ V .
2.2 Node embeddings with continuous message passing neural networks
Here we adapt the degree-normalized deﬁnition of Maskey et al. [46] on continuous message passing neural networks for structural node embeddings to our continuous integral aggregation (N-normalized GNNs).
Deﬁnition 4 (Continuous message-passing). Given a MPNN Θ as in Deﬁnition 2, the node continuous message passing neural network (cMPNN•) on graphons and metric-space signals f : X → RF0 can be deﬁned by replacing the graph node features and the aggregation scheme in Deﬁnition 3 by the following continuous counterparts. Using a message signal U : X × X → RH , the continuous integral aggregation is deﬁned as MW• (U )(x) = X W (x, y)U (x, y)dµ(y), where W is a graphon.
As deﬁned in Maskey et al. [46, Deﬁnition 2.4], the same MPNN Θ can process metric-space signals instead of graph signals with the continuous aggregations. Instead of using continuous mean aggregation as [46, Deﬁnition 2.4], we are using continuous integral aggregation. Deﬁnition 5 (cMPNN•). (Adaptation of [46, Deﬁnition 2.4] to N-normalized GNNs) Let (W, f ) be a random graph model as in Deﬁnition 1 and Θ be a MPNN as in Deﬁnition 2. For t = 1, ..., T , deﬁne Θ•W(t) as maps from input metric-space signal f •(0) = f : X → RF0 to the features in the t-th layer by
Θ•W(t) : L2(X ) → L2(X ), f • → f •(t),
where f •(t) are deﬁned sequentially through the integral aggregation for

ΘW •(t) : g•(t)(x) : = MW• (Φ(t)(f •(t−1), f •(t−1)))(x)
= W (x, y)Φ(t)(f •(t−1)(x), f •(t−1)(y))dµ(y),
X
f •(t)(x) : = Ψ(t)(f •(t−1)(x), g•(t)(x)).

3 Size-stability of node representation and its drawbacks
We now present our results about convergence of gMPNN• to cMPNN• for test graphs Gte sampled from the graphon random graph model (see Deﬁnition 1), and how it leads to size-stability of gMPNN• for nodes that have the same representation under cMPNNs•. In what follows we will focus on the neighbor-average aggregation procedure (a) of Deﬁnition 3, since this is the more difﬁcult case to prove. Similar results for the (N -normalized) sum aggregation procedure (Deﬁnition 3(b)) are shown in the Appendix due to space constraints. Moreover, common deﬁnitions (e.g., Lipschitz continuous functions) are also relegated to the Appendix to save space.
4

3.1 Convergence of gMPNNs towards cMPNNs as test graph size increase

We now prove that, with high probability, the maximum inﬁnity difference between the gMPNN• and cMPNN• node representations decreases with N te, the size of Gte. The proof of Theorem 1 closely follows the pointwise convergence proof in Maskey et al. [45], adapted to our OOD setting and can be found in the Appendix.

Theorem 1 (OOD convergence without in-distribution convergence). For a random graph model

(W, f ) satisfying Deﬁnition 1, let N tr be a random variable deﬁning the distribution of graph

sizes in training. Deﬁne the test distribution (Gte, F te) ∼ (W, f ) through the causal graph in

Figure 1 as an interventional change to obtain larger test graph sizes where min(supp(N te)) Mtr = max(supp(N tr)) (which means any test graph is much larger than the largest possible training

graph). Let Θ = ((Φ(l))Tl=1, (Ψ(l))Tl=1) be a MPNN as in Deﬁnition 2 with T layers such that Φ(l) : R2Fl−1 → RHl−1 and Ψ(l) : RFl−1+Hl−1 → RFl are learned from the training distribution

and are Lipschitz continuous with Lipschitz constants L(Φl)(Mtr) and L(Ψl)(Mtr) that depend on Mtr.

Let gMPNN• Θ•A(T ) and cMPNN• Θ•W(T ) be as in Deﬁnitions 3 and 5. Let X1te, ..., XNte te and Ate be

as in Deﬁnition 1. Let p ∈ (0,

T l=1

1

).

2(Hl

+1)
√

Then,

if

√

N te

42

≥,

(1)

log (2N te/p) dmin

we have with probability at least 1 −

T l=1

2(Hl

+

1)p,

δA•-W

:=

max
i=1,...,N te

ΘA•(tTe )(F te)i − Θ•W(T )(f )(Xite) ∞ ≤ (C1 + C2 f

∞)

log(2N te/p)

√

,

N te

where the constants and the distribution

C1 of

and C2 are (Gtr, F tr).

deﬁned

in

the

Appendix

and

depend

on

{L(Φl)

(Mtr),

L(Ψl)(Mtr

)}Tl=1

Theorem 1 above shows that as the test graph size N te grows, the node representations from the discrete gMPNNs•learned in the training data converge to the continuous cMPNNs•. Theorem 1’s
OOD statement has profound consequences when it comes to predicting links using the node representations obtained by a gMPNN•. Next, Corollary 1 shows that for any two nodes i, j ∈ V te that are indistinguishable in the cMPNN• (deﬁned as Θ•W(T )(f )(Xite) = Θ•W(T )(f )(Xjte)), they will get increasingly similar representations in the discrete gMPNN• as N te grows.

Corollary 1. Let Θ = ((Φ(l))Tl=1, (Ψ(l))Tl=1), ΘA•(T ), ΘW •(T ), p, (W, f ), (Gtr, F tr), (Gte, F te), N tr,

N te, Ate, and X1te, ..., XNte te be as in Theorem 1. If there exists i, j ∈ V te, i = j, s.t. Θ•W(T )(Xi) =

ΘW •(T )(Xj) and Equation (1) is satisﬁed, then, with C1 and C2 as in Theorem 1, we have that with

probability at least 1 −

T l=1

2(Hl

+

1)p,

Θ•A(tTe )(F te)i − ΘA•(tTe )(F te)j

∞ ≤ (C1 + C2

f

2 ∞)

log(2N te/p)

√

.

N te

Implications of Corollary 1 on Stochastic Block Models (SBMs). In what follows, we will discuss circumstances where two nodes i, j ∈ V get the same cMPNN• representations (i.e., both Θ•W(T )(f )(Xi) = Θ•W(T )(f )(Xj). In what follows we restrict our results to an important family of graphon models: Stochastic Block Models (SBMs) [63], where we also model node attributes. SBMs
were chosen because they can consistently model large graphs generated by any piecewise Lipschitz
graphon model [3]. SBMs are also intuitive models, which makes them useful to illustrate our results.
Deﬁnition 6 (Stochastic Block Model (SBM)). An SBM (W, f ) is a random graph model (Deﬁnition 1) with cluster structures in W and f . Partition the node set into r ≥ 2 disjoint subsets S1, S2, ..., Sr ⊆ V (known as blocks or communities) with an associated r × r symmetric matrix S, where the probability of an edge (i, j), i ∈ Sa and j ∈ Sb is Sab, for a, b ∈ {1, . . . , r}. Let X = [0, 1], and µ be the uniform distribution on [0, 1]. By dividing X = [0, 1] into disjoint convex sets [t0, t1), [t1, t2), . . . , [tr−1, tr], where t0 = 0 and tr = 1, node i belongs to block Sa if Xi ∼ Uniform(0, 1) satisﬁes Xi ∈ [ta−1, ta). The graphon function W is deﬁned as W (Xi, Xj ) = a,b∈{1,...,r} Sab1(Xi ∈ [ta−1, ta))1(Xj ∈ [tb−1, tb)). We take the liberty to also deﬁne node signals in our SBM model, where for B = [B1, ..., Br]T ∈ Rr×F0 the metric-space signal f : X → RF0 is deﬁned as f (x) = a∈{1,...,r} 1(x ∈ [ta−1, ta))Ba.

5

We deﬁne the action of permutation π on B of Deﬁnition 6 as π ◦ B, where (π ◦ B)πa = Ba. Deﬁnition 7 (Isomorphic SBM blocks). For the SBM model (W, f ) in Deﬁnition 6, we say two blocks a, b ∈ {1, . . . , r} are isomorphic if the SBM satisﬁes the following two conditions: (a) ta − ta−1 = tb − tb−1, and (b) for π ∈ Sr, such that πa = b, πb = a and πc = c, ∀c ∈ {1, ..., r}, c = a, b, S = π ◦ S, and B = π ◦ B.
A similar deﬁnition can be obtained for the general graphons in Deﬁnition 1 using the isomorphic graphon deﬁnition of Lovász and Szegedy [40].
Now that we have the deﬁnition for isomorphic blocks in SBM models, we can prove that all nodes in these isomorphic blocks will obtain the same representations under integral aggregation cMPNNs•. Lemma 1. Let Θ = ((Φ(l))Tl=1, (Ψ(l))Tl=1) be a MPNN as in Deﬁnition 2, and Θ•W(T ) as in Deﬁnition 5. For the SBM model (W, f ) in Deﬁnition 6 with N te nodes X1, . . . , XNte . If there exists i, j ∈ V te such that Xite, Xjte are nodes that belong to isomorphic SBM blocks (Deﬁnition 7), then Θ•W(T )(f )(Xite) = Θ•W(T )(f )(Xjte).
Note that even though any two nodes in isomorphic SBM blocks get the same cMPNN• representations per Lemma 1, these nodes are likely not isomorphic in Gte (as shown in Proposition 1 in Appendix) and, hence, they get different gMPNN• representations. However, Corollary 1 shows that these representations become increasingly similar as the test graph size grows (N te 1). We use this observation to understand the ability of gMPNNs• to perform link prediction tasks next.

3.2 The hardness of OOD inductive link prediction using structural node embeddings

The convergence of gMPNNs• to cMPNNs• as the test graph size N te grows (Theorem 1) implies
through Corollary 1 and Lemma 1 that node representations of distinct SBM blocks can become increasingly similar as the test graph size grows (N te 1), even though these nodes are not isomorphic in Gte with high probability (see Proposition 1 in the Appendix).
Deﬁnition 8 (Link prediction function from structural node embeddings). An inductive link prediction function η• : RFT × RFT → [0, 1] takes the gMPNN• node representations of two nodes i, j ∈ V te and predicts the edge probability P (Atiej = 1). We assume η• is Lipschitz continuous with Lipschitz constant Lη• (Mtr) that depends on max(supp(N tr)). In the context of graphon random graph models (Deﬁnition 1), we aim to learn η•(Θ•A(tTe )(F te)i, Θ•A(tTe )(F te)j) ≈ W (i, j). We further assume we predict a link if η•(·, ·) > τ , while no link if η•(·, ·) < τ , for some (arbitrary) threshold τ ∈ [0, 1]
chosen by the user of such system.

The next corollary showcases the difﬁculty in OOD predicting links using structural node representations as N te grows.

Corollary 2. Let Θ = ((Φ(l))Tl=1, (Ψ(l))Tl=1) be the MPNN with T layers and Θ•A(T ), Θ•W(T ) as in Theorem 1. Let η• : RFT × RFT → [0, 1] be as in Deﬁnition 8. Consider the SBM (W, f ) in Deﬁnition 6 with isomorphic blocks (Deﬁnition 7). Let (Gtr, F tr) ∼ (W, f ) and (Gte, F te) ∼

(W, f ) be the training and test graphs with N tr and N te nodes, respectively. Consider any two test

nodes i, j ∈ {1, ..., N te}, i = j, for which we can make a link prediction decision with η• (i.e.,

η•(Θ•A(tTe )(F te)i, Θ•A(T )(F te)j) = τ ). Let Gte be large enough to satisfy both Equation (1) and

√

N te >

2(C1 + C2 f ∞)

,

log(2N te/p) |η•(Θ•A(tTe )(F te)i, Θ•A(tTe )(F te)j ) − τ |/L•η(Mtr)

where p, C1, and C2 are as given in Corollary 1. Then, if i and j belong to isomorphic blocks (i.e.,

Θ•W(T )(f )(Xite) = Θ•W(T )(f )(Xjte)), with probability at least 1 −

T l=1

2(Hl

+

1)p

the

link

prediction

method in Deﬁnition 8 will make the same link prediction regardless of the SBM probability matrix

S (Deﬁnition 6) and whether i and j are in the same block or distinct isomorphic blocks.

Corollary 2 proves that link prediction with structural node embeddings form gMPNNs• is unreliable. That is, for any link prediction method satisfying Deﬁnition 8, as the test graph grows N te 1, the method will increasingly struggle to give different predictions within and across isomorphic SBM blocks, even when these probabilities are arbitrarily different in the underlying graph model. In what follows we show that pairwise embeddings can address this challenge.

6

4 Size-stability of structural pairwise embeddings and its advantages

We have discussed the limitation of gMPNNs• on node representation for link prediction. Now we
claim that a joint continuous message passing graph neural network is capable of link prediction
in graphon random graph models (Deﬁnition 1). We deﬁne the joint continuous message passing graph neural network inspired by the cMPNNs• for node representations (Deﬁnition 5). First, we need to deﬁne the graphon fraction of common neighbors for graphon nodes x and y, cW (x, y) := X W (x, z)W (y, z)dµ(z). We only consider graphons W such that there exists dcmin satisfying cW (x, y) ≥ dcmin > 0, ∀x, y ∈ X in this section. Since we do not have edge feature as in Deﬁnition 1, we deﬁne the metric-space pair-wise signal as f ••(x, y) = 1, ∀x, y ∈ X .
Deﬁnition 9 (cMPNN••). Let (W, f ) be a random graph model as in Deﬁnition 1 and Θ be a MPNN as in Deﬁnition 2. For t = 1, ..., T , deﬁne the continuous (pairwise) cMPNN•• Θ•W•(t) as the mapping that maps input pairwise metric-space signals f ••(0) = f •• to the features in the t-th layer by

Θ•W•(t) : L2(X , X ) → L2(X , X ), where f ••(t) are deﬁned recursively by

f ••(0) → f ••(t),

g••(t)(x, y)

:=

MW••(Φ(t)(f ••(t−1)))(x, y)

=

1 2

( W (y, z) Φ(t)(f ••(t−1)(x, y), f ••(t−1)(x, z)) X cW (x, y)

+ W (x, z) Φ(t)(f ••(t−1)(x, y), f ••(t−1)(y, z)))dµ(z), cW (x, y)

f ••(t)(x, y) := Ψ(t)(f ••(t−1)(x, y), g••(t)(x, y)).

The intuition of the aggregation function is that two edges with one same node is considered neighbors in a higher-order graph [51], and to go from (x, y) to (x, z), we need to transition from y to z, which has probability W (y, z). The same holds for going from (x, y) to (y, z).
Lemma 2. If Φ(x, y) = y and Ψ(x, y) = x/y, then f ••(t)(x, y) = W (x, y), ∀x, y ∈ X is a stationary point in the cMPNN••, i.e. if f ••(t−1)(x, y) = W (x, y), then f ••(t)(x, y) = W (x, y), ∀x, y ∈ X.

We deﬁne the corresponding gMPNN•• as follows. First we deﬁne the fraction of common neighbors

between

nodes

i

and

j

as

cAi,j

=

1 N

N z=1

Ai,z

·

Aj,z .

If

two

nodes

do

not

have

common

neighbors,

tghreanphwGe ,saent dcAFi,•j•==N1(f

to
••

avoid computation error. Further, we deﬁne i,j)i,j∈V as the pair-wise graph signals.

f

•• i,j

=

1 ∀i, j

∈

V

for any

Deﬁnition 10 (gMPNN••). Let (G, F ) be a graph with graph signals as in Deﬁnition 1 and Θ be a

MPNN as in Deﬁnition 2. For t = 1, ..., T layers we deﬁne the gMPNN•• Θ•A•(t) as the mapping that maps input pairwise graph signals F ••(0) = F •• to the features in the t − th layer by

Θ•A•(t) : RN2×F0 → RN2×Ft , F ••(0) → F ••(t) = (f ••(i,tj))Ni,j=1

where f ••(t) are deﬁned recursively by the following function,

m••(i,tj)

:=

1 2N

N z=1

Aj,z cAi,j

Φ(t)(f

•• (t−1) i,j

,

f

••(i,tz−1))

+

Ai,z cAi,j

Φ(t)

(f

••

(i,tj−1),

f

••

(t−1) j,z

),

f

•• (t) i,j

:=

Ψ(t)(f ••(i,tj−1),

m••(i,tj)).

Next, Theorem 2 shows that these discrete joint representations gMPNN•• converge to the continuous pairwise representation cMPNN•• under the causal DAG of Figure 1.
Theorem 2 (OOD convergence without in-distribution convergence). For a random graph model (W, f ) satisfying Deﬁnition 1, let N tr be a random variable deﬁning the distribution of graph sizes in training. Deﬁne the test distribution (Gte, F te) ∼ (W, f ) through the causal graph in Figure 1 as an interventional change to obtain larger test graph sizes where min(supp(N te)) Mtr = max(supp(N tr)) (which means any test graph is much larger than the largest possible training

7

graph). Let Θ = ((Φ(l))Tl=1, (Ψ(l))Tl=1) be a MPNN as in Deﬁnition 2 with T layers such that Φ(l) and Ψ(l) that are learned from the training data and are Lipschitz continuous with Lipschitz constants

L(Φl)(Mtr) and L(Ψl)(Mtr). Let gMPNN•• Θ•W•(T ) and cMPNN•• Θ•W•(T ) be as in Deﬁnitions 9 and 10.

For a random graph model (W, f ) as in Deﬁnition 1 with√dcmin > 0. Let√X1te, ..., XNte te and Ate be as

in Deﬁnition 1. Let p ∈ (0,

T l=1

1 2(Hl

+1)

).

Then,

if

√

N te

log (2(N te)2/p)

≥

, 4 2
dcmin

we

have

with

probability

at least 1 −

T l=1

2(Hl

+

1)p,

δA•-•W

=
i,j

max
=1,...,N

te

Θ•A•(T )(F ••)i,j −Θ•W•(T )(f ••)(Xite, Xjte) ∞ ≤ (C3 +C4

f ••

∞)

log(2(N te)2/p)

√

,

N te

where the constants C3 and C4 are deﬁned in the Appendix and depend on {L(Φl)(Mtr), L(Ψl)(Mtr)}Tl=1 and the distribution of (Gtr, F tr).

Hence, as the test graph size N te gets larger w.r.t. N tr (that is, as we intervene on the causal DAG of Figure 1 to change the support of the distribution of N in order to obtain larger test graphs), the link predictor learned in the training data using gMPNN•• will converge to a continuous method (cMPNN••) that can predict links in OOD tasks (i.e., W (Xite, Xjte) is a stationary solution of cMPNN•• per Lemma 2). This convergence is also observed in our empirical results.

5 Further Related Work
In what follows we describe works related to learning transferability in GNNs. The concept of transferability of GNN is introduced by Levie et al. [35], Ruiz et al. [58], which state that if two graphs represent same phenomena (e.g., are sampled from the same distribution), then a transferable GNN has approximately the same predictive performance on both graphs. This is closely related to in-distribution generalization capabilities of GNNs to unseen test data, i.e., generalization error when train and test data come from the same distribution. Existing works [26, 44, 58, 59] prove the transferability for spectral-based GCNs under graphon models, and Maskey et al. [46] extends these results to more general message passing GNNs. The GNN smoothness conditions needed to prove uniform convergence of node-embedding equivariant GNNs in Maskey et al. [46] means their GNNs would be unable to perform in-distribution link prediction in some tasks (such as the graphs in Deﬁnition 7). However, in practice, we observe (Section 6) that GNNs are capable of performing these in-distribution link prediction tasks. Our results are also based on general message passing GNNs. Our goal (OOD link prediction) is, however, signiﬁcantly different than these prior works, which focus on in-distribution graph and node classiﬁcation. The link prediction challenge for node-embedding equivariant GNNs is either in symmetric graphs (Srinivasan and Ribeiro [65]) or OOD (this work). Theorem 1 says they vanish as the test graphs grow larger, but Theorem 2 says that our pairwise equivariant representation is capable of performing these OOD link prediction task. Related works relating to the representation power, higher order structural and positional link prediction methods (not already covered in our introduction) can be found in Appendix A due to space constraints.

6 Empirical Evaluation

In what follows we empirically validate our theoretical results in two parts. We implement all our models in Pytorch Geometric [19] and make it available1. Due to space constraints we relegate a detailed description of our experiments to the Appendix.
Convergence and stability. First we will empirically validate Theorems 1 and 2 and Corollary 1. Consider an SBM (Deﬁnition 6) with three blocks (r = 3) and Sa,a = 0.55, a = 1, 2, 3, S1,2 = S2,1 = 0.05, S1,3 = S3,1 = 0.02. Note that one and three are isomorphic blocks (see Deﬁnition 7). We use a randomly initialized GraphSAGE [24] GNN model for node embedding, and test both the Φ and Ψ of Lemma 2, and a scenario where Ψ is a randomly-initialized MLP for pairwise embeddings.

Figures 2(a-c) show log-log plots of the convergence of gMPNNs to their continuous cMPNN

counterparts as the test graph size N te increases. The empirical approximation errors δA•-W (Theo-

rem 1) (Figure n = 5, ..., 13.

2(a)) The

and δA••-W (Theorem 2) are shown as empirical results show agreement

a function of the with the theory

test graph size N te = since δA•-W and δA••-W

2n, are

1 https://github.com/yangzez/OOD- Link- Prediction- Generalization- MPNN

8

log10( A W) log10( A W) log10( A W)

(a)
1.0

1.2

1.4

slope=-1/2

1.6

1.8

2.0

2.2
1.5 2.0 2.5 3.0 3.5 4.0
log10(N te)

(b)
1.0 0.8 0.6 0.4 0.2 0.0
slope=-1/2
0.2 0.4
1.5 2.0 2.5 3.0 3.5 4.0
log10(N te)

(c)
0.6
0.8
1.0
1.2
1.4 slope=-1/2
1.6
1.8
2.0 1.5 2.0 2.5 3.0 3.5 4.0
log10(N te)

1000 800 600 400 200
0 1.0

(d)
Nte=200 Nte=2000 Nte=20000

0.5

0.0
iso

0.5 1e 12.0

(e)

600

Nte=200 Nte=2000

500

Nte=20000

400

300

200

100

0 6.0

5.5

5.0 4.5 4.0
non iso

3.51e 23.0

FGiNguNrea2s:aEfxupnecrtiiomnenotfaNl ateg;re(bem) sehnotwwsitδhA••-tWhe(oTrhye:o(rae)msh2o)wws iδthA•-Wthe(TghMeoPrNemN•1•)

of of

a GraphSAGE Lemma 2 as a

function of N te; (c) replicates (b) with Ψ as a randomly-initialized neural network. Results shows

close agreement with Theorems 1 and 2 that predicts slope ≈ −1/2 in log-log scale for large N te;

(d) shows stable node representations between isomorphic SBM blocks, while (e) shows constant

difference in node representations between non-isomorphic SBM blocks, which validate Corollary 1.

√

√

bounded above by O( log N te/ N te), which is approximated by the slope −1/2 in a log-log

plot. Figures 2(d-e) show histograms of the difference between gMPNN• embeddings of different

nodes in Gte. Let ∆•i,j := ΘA•(T )(F )i − Θ•A(T )(F )j for i, j ∈ V te, ∆•i,j ∈ RFT and further deﬁne ∆•iso (resp. non-iso)i,j := (∆•i,j )arg maxk |(∆•i,j)k|, where k ∈ {1, . . . , FT } is the dimension of the embed-
ding. We use subscript iso (resp. non-iso) when i, j ∈ V te are in isomorphic (resp. non-isomorphic)

SBM blocks (Deﬁnition 7). As N te increases, Figure 2(d) shows that embeddings between isomorphic

blocks converge, validating Corollary 1, while Figure 2(e) shows that non-isomorphic blocks do not.

Link prediction performance evaluation with SBMs (in-distribution and OOD). In what follows
we introduce empirical results using a SBM similar in the previous setting. Details can be found in Appendix B.3. We start by sampling the training graph (Gtr, F tr) with N tr = 103 nodes. We randomly hide 10% of Etr from the original graph Gtr for link prediction purpose since the goal of
link prediction is to predict possible missing links that is not observed in the original graph. We call these edges Ehid-tr. Then we split Ehid-tr into positive train (80%) and validation (10%) edges (we reserve 10% of Ehid-tr for the transductive test scenario), and uniformly sample the same number of across-block non-edges as negative train and validation edges. The embedding method gMPNN• (resp. gMPNN••) along with link predictor η• (resp. η••) are trained in an end-to-end manner for
predicting positive and negative edges in training using cross-entropy loss. Our experiments consider
three scenarios (in all scenarios we use the same number of negative test edges as positive test edges, sampled from non-edges in Gte with endpoints in different isomorphic blocks): (i) (In-distribution) transductive scenario where Gte = Gtr, where positive test edges are the 10% reserved in Ehid-tr not used in training or validation; (ii) In-distribution inductive scenario where Gte is sampled from the same SBM with N te = N tr, where we also hide 10% of the edges and sample 0.1|Ehid-tr| positive test edges from Ehid-te (for fair comparison across all scenarios); (c) OOD inductive scenario where Gte is sampled from the same SBM with N te = 10 × N tr, where we also hide 10% of the edges and sample 0.1|Ehid-tr| positive test edges from Ehid-te(for fair comparison across all scenarios).

For structural node embeddings we consider GraphSAGE [24], GCN [28] (without positional features), GAT [70] and GIN [78] as the representatives of gMPNN• models. The link predictor η• is as feedfoward network (with 3 hidden layers and 10 neurons each) that receives the two node embeddings as input, and has link prediction threshold τ = 0.5 (see Deﬁnition 8 for details).
For structural pairwise embeddings we choose our proposed gMPNN•• method of Deﬁnition 10, since we can prove that our approach is theoretically sound in Lemma 2. We test gMPNN•• in two versions: The Φ and Ψ functions in Lemma 2 (denoted ﬁxed Ψ) and a feedforward neural network for Ψ with 2 hidden layers and 5 neurons each (denoted learn Ψ). The link predictor η•• is the same as η• except it just takes one pairwise embedding as input, rather than two node embeddings.

Table 1 presents our empirical results. The oracle predictor knows the graphon values W (Xite, Xjte). Our evaluation metrics include the Matthews correlation coefﬁcient (mcc) [47], balanced accuracy, and Hits@K for K = 10, 50, 100 that counts the ratio of positive edges ranked at the k-th place or above against all negative edges. Note that gMPNN• structural node representations can very accurately predict links in the transductive tasks, and still performs reasonably well in inductive in-distribution tasks. However, as expected from Corollary 2, this performance suffers signiﬁcantly as N te becomes 10× larger than N tr. Now all gMPNN• methods produce predictors that are no better than a random guess over all metrics (e.g., see OOD mcc and accuracy (in red)). In contrast, the gMPNN•• is able to consistently offer good performance on both in-distribution and OOD tasks.

9

Table 1: Test performance over 50 runs of node and pairwise gMPNNs for in-distribution and OOD link prediction over SBM graphs. Methods marked with ∗ indicate best result out of distinct conﬁgurations detailed in the Appendix.
Training graph size N tr = 103

Tasks

Model

Hit@10(%) Hit@50(%) Hit@100(%) mcc.(%) balanced acc.(%)

Transductive

GraphSAGE* GCN* GAT* GIN*
gMPNN•• (ﬁxed Ψ) gMPNN•• (learn Ψ)

95.55( 0.52) 93.15(14.57) 93.77(13.03) 95.77( 0.59)
93.76( 0.55) 96.71( 0.32)

95.93( 0.73) 93.99(13.08) 94.01(13.02) 96.09( 0.58)
94.17( 0.51) 96.88( 0.31)

96.14( 0.74) 94.35(12.72) 94.14(13.03) 96.28( 0.59)
94.51( 0.49) 97.00( 0.30)

95.42( 0.37) 92.41(14.72) 90.94(16.09) 95.48( 0.41)
93.64( 0.53) 94.23( 0.55)

97.66( 0.19) 95.97( 8.24) 95.26( 8.38) 97.69( 0.22)
96.72( 0.28) 97.03( 0.29)

In-distribution link prediction

Oracle

96.92( 0.36) 96.92( 0.36) 96.92( 0.36) 93.74( 0.42)

96.77( 0.22)

GraphSAGE* GCN* GAT* GIN*
gMPNN•• (ﬁxed Ψ) gMPNN•• (learn Ψ)

47.38(39.08) 66.29(37.67) 40.05(39.05) 39.33(34.62)
93.85( 0.49) 96.71( 0.30)

52.13(38.87) 68.52(35.87) 41.34(39.39) 42.93(33.86)
94.23( 0.51) 96.91( 0.28)

54.94(37.83) 69.92(35.12) 41.96(39.54) 43.90(33.72)
94.55( 0.49) 97.02( 0.27)

19.34(43.19) 31.76(35.12) 19.44(35.22) 18.59(39.43)
93.74( 0.48) 94.23( 0.59)

61.46(20.17) 67.21(22.75) 59.52(16.94) 59.79(18.24)
96.77( 0.25) 97.03( 0.31)

Oracle

97.01( 0.31) 97.01( 0.31) 97.01( 0.31) 93.87( 0.39)

96.84( 0.20)

GraphSAGE* GCN* GAT* GIN*
gMPNN•• (ﬁxed Ψ) gMPNN•• (learn Ψ)

9.97(19.47) 39.29(31.33) 27.31(26.93) 0.00( 0.00)
96.74( 0.07) 96.97( 0.04)

11.73(21.80) 42.15(30.81) 28.13(26.78) 0.00( 0.00)
96.93( 0.04) 97.02( 0.04)

12.98(23.70) 44.19(30.97) 28.72(26.93)
0.00( 0.00)
97.01( 0.04) 97.08( 0.04)

-6.56( 5.12) -4.88(14.84) -2.00( 8.96) -3.93( 5.12)
93.76( 0.05) 93.94( 0.67)

49.32( 0.60) 50.33( 6.72) 50.20( 3.37) 49.59( 0.57)
96.78( 0.03) 96.88( 0.35)

Oracle

96.96( 0.03) 96.96( 0.03) 96.96( 0.03) 93.77( 0.04)

96.79( 0.02)

Inductive N te = 104 Inductive N te = N tr

OOD link prediction

Link prediction performance evaluation with ogbl-ddi (in-distribution and OOD). In what fol-
lows we introduce empirical results using the ogbl-ddi dataset, which represents a drug-drug interaction network. For the purpose of performing OOD tasks, we start by sampling 10% of the nodes (427
nodes) and its induced subgraph to be the training graph. Further experimental details can be found in Appendix B.4. The in-distribution inductive scenario has Gte constructed as an induced subgraph with N te = N tr nodes from the remaining ogbl-ddi graph. Our OOD inductive scenario has Gte as the induced subgraph without the training nodes (N te = 3840 nodes). The test edges are obtained by
applying the original edge split on the newly induced test subgraph, where we further down-sample
to the same amount of test edges as in our in-distribution scenarios for fair comparison across all
scenarios. Table 3 in the Appendix presents our empirical results on the ogbl-ddi link prediction task. All gMPNN• methods performs worse in inductive settings than transductive settings, and suffer much worse performance in OOD transductive setting except GCNs. In contrast, the gMPNN•• is
able to consistently offer good performance on both in-distribution and OOD tasks, showing that the
theoretical results are not limited to SBM models.

7 Conclusions
This work studied and provided the ﬁrst theoretical framework for the task of out-of-distribution (OOD) link prediction, where test graphs are larger than training graphs. Using non-asymptotic bounds, this work showed that OOD link prediction methods using structural node embeddings given by message-passing GNNs converge to link predictors that may perform no better than random guesses. The work also proposed a theoretically-sound structural pairwise embedding with a messagepassing algorithm which is able to perform our OOD link prediction task by being approximately invariant to interventions on test graph sizes, as the discrete joint embedding converges to the continuous one. This means that as graph sizes grow in test (OOD), it is still possible to ﬁnd neural networks parameters that allows our joint representation to converge to the true link probability. We show that the same is not guaranteed for node-embedding equivariant message-passing GNNs. Extensive empirical evaluation showed agreement with these theoretical results. We do not foresee adverse social impacts for this theoretical work, but it does raise awareness of the shortcomings of node-embedding equivariant massage-passing GNNs for link prediction tasks in applications such as recommender systems.

10

References
[1] Lada A Adamic and Eytan Adar. Friends and neighbors on the web. Social networks, 25(3): 211–230, 2003.
[2] Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josifovski, and Alexander J Smola. Distributed large-scale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web, pages 37–48, 2013.
[3] Edo M Airoldi, Thiago B Costa, and Stanley H Chan. Stochastic blockmodel approximation of a graphon: Theory and consistent estimation. Advances in Neural Information Processing Systems, 26, 2013.
[4] Gabor Angeli and Christopher D Manning. Philosophers are mortal: Inferring the truth of unseen facts. In Proceedings of the seventeenth conference on computational natural language learning, pages 133–142, 2013.
[5] Fabian Ball and Andreas Geyer-Schulz. How symmetric are real-world graphs? a large-scale study. Symmetry, 10(1):29, 2018.
[6] Robert M Bell and Yehuda Koren. Lessons from the netﬂix prize challenge. Acm Sigkdd Explorations Newsletter, 9(2):75–79, 2007.
[7] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classiﬁcation extrapolations. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 837–851. PMLR, 2021.
[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34 (4):18–42, 2017.
[9] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In International Conference on Learning Representations, 2014.
[10] Yongqiang Chen, Yonggang Zhang, Han Yang, Kaili Ma, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Invariance principle meets out-of-distribution generalization on graphs. CoRR, abs/2202.05441, 2022.
[11] Abhinandan S Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. Google news personalization: scalable online collaborative ﬁltering. In Proceedings of the 16th international conference on World Wide Web, pages 271–280, 2007.
[12] Luc De Raedt. Logical and relational learning. Springer Science & Business Media, 2008.
[13] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. Advances in neural information processing systems, 29, 2016.
[14] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018.
[15] Persi Diaconis and Svante Janson. Graph limits and exchangeable random graphs. arXiv preprint arXiv:0712.2749, 2007.
[16] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. AAAI Workshop on Deep Learning on Graphs: Methods and Applications, 2021.
[17] Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Graph neural networks with learnable structural and positional representations. In International Conference on Learning Representations, 2022.
11

[18] Paul Erdos and Alfréd Rényi. Asymmetric graphs. Acta Math. Acad. Sci. Hungar, 14(295-315): 3, 1963.
[19] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.
[20] Lise Getoor and Christopher P Diehl. Link mining: a survey. Acm Sigkdd Explorations Newsletter, 7(2):3–12, 2005.
[21] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1263–1272. PMLR, 2017.
[22] Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In Proceedings. 2005 IEEE international joint conference on neural networks, volume 2, pages 729–734, 2005.
[23] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proc. of KDD, pages 855–864. ACM, 2016.
[24] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.
[25] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020.
[26] Nicolas Keriven, Alberto Bietti, and Samuel Vaiter. Convergence and stability of graph convolutional networks on large random graphs. Advances in Neural Information Processing Systems, 33:21512–21523, 2020.
[27] Jeong Han Kim, Benny Sudakov, and Van H Vu. On the asymmetry of random regular graphs and random graphs. Random Structures & Algorithms, 21(3-4):216–224, 2002.
[28] Thomas Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In International Conference on Learning Representations, 2017.
[29] Thomas N Kipf and Max Welling. Variational graph auto-encoders. NIPS Workshop on Bayesian Deep Learning, 2016.
[30] Daphne Koller, Nir Friedman, Sašo Džeroski, Charles Sutton, Andrew McCallum, Avi Pfeffer, Pieter Abbeel, Ming-Fai Wong, Chris Meek, Jennifer Neville, et al. Introduction to statistical relational learning. MIT press, 2007.
[31] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30–37, 2009.
[32] Yehuda Koren, Steffen Rendle, and Robert Bell. Advances in collaborative ﬁltering. Recommender systems handbook, pages 91–142, 2022.
[33] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Létourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. Advances in Neural Information Processing Systems, 34, 2021.
[34] Neil Lawrence and Aapo Hyvärinen. Probabilistic non-linear principal component analysis with gaussian process latent variable models. Journal of machine learning research, 6(11), 2005.
[35] Ron Levie, Wei Huang, Lorenzo Bucci, Michael Bronstein, and Gitta Kutyniok. Transferability of spectral graph convolutional neural networks. Journal of Machine Learning Research, 22 (272):1–59, 2021.
[36] Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably more powerful neural networks for graph representation learning. Advances in Neural Information Processing Systems, 33:4465–4478, 2020.
12

[37] David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019–1031, 2007.
[38] Derek Lim, Joshua Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, and Stefanie Jegelka. Sign and basis invariant networks for spectral graph representation learning. arXiv preprint arXiv:2202.13013, 2022.
[39] Greg Linden, Brent Smith, and Jeremy York. Amazon. com recommendations: Item-to-item collaborative ﬁltering. IEEE Internet computing, 7(1):76–80, 2003.
[40] László Lovász and Balázs Szegedy. The automorphism group of a graphon. Journal of Algebra, 421:136–166, 2015.
[41] Tomasz Luczak, Abram Magner, and Wojciech Szpankowski. Asymmetry and structural information in preferential attachment graphs. Random Structures & Algorithms, 55(3):696– 718, 2019.
[42] Ben D MacArthur, Rubén J Sánchez-García, and James W Anderson. Symmetry in complex networks. Discrete Applied Mathematics, 156(18):3525–3531, 2008.
[43] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. In Advances in Neural Information Processing Systems, pages 2156–2167, 2019.
[44] Sohir Maskey, Ron Levie, and Gitta Kutyniok. Transferability of graph neural networks: an extended graphon approach. arXiv preprint arXiv:2109.10096, 2021.
[45] Sohir Maskey, Yunseok Lee, Ron Levie, and Gitta Kutyniok. Convergence and transferability of message passing graph neural networks. (under submission) currently available as arXiv:2202.00645v3, 2022.
[46] Sohir Maskey, Ron Levie, Yunseok Lee, and Gitta Kutyniok. Generalization analysis of message passing neural networks on large random graphs. Advances in neural information processing systems, 2022.
[47] Brian W Matthews. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442–451, 1975.
[48] Brendan McKay. Practical graph isomorphism. Dagstuhl Reports, Vol. 5, Issue 12 ISSN 2192-5283, page 11, 2016.
[49] Andriy Mnih and Russ R Salakhutdinov. Probabilistic matrix factorization. Advances in neural information processing systems, 20, 2007.
[50] Federico Monti, Oleksandr Shchur, Aleksandar Bojchevski, Or Litany, Stephan Günnemann, and Michael M Bronstein. Dual-primal graph convolutional networks. arXiv preprint arXiv:1806.00770, 2018.
[51] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 33, pages 4602–4609, 2019.
[52] Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils M Kriege, Martin Grohe, Matthias Fey, and Karsten Borgwardt. Weisfeiler and leman go machine learning: The story so far. arXiv preprint arXiv:2112.09992, 2021.
[53] Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling for graph representations. In Proceedings of the 36th International Conference on Machine Learning, 2019.
[54] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11–33, 2015.
13

[55] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchéBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.
[56] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710, 2014.
[57] Yanjun Qi, Ziv Bar-Joseph, and Judith Klein-Seetharaman. Evaluation of different biological data and computational classiﬁcation methods for use in protein interaction prediction. Proteins: Structure, Function, and Bioinformatics, 63(3):490–500, 2006.
[58] Luana Ruiz, Luiz Chamon, and Alejandro Ribeiro. Graphon neural networks and the transferability of graph neural networks. Advances in Neural Information Processing Systems, 33: 1702–1712, 2020.
[59] Luana Ruiz, Fernando Gama, and Alejandro Ribeiro. Graph neural networks: architectures, stability, and transferability. Proceedings of the IEEE, 109(5):660–682, 2021.
[60] Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. Advances in neural information processing systems, 30, 2017.
[61] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61–80, 2008.
[62] Brent Smith and Greg Linden. Two decades of recommender systems at amazon. com. Ieee internet computing, 21(3):12–18, 2017.
[63] Tom AB Snijders and Krzysztof Nowicki. Estimation and prediction for stochastic blockmodels for graphs with latent block structure. Journal of classiﬁcation, 14(1):75–100, 1997.
[64] Alessandro Sperduti and Antonina Starita. Supervised neural networks for the classiﬁcation of structures. IEEE Transactions on Neural Networks, 8(3):714–735, 1997.
[65] Balasubramaniam Srinivasan and Bruno Ribeiro. On the equivalence between positional node embeddings and structural graph representations. ICLR, 2020.
[66] Ben Taskar, Ming-Fai Wong, Pieter Abbeel, and Daphne Koller. Link prediction in relational data. Advances in neural information processing systems, 16, 2003.
[67] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In International conference on machine learning, pages 2071–2080. PMLR, 2016.
[68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
[69] Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
[70] Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. ICLR, 2018.
[71] Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equivariant and stable positional encoding for more powerful graph neural networks. In International Conference on Learning Representations, 2022.
14

[72] Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. Inductive representation learning in temporal networks via causal anonymous walks. In International Conference on Learning Representations, 2021.
[73] Boris Weisfeiler and AA Lehman. A reduction of a graph to a canonical form and an algebra arising during this reduction. Nauchno-Technicheskaya Informatsia, 2(9):12–16, 1968.
[74] David S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Jason R Grant, Tanvir Sajed, Daniel Johnson, Carin Li, Zinat Sayeeda, et al. Drugbank 5.0: a major update to the drugbank database for 2018. Nucleic acids research, 46(D1):D1074–D1082, 2018.
[75] Patrick J Wolfe and Soﬁa C Olhede. Nonparametric graphon estimation. arXiv preprint arXiv:1309.5936, 2013.
[76] Qitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. Handling distribution shifts on graphs: An invariance perspective. In International Conference on Learning Representations, 2022.
[77] Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. In International Conference on Learning Representations, 2022.
[78] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019.
[79] Keyulu Xu, Mozhi Zhang, Jingling Li, Simon Shaolei Du, Ken-Ichi Kawarabayashi, and Stefanie Jegelka. How neural networks extrapolate: From feedforward to graph neural networks. In International Conference on Learning Representations, 2021.
[80] Keyulu Xu, Mozhi Zhang, Jingling Li, Simon Shaolei Du, Ken-Ichi Kawarabayashi, and Stefanie Jegelka. How neural networks extrapolate: From feedforward to graph neural networks. In International Conference on Learning Representations, 2021.
[81] Gilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, and Haggai Maron. From local structures to size generalization in graph neural networks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11975–11986. PMLR, 18–24 Jul 2021.
[82] Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In International Conference on Machine Learning, pages 7134–7143. PMLR, 2019.
[83] Jiaxuan You, Jonathan Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware graph neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2021.
[84] Muhan Zhang and Yixin Chen. Weisfeiler-lehman neural machine for link prediction. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’17, page 575–583, 2017.
[85] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances in neural information processing systems, 31, 2018.
[86] Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Labeling trick: A theory of using graph neural networks for multi-node representation learning. Advances in Neural Information Processing Systems, 34, 2021.
[87] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford networks: A general graph neural network framework for link prediction. Advances in Neural Information Processing Systems, 34, 2021.
15

Appendix of “OOD Link Prediction Generalization Capabilities of Message-Passing GNNs in Larger Test Graphs”
In Appendix A, we introduce more related work that has not been discussed in the main paper. In Appendix B, we provide more details in experiments set up and model training. In Appendix C, we introduce notations and deﬁnitions that we will use throughout the rest of the appendix. In Appendix D, we show large random and real world graphs have few isomorphic nodes. In Appendix E, we prove the convergence results (Theorem 1) for gMPNN• when different aggregation functions are used. In Appendix F, we prove the results for hardness of link prediction for gMPNN•. Finally, we prove the convergence results for gMPNN•• and cMPNN•• (Theorem 2) in Appendix G.
A Further Related Work
Representation power of GNNs. The representation power of GNNs is widely studied in recent years. [51, 78] ﬁrst show that gMPNN is no more powerful than 1-WL test [73]. Many works have been proposed [43, 51–53] to increase the representation power of GNNs for graph representation, but little has studied on representation power for node and link prediction.
Structural link prediction. Existing link prediction methods assume that, with powerful enough node representations, combining them can guarantee powerful link representations [23, 29]. However, Hu et al. [25] empirically shows that these approaches perform worse than simple heuristic approaches such as Common Neighbor and Adamic-Adar [1, 37]. Theoretically, Srinivasan and Ribeiro [65] was the ﬁrst work to formally analyze the difference between structural node and link representations, and show that even most-expressive structural node representations are not able to perform link prediction tasks in graphs with high degree of symmetry. In order to remedy this, the state-of-the-art link prediction methods like SEAL [85] use GNNs but transform the task into a graph classiﬁcation task (the link is an attribute of an induced subgraph around the two target end nodes), where each node in the subgraph are labeled according to their distances to the pair of target end nodes. Zhang et al. [86] uniﬁes such approaches [36, 83, 85] through a method they call “labeling trick”, which they show is able to learn structural link representations with a node-most expressive GNN.
Ability of GNNs to emulate graph algorithms as graph sizes increase. Recently, Xu et al. [79] shows that GNNs can extrapolate in algorithmic-related tasks as the graph size grows, if the GNN uses max as an aggregator (rather than mean and sum we considered in this paper). Unfortunately, our Deﬁnition 3 of gMPNN• does not allow max aggregators, in part because it is unclear how one could reach stability using the max aggregator. Fortunately, while we could not obtain theoretical results using the max aggregator, we can test it empirically. Table 2 reproduces all our empirical results using the max aggregator (on GraphSAGE and GAT, since these are the only GNNs designed for the max aggregator). Our experiments show that the max aggregator, just like the mean and sum aggregators, shows poor OOD performance as test graph sizes increase. Other works Bevilacqua et al. [7], Yehudai et al. [81] also talk about graph extrapolation as size grows but focus on graph classiﬁcation. Chen et al. [10], Wu et al. [76, 77] also explore environment-invarian GNN representations for graph classiﬁcation or node classiﬁcation tasks. These works differ in that they focus on node classiﬁcation and graph classiﬁcation. As Srinivasan and Ribeiro [65] shows, link prediction tasks are signiﬁcantly different from graph and node classiﬁcation. Moreover, whether or not one can prove that the max aggregator is or is not able to perform our OOD task is left as future work.
Positional node embeddings for link prediction. Another way to perform link prediction tasks is to use positional node embeddings (PE), which preserves relative positions of the nodes in a graph. The original link predictor in Kipf and Welling [28] uses positional embedding as node attributes for this type of task. However, such approaches can lose the desired permutation equivariance property in graph models. Traditional PE methods include DeepWalk [56] and matrix factorization [2, 49]. You et al. [82] proposes position-aware GNN that only aggregates message from randomly selected anchor nodes, which has poor generalization ability on inductive tasks. Srinivasan and Ribeiro [65] proves that using set representations of PE embeddings over all permutations of the graph input and all random decisions made by the embedding algorithm (e.g., the set of all eigenvectors of randomly permuted graph Laplacian matrices and random eigenvectors due to geometric multiplicity of eigenvalues) can achieve the desired permutation equivariance for link prediction. Dwivedi and
16

Bresson [16], Kreuzer et al. [33] propose PE that randomly ﬂips of the sign of eigenvectors to alleviate sign ambiguity and pass it to a transformers architecture [68]. Lim et al. [38] proposes a representation that is invariant to the elements of the set described by Srinivasan and Ribeiro [65] in order to achieve equivariant representations for spectral node embeddings. Wang et al. [71] proposes a provable solution for using PEs to learn equivariant and stable representation using separate channels in GNN layers. Dwivedi et al. [17] turns to the idea of learning PE that can be combined with structural representations, and design architecture to decouple structural and positional representations in order to improve both representations.

B Further Experiment details

In this section we present the details of the experimental section, discussing implementation details. Training was performed on NVIDIA Telsa P100, GeForce RTX 2080 Ti, and TITAN V GPUs.

B.1 Model implementation
All neural network approaches, including the models proposed in this paper, are implemented in PyTorch [55] and Pytorch Geometric [19] (available respectively under the BSD and MIT license).
Our GIN [78], GCN [28], GAT [70] and GraphSAGE [24] implementations are based on their Pytorch Geometric implementations. We also consider max aggregation as proposed by Xu et al. [80] for extrapolations although it does not ﬁt our theoretical framework.
We use the Adam optimizer to optimize all the neural network models. We use the neural network weights that achieve best validation-set performance for prediction.

B.2 Empirical validation for convergence and Stability

Consider an SBM (Deﬁnition 6) with three blocks (r = 3) and Sa,a = 0.55, a = 1, 2, 3, S1,2 =

S2,1 = 0.05, S1,3 = S3,1 = 0.02. The probability a node belongs to block one or three is 0.45, while for block two it is 0.1. Note that one and three are isomorphic blocks (see Deﬁnition 7). Since

our results are valid for any gMPNN functions Θ, for our ﬁrst experiment with node embeddings we

use a randomly initialized GraphSAGE [24] GNN model, where following standard GNN procedures

we initialize

node

features as

size-normalized degrees (where di

=

1 N

j=1,...,N Ai,j ). For the

experiment with pairwise embeddings, we test both the Φ and Ψ of Lemma 2, and a scenario where Ψ

is a randomly-initialized feedforward neural network. Later in this section we show how to efﬁciently compute the exact cMPNN• and cMPNN•• embeddings of our GraphSAGE and gMPNN•• models.

The validation procedure follows Maskey et al. [45]. We use SBM graphs as examples. Consider an 0.55 0.05 0.02
SBM (Deﬁnition 6) with three blocks (r = 3) and S = 0.05 0.55 0.05 . The probability a node 0.02 0.05 0.55
belongs to block one or three is 0.45, while for block two it is 0.1. The in-block edge probability is 0.55, and across-isomorphic block probability is 0.02 and across-non-isomorphic block probability is 0.05. Note that blocks one and three are isomorphic blocks (see Deﬁnition 7).

Since our results is valid for any gMPNN functions, we use a randomly initialized GraphSAGE [24] GNN model for our ﬁrst experiments with node embeddings. Following our Deﬁnition 6, the initial node embeddings within the same block should be the same, however, following standard GNN procedures we initialize node features as size-normalized degrees. Note that in theory, node within each block has the same expected graphon degree, but this setting is more realistic and shows a stronger results than proposed in our theorems when initial node embeddings also have variance.

To efﬁciently calculate exact cMPNN• embeddings, we need to make use of the property

of SBMs, i.e. the graphon values within a block is constant. The graphon degree dW

for nodes in block 1, 2 and 3 is 0.2615, 0.1 and 0.2615. Then we can write the integral

X

W (x,y) dW (x)

Φ(t)

(f

•

(t−1)

(x),

f

•

(t−1)

(y))dµ(y

)

as

1 0.2615

(0.45

×

S1,1Φ(t)(B1(t−1),

B1(t−1))

+

0.1

×

S1,2Φ(t)(B1(t−1), B2(t−1)) + 0.45 × S1,3Φ(t)(B1(t−1), B3(t−1))). This can be calculated exactly by

extracting the neural network weights from the GNN model for Φ and Ψ.

17

Table 2: Test performance over 50 runs of node and pairwise gMPNNs for in-distribution and OOD link prediction over SBM graphs. Methods marked with ∗ indicate best result out of distinct conﬁgurations detailed in the Appendix.
Training graph size N tr = 103

Tasks

Model

Hit@10(%) Hit@50(%) Hit@100(%) mcc.(%) balanced acc.(%)

Transductive

In-distribution link prediction

GraphSAGE* GraphSAGE(max)* GCN* GAT* GAT(max)* GIN*
gMPNN•• (ﬁxed Ψ) gMPNN•• (learn Ψ)

95.55( 0.52) 95.43( 0.38) 93.15(14.57) 93.77(13.03) 92.91(12.27) 95.77( 0.59)
93.76( 0.55) 96.71( 0.32)

95.93( 0.73) 96.13( 0.57) 93.99(13.08) 94.01(13.02) 93.88( 9.12) 96.09( 0.58)
94.17( 0.51) 96.88( 0.31)

96.14( 0.74) 96.54( 0.60) 94.35(12.72) 94.14(13.03) 94.08( 8.82) 96.28( 0.59)
94.51( 0.49) 97.00( 0.30)

95.42( 0.37) 95.38( 0.36) 92.41(14.72) 90.94(16.09) 87.36(20.41) 95.48( 0.41)
93.64( 0.53) 94.23( 0.55)

97.66( 0.19) 97.64( 0.19) 95.97( 8.24) 95.26( 8.38) 93.34(10.95) 97.69( 0.22)
96.72( 0.28) 97.03( 0.29)

Oracle

96.92( 0.36) 96.92( 0.36) 96.92( 0.36) 93.74( 0.42)

96.77( 0.22)

Inductive N te = N tr

GraphSAGE* GraphSAGE(max)* GCN* GAT* GAT(max)* GIN*
gMPNN•• (ﬁxed Ψ) gMPNN•• (learn Ψ)

47.38(39.08) 17.72(22.89) 66.29(37.67) 40.05(39.05) 41.98(39.23) 39.33(34.62)
93.85( 0.49) 96.71( 0.30)

52.13(38.87) 25.91(27.75) 68.52(35.87) 41.34(39.39) 43.34(38.71) 42.93(33.86)
94.23( 0.51) 96.91( 0.28)

54.94(37.83) 31.43(30.18) 69.92(35.12) 41.96(39.54) 43.54(38.69) 43.90(33.72)
94.55( 0.49) 97.02( 0.27)

19.34(43.19) 18.24(30.43) 31.76(35.12) 19.44(35.22) 22.66(38.99) 18.59(39.43)
93.74( 0.48) 94.23( 0.59)

61.46(20.17) 58.65(14.53) 67.21(22.75) 59.52(16.94) 61.46(19.02) 59.79(18.24)
96.77( 0.25) 97.03( 0.31)

Oracle

97.01( 0.31) 97.01( 0.31) 97.01( 0.31) 93.87( 0.39)

96.84( 0.20)

OOD link prediction Inductive N te = 104

GraphSAGE* GraphSAGE(max)* GCN* GAT* GAT(max)* GIN*
gMPNN•• (ﬁxed Ψ) gMPNN•• (learn Ψ)

9.97(19.47) 1.44( 2.35) 39.29(31.33) 27.31(26.93) 32.56(26.94) 0.00( 0.00)
96.74( 0.07) 96.97( 0.04)

11.73(21.80) 2.60( 4.76) 42.15(30.81) 28.13(26.78) 33.01(27.16) 0.00( 0.00)
96.93( 0.04) 97.02( 0.04)

12.98(23.70) 3.58( 6.53)
44.19(30.97) 28.72(26.93) 33.24(27.27)
0.00( 0.00)
97.01( 0.04) 97.08( 0.04)

-6.56( 5.12) -2.52( 4.44) -4.88(14.84) -2.00( 8.96) -2.85( 9.76) -3.93( 5.12)
93.76( 0.05) 93.94( 0.67)

49.32( 0.60) 49.83( 0.57) 50.33( 6.72) 50.20( 3.37) 49.82( 3.43) 49.59( 0.57)
96.78( 0.03) 96.88( 0.35)

Oracle

96.96( 0.03) 96.96( 0.03) 96.96( 0.03) 93.77( 0.04)

96.79( 0.02)

Then we compare the difference between gMPNN• and cMPNN• for increasing number of nodes.

We

ﬁrst

plot

log-log

plots,

where

a

O( √1 )
N

decay

rate

will

have

slope

−

1 2

in

the

log-log

plot.

Our

theory

bounds

the

decay

rate

by

O( lo√g N ),
N

which

can

be

approximated

by

the

−

1 2

slope

and

is

validated in Figure 2.

Pairwise embeddings For the experiment with pairwise embeddings, we test both the Φ and Ψ of Lemma 2, and a scenario where Ψ is a randomly initialized two layer feed-forward neural network. To compute the cMPNN•• embeddings, without choosing the adjacency matrix as input to the model, we can input the graphon value matrix W where Wi,j = W (Xi, Xj). In our experiment, we choose graph with 20 nodes, 9 in block 1, 2 in block 2, and 9 in block 3. The result of cMPNN•• is stable for graphs with different sizes. Then we plot the same log-log plot as above.

B.3 Link prediction performance evaluation with SBMs
0.6 0.05 0.02 First, we use a slightly modiﬁed SBM with S = 0.05 0.6 0.05 with other things the same as
0.02 0.05 0.6 in the above subsection. Here we increase the in-block edge probability to 0.6 since we are going to hide edges for link prediction purpose.
We start by sampling the training graph (Gtr, F tr) with N tr = 103 nodes. We randomly hide 10% of Etr from the original graph Gtr for link prediction purpose since the goal of link prediction is to predict possible missing links that is not observed in the original graph. We call these edges Ehid-tr.
Then we split Ehid-tr into positive train (80%) and validation (10%) edges (we reserve 10% of Ehid-tr for the transductive test scenario), and uniformly sample the same number of across-block non-edges

18

Inductive N te = 3840 Inductive N te = N tr

Table 3: Test performance over 50 runs of node and pairwise gMPNNs for in-distribution and OOD link prediction over the ogbl-ddi graph. Methods marked with ∗ indicate best result out of distinct conﬁgurations detailed in the Appendix.
Training graph size N tr = 427

Tasks

Model

Hit@10(%) Hit@50(%) Hit@100(%) mcc.(%) balanced acc.(%)

Transductive

In-distribution link prediction

GraphSAGE* GCN* GAT* GIN*
gMPNN•• (ﬁxed Ψ) gMPNN•• (learn Ψ)

30.23(2.03) 17.91(0.52)
1.46(0.52) 17.21(4.74)
14.09(0.06) 38.60(1.68)

47.70(1.75) 33.69(0.60)
8.20(1.34) 28.76(5.79)
50.32(0.01) 59.04(0.22)

60.36(1.79) 44.34(0.85) 16.37(1.95) 37.46(6.60)
65.41(0.01) 68.63(0.06)

71.47(0.70) 59.45(0.50) 52.64(1.62) 54.27(1.59)
73.23(0.10) 71.96(0.06)

85.72(0.36) 78.85(0.36) 74.75(0.61) 76.84(1.19)
86.60(0.04) 85.74(0.03)

Random

0.48(2.58) 1.16(4.58) 2.01(6.54) 0.05(0.39)

50.00(0.01)

GraphSAGE* GCN* GAT* GIN*
gMPNN•• (ﬁxed Ψ) gMPNN•• (learn Ψ)

10.52(1.33) 10.76(0.90) 0.07(0.02) 10.95(4.19)
34.24(0.07) 56.45(0.08)

23.85(1.29) 24.79(0.73) 0.22(0.10) 24.42(5.75)
66.87(0.03) 68.42(0.03)

36.60(1.37) 34.99(0.70)
0.51(0.07) 33.71(6.70)
73.91(0.02) 74.93(0.02)

47.58(2.98) 50.82(0.19) -0.93(0.77) 40.67(2.36)
67.89(0.34) 65.55(0.15)

71.59(2.46) 74.73(0.21) 50.00(0.01) 66.24(1.75)
83.76(0.20) 82.62(0.09)

Random

0.41(1.64) 2.20(4.88) 4.97(8.77) -0.03(0.22)

50.00(0.00)

GraphSAGE* GCN* GAT* GIN*
gMPNN•• (ﬁxed Ψ) gMPNN•• (learn Ψ)

1.79(1.21) 12.38(1.23) 2.76(1.27) 0.00(0.00)
9.31(5.23) 57.97(0.02)

13.70(6.71) 27.28(1.27) 7.55(3.28) 0.00(0.00)
67.42(0.02) 74.75(0.07)

25.31(8.77) 37.45(1.43) 12.78(4.50)
0.00(0.00)
78.44(0.01) 80.00(0.11)

16.65(3.31) 55.03(0.76) 23.83(16.31) 45.87(3.55)
75.42(0.17) 72.04(0.20)

52.79(1.01) 77.38(0.36) 59.54(6.96) 68.92(2.78)
87.37(0.11) 84.57(0.14)

Random

1.21(3.50) 3.39(7.72) 5.71(11.13) 0.00(0.00)

50.00(0.00)

OOD link prediction

Table 4: Test performance over 50 runs of node and pairwise gMPNNs for in-distribution (large) and OOD (small) link prediction over SBM graphs. Methods marked with ∗ indicate best result out of distinct conﬁgurations detailed in the Appendix.
Training graph size N tr = 104

Tasks

Model

Hit@10(%) Hit@50(%) Hit@100(%) mcc.(%) balanced acc.(%)

Transductive

In-distribution link prediction

GraphSAGE* GCN* GAT* GIN*
gMPNN•• (ﬁxed Ψ) gMPNN•• (learn Ψ)

75.35(38.50) 86.23(27.88) 59.21(43.07) 80.89(33.65)
95.74( 0.12) 96.95( 0.03)

75.41(38.53) 86.48(27.85) 59.62(43.09) 81.12(33.71)
96.15( 0.06) 96.95( 0.03)

75.46(38.55) 86.56(27.85) 59.79(43.12) 81.20(33.72)
96.33( 0.04) 96.95( 0.03)

70.81(43.47) 82.73(32.32) 50.19(42.84) 82.46(30.01)
93.77( 0.04) 93.76( 0.06)

85.93(20.62) 91.36(15.84) 75.51(21.35) 90.49(16.59)
96.79( 0.02) 96.79( 0.03)

Oracle

96.96( 0.03) 96.96( 0.03) 96.96( 0.03) 93.77( 0.04)

96.79( 0.02)

GraphSAGE* GCN* GAT* GIN*
gMPNN•• (ﬁxed Ψ) gMPNN•• (learn Ψ)

64.77(40.22) 79.67(34.82) 46.73(37.62) 59.68(41.62)
95.67( 0.11) 96.94( 0.04)

65.88(39.91) 79.90(34.55) 47.12(37.64) 61.15(41.47)
96.15( 0.06) 96.94( 0.04)

66.60(39.87) 80.07(34.31) 47.31(37.65) 61.69(41.37)
96.33( 0.04) 96.94( 0.04)

33.19(50.16) 51.16(49.53) 19.14(39.03) 44.80(46.15)
93.77( 0.05) 93.76( 0.06)

68.30(23.45) 76.23(23.72) 60.02(18.77) 71.90(22.57)
96.79( 0.03) 96.78( 0.03)

Oracle

96.95( 0.04) 96.95( 0.04) 96.95( 0.04) 93.77( 0.05)

96.79( 0.03)

GraphSAGE* GCN* GAT* GIN*
gMPNN•• (ﬁxed Ψ) gMPNN•• (learn Ψ)

33.52(44.93) 72.28(40.06) 23.31(39.07) 1.31( 1.62)
93.68( 0.40) 96.12( 0.28)

33.70(44.87) 73.95(38.58) 23.32(39.07) 1.39( 1.62)
93.72( 0.41) 96.44( 0.34)

33.97(44.77) 74.17(38.56) 23.34(39.07)
1.42( 1.62)
93.74( 0.41) 96.57( 0.36)

32.72(47.00) 68.93(40.98) 24.07(39.18) -0.64( 5.63)
93.40( 0.42) 94.43( 0.31)

66.97(22.73) 84.54(19.69) 61.74(19.39) 49.93( 0.63)
96.59( 0.22) 97.14( 0.16)

Oracle

96.94( 0.30) 96.94( 0.30) 96.94( 0.30) 93.82( 0.40)

96.81( 0.21)

Inductive N te = 103 Inductive N te = N tr

OOD link prediction

as negative train and validation edges. The embedding method gMPNN• (resp. gMPNN••) along with link predictor η• (resp. η••) are trained in an end-to-end manner for predicting positive and
negative edges in training using cross-entropy loss. Our experiments consider three scenarios (in
all scenarios we use the same number of negative test edges as positive test edges, sampled from non-edges in Gte with endpoints in different isomorphic blocks): (i) (In-distribution) transductive scenario where Gte = Gtr, where positive test edges are the 10% reserved in Ehid-tr not used in training or validation; (ii) In-distribution inductive scenario where Gte is sampled from the same SBM with N te = N tr, where we also hide 10% of the edges and sample 0.1|Ehid-tr| positive test edges from Ehid-te (for fair comparison across all scenarios); (c) OOD inductive scenario where Gte is

19

sampled from the same SBM with N te = 10 × N tr, where we also hide 10% of the edges and sample 0.1|Ehid-tr| positive test edges from Ehid-te(for fair comparison across all scenarios).
For structural node embeddings we consider GraphSAGE [24], GCN [28] (without positional features), GAT [70] and GIN [78] as the representatives of gMPNN• models. Here we also add max aggregation for GAT and GraphSAGE model as proposed by Xu et al. [80] for extrapolation. The link predictor η• is as feedfoward network that receives the two node embeddings as input, and has link prediction threshold τ = 0.5 (see Deﬁnition 8 for details). We initialize the node features as the size-normalized degrees.
For structural pairwise embeddings we choose our proposed gMPNN•• method of Deﬁnition 10, since we can prove that our approach is theoretically sound in Lemma 2. We test gMPNN•• in two versions: The Φ and Ψ functions in Lemma 2 (denoted ﬁxed Ψ) and a feedforward neural network for Ψ (denoted learn Ψ). The link predictor η•• is the same as η• except it just takes one pairwise embedding as input, rather than two node embeddings. We initialize the pairwise features as all 1’s to contain no additional information about connectivity between the pair of the nodes.
Many existing link prediction methods rely on positional node embeddings and our work focuses on permutation-equivariant MPNN GNNs. These positional node embedding link prediction methods are not equivariant (they are positional node representations) based on matrix and tensor factorization methods. Developing a theory for the effect of positional node representations in OOD link prediction is far from trivial and an entirely new paper that requires a new theory. At this point we do not even know how positional representations could be approximately counterfactually-invariant.
For all models including gMPNN•, gMPNN••, η• and η••. The number of hidden layers was chosen between {2, 3}, and the number of hidden neurons was chosen between {5, 10} due to the simple experimental set up. For GAT, we have 2 attention heads. Speciﬁcally. We optimized all models using Adam with learning rate chosen from {1×10−3, 5×10−4, 1×10−4}. We also choose η• as taking the inner product between pair of nodes as input (as Hu et al. [25]) and the concatenated node embeddings as input. The hyperparameter search is performed by training all models with 10 different initialization seeds and selecting the conﬁguration that achieved the highest mean accuracy on the validation data, and we mark the methods with ∗ in Tables 1 and 2 indicating the optimal conﬁguration is being used. The training time is around 10 minutes for 1, 000 epochs.
Table 2 presents our empirical results in the new setting over 50 independent runs. The oracle predictor knows the graphon values W (Xite, Xjte). The reason why it can not achieve 100% accuracy is because there exists rarely sampled positive edges between blocks. Our evaluation metrics include the Matthews correlation coefﬁcient (mcc) [47], balanced accuracy, and Hits@K for K = 10, 50, 100 that counts the ratio of positive edges ranked at the k-th place or above against all negative edges. The results from the new table conveys the same message as Table 1 and has been discussed in Section 6.
We also include a new setting for training on larger graphs (104 nodes) and extrapolating to smaller graphs (103 nodes) in Table 4. We are able to see the structure node representations gMPNN• are still able to perform relatively well on in-distribution inductive tasks, although the graphs are large, while still suffer from OOD performance to smaller graphs except GCN, although it is not related to the theoretical discussions of this paper. In contrast, the gMPNN•• is able to consistently offer good performance on both in-distribution and OOD tasks.
As discussed in Appendix A, Xu et al. [79] shows that GNNs can extrapolate in algorithmic-related tasks as the graph size grows, if the GNN uses max as an aggregator (rather than mean and sum we considered in this paper). Unfortunately, our Deﬁnition 3 of gMPNN• does not allow max aggregators, in part because it is unclear how one could reach stability using the max aggregator. Fortunately, while we could not obtain theoretical results using the max aggregator, we can test it empirically. Table 2 reproduces all our empirical results using the max aggregator (on GraphSAGE and GAT, since these are the only GNNs designed for the max aggregator). Our experiments show that the max aggregator, just like the mean and sum aggregators, shows poor OOD performance as test graph sizes increase. Whether there is theoretical proof that the max aggregator is not able to perform this OOD task is left as future work.
20

B.4 Link prediction performance evaluation with ogbl-ddi
In what follows we introduce empirical results using the ogbl-ddi dataset, which represents a drugdrug interaction network. For the purpose of performing OOD tasks, we start by sampling 10% of the nodes (427 nodes) and its induced subgraph to be the training graph, where node features are constructed as size-normalized degrees in the training graph. Validation positive and negative edges are obtained by applying the original edge split on the induced training subgraph. Our experiments consider three scenarios: (i) (In-distribution) transductive scenario where Gte = Gtr, where test positive and negative edges are obtained by applying the original edge split on the induced training subgraph; (ii) In-distribution inductive scenario where Gte is constructed as sampling N te = N tr nodes from the remaining ogbl-ddi graph and its induced subgraph, where the test edges are obtained by applying the original edge split on the newly induced test subgraph; (iii) OOD inductive scenario where Gte is the induced subgraph without the training nodes with N te = 3840, the test edges are obtained by applying the original edge split on the newly induced test subgraph, where we further down-sample to the same amount of test edges as in (ii) for fair comparison across all scenarios.
We used the same benchmarking methods as in the SBM experiments, and add a random guesser where it is constructed as randomly-initialized GraphSAGE model with a randomly-initialized link predictor. We initialize the pairwise features as all 1’s to contain no additional information about connectivity between the pair of the nodes for gMPNN••.
For structural node embeddings we consider GraphSAGE [24], GCN [28] (without positional features), GAT [70] and GIN [78] as the representatives of gMPNN• models. The link predictor η• is as feedfoward network that receives the two node embeddings as input, and has link prediction threshold τ = 0.5 (see Deﬁnition 8 for details). We initialize the node features as the size-normalized degrees.
For structural pairwise embeddings we choose our proposed gMPNN•• method of Deﬁnition 10, since we can prove that our approach is theoretically sound in Lemma 2. We test gMPNN•• in two versions: The Φ and Ψ functions in Lemma 2 (denoted ﬁxed Ψ) and a feedforward neural network for Ψ (denoted learn Ψ). The link predictor η•• is the same as η• except it just takes one pairwise embedding as input, rather than two node embeddings. We initialize the pairwise features as all 1’s to contain no additional information about connectivity between the pair of the nodes.
For all models including gMPNN•, gMPNN••, η• and η••. The number of hidden layers was chosen between {2, 3}, and the number of hidden neurons was chosen between {16, 32} due to the simple experimental set up. For GAT, we have 2 attention heads. Speciﬁcally. We optimized all models using Adam with learning rate chosen from {1×10−3, 5×10−4, 1×10−4}. We also choose η• as taking the inner product between pair of nodes as input (as Hu et al. [25]) and the concatenated node embeddings as input. We train all the models with 200 epochs. The hyperparameter search is performed by training all models with 10 different initialization seeds and selecting the conﬁguration that achieved the highest mean accuracy on the validation data, and we mark the methods with ∗ in Table 3 indicating the optimal conﬁguration is being used.
Table 3 presents our empirical results on the ogbl-ddi link prediction task. All gMPNN• methods performs worse in inductive settings than transductive settings, and suffer much worse performance in OOD transductive setting except GCNs. In contrast, the gMPNN•• is able to consistently offer good performance on both in-distribution and OOD tasks, showing that the theoretical results are not limited to SBM models.
C Deﬁntion and notations
In this section, we follow the deﬁnitions and notations from Maskey et al. [46, Appendix A]. As in Maskey et al. [46, Appendix A], we call the metric space (χ, d), where the metric in the space χ is deﬁned as d : χ × χ → [0, ∞). The nodes of the graph are considered as sampled point from χ, the node i is identiﬁed with Xi for the graph G with nodes X = (X1, . . . , XN ). We also represent F (Xi) := fi for i = 1, . . . , N .
Next, we deﬁne various notions of degree for the pairwise node embedding.
Deﬁnition 11. Let W deﬁned in Deﬁnition 1, and G as the sampled graph with nodes X = (X1, ..., XN ).
21

• We deﬁne the graphon fraction of common neighbors at x, y ∈ X by

cW (x, y) = W (x, z)W (y, z)dµ(z),

(2)

X

• Given two points x, y that need not be in X, we deﬁne the graph-graphon fraction of common neighbors of X at x, y by

1N

cX (x, y) = N W (x, Xi)W (y, Xi),

(3)

i=1

• Given two points x, y that need not be in X, we deﬁne the sampled-graph fraction of common neighbors of X at x, y by

1N

cA(x, y) = N A(x, Xi)A(y, Xi),

(4)

i=1

where we deﬁne A(x, Xi) ∼ Ber(W (x, Xi)) and A(y, Xi) ∼ Ber(W (y, Xi)) as independent random variables.

where cX (x, y) and cA(x, y) are interpreted as the graph fraction of common of neighbors of the node pair (x, y) in the graph (x, y, X1, ..., Xn).
Adapting Maskey et al. [46, Deﬁnition A.3] to the continuous integral aggregation, Deﬁnition 12. Let W be deﬁned in Deﬁnition 1, for a metric-space message signal U : X ×X → RF , the continuous integral aggregation is deﬁned by

MW• U = W (·, y)U (·, y)dµ(y).
X

Adapting Maskey et al. [46, Deﬁnition A.4] to the N-normalized sum aggregation,

Deﬁnition 13. Let W be deﬁned in Deﬁnition 1, X = X1, ..., Xn sample points. For a metric-space message signal U : X × X → RF , we deﬁne the graph-graphon (N-normalized) sum aggregation by

MX• U

=

1 N

W (·, Xi)U (·, Xi),

i

and the sampled-graph (N-normalized) sum aggregation by

MA• U

=

1 N

A(·, Xi)U (·, Xi),

i

where we deﬁne A(x, Xi) ∼ Ber(W (x, Xi)) as a random variable. Deﬁnition 14. Let W be deﬁned in Deﬁnition 1, X = X1, ..., Xn sample points. For a metric-space message signal U : (X × X ) × (X × X ) → RF , we deﬁne the graphon pairwise aggregation by

MW••U

=

1 2

W (y, z)

W (x, z)

(

U (·, (x, z)) +

U (·, (y, z)))dµ(z),

X cW (·, ·)

cW (·, ·)

and the graph-graphon pairwise aggregation by

MX••U

=

1 2N

N i=1

(

W (y, Xi) cX (·, ·)

U

(·,

(x,

Xi))

+

W (x, Xi) cX (·, ·)

U

(·,

(y,

Xi))),

and the sample-graph pairwise aggregation by

MA••U

=

1 2N

N i=1

(

A(y, Xi) cA(·, ·)

U

(·,

(x,

Xi))

+

A(x, Xi) cA(·, ·)

U

(·,

(y,

Xi))),

where we deﬁne A(x, Xi) ∼ Ber(W (x, Xi)) as a random variable.

22

Maskey et al. [46, Deﬁnition A.7] has deﬁned for a vector z = (z1, . . . , zF ) ∈ RF , we deﬁne as usual
z ∞ = max |zk|.
1≤k≤F
For every x, x ∈ X , we say a function f : χ → RF is Lipschitz continuous if there exists a Lf > 0 such that for every x, x ∈ χ, we have
f (x) − f (x ) ∞ ≤ Lf d(x, x ).
Here if χ = RF , d(x, x ) = x − x ∞. For our theoretical results, we make the following assumptions: Assumption 1. (extension of Maskey et al. [46, Deﬁnition A.10]) Let (χ, d) be a metric space and W : χ × χ → [0, ∞). Let Θ be a MPNN with message and update functions Φ(l) : R2Fl → RHl and Ψ(l) : RFl+Hl → RFl+1 , l = 1, . . . , T − 1.
1. By Deﬁnition 1 of the graphon , the graphon satisﬁes W ∞ ≤ 1.
2. [46, Deﬁnition A.10, item 6]: There exists a constant dmin > 0 such that for every x ∈ χ, we have dW (x) ≥ dmin.
3. There exists a constant dcmin such that for every x, y ∈ X , we have cW (x, y) ≥ dcmin.
4. Mtr = max(supp(N tr)) is the largest graph in training, where N tr is the distribution of graph sizes in the training data.
5. [46, Similar to Deﬁnition A.10, item 7 adding dependence on Mtr]: For every l = 1, . . . , T , the message function Φ(l) and update function Ψ(l) are Lipschitz continuous with Lipschitz constants L(Φl)(Mtr) and L(Ψl)(Mtr) respectively.
D Large real-world and random graphs have relatively few isomorphic nodes
In what follows we show that isomorphic nodes are rare both in many real-world networks and SBMs. We start with real-world graphs. MacArthur et al. [42] has computed the fraction of non-isomorphic nodes (denoted as the network redundancy rG by [42]) of different types of small (< 23,000 nodes) real-world graphs. MacArthur et al. [42] shows that the majority of biological graphs are composed of mostly non-isomorphic nodes. Small technological networks (e.g., road network) tend to have signiﬁcantly more isomorphic nodes.
In order to see whether these results also hold for larger graphs, we performed a similar experiment on the following datasets.
• The ogbl-ppa dataset is an undirected, unweighted graph. Nodes represent proteins from 58 different species, and edges indicate biologically meaningful associations between proteins [74], e.g., physical interactions, co-expression, homology or genomic neighborhood.
• The ogbl-ddi dataset is a homogeneous, unweighted, undirected graph, representing the drug-drug interaction network. Each node represents an FDA-approved or experimental drug [74]. Edges represent interactions between drugs and can be interpreted as a phenomenon where the joint effect of taking the two drugs together is considerably different from the expected effect in which drugs act independently of each other.
• The Slashdot graph contains friend/foe links between the users of Slashdot (where we ignore edge types).
• HepPh is a co-authorship network where if an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes.
• The Github graph shows GitHub developers (nodes) who have starred at least 10 repositories and edges are mutual follower relationships between them (we make the graph undirected for our analysis).
23

Fraction of non-isomorphic nodes

1.0

Fraction of non-isomorphic nodes vs graph size

Github

ppa

ddiTwitch/En

0.8

Slashdot

HepPh Epinions

0.6

0.4

0.2

0.0 0

200000

400000

Graph size

600000

Figure 3: Fraction of isomorphic nodes in real-world graphs: The fraction of non-isomorphic nodes (also denoted as the network redundancy rG by [42]) in real-world graphs tends to be close to 90%, except in the HepPh collaboration network and Slashdot, which contain many small disconnected components. We assume all graphs are undirected and unattributed for this analysis.

• The Twitch/En graph shows Twitch users (who stream in English) as nodes and links are mutual friendships between them.
• The Epinions graph shows users of the consumer review site Epinions.com as nodes and edges as trust relationships between users.

Figure 3 shows the fraction of non-isomorphic node shown against the size of the graph. This analysis considers a few datasets widely used in the neural network literature to benchmark link prediction methods such as OGB2 ppa and ddi, where ppa is the largest dataset we were able to run the nauty3 isomorphism checking algorithm without crashing. Nauty [48] is one of the most efﬁcient graph isomorphism algorithms available, which we use to calculate a lower bound on the size of the automorphism group of our graphs. Using social networks in the SNAP4 repository we again observe a large fraction of the nodes are non-isomorphic. Visual inspection shows that most isomorphic nodes are low-degree siblings (e.g., the most common are nodes with degree one that have the same parent). Note that our results do not contradict Ball and Geyer-Schulz [5], which shows that many real-world graphs have isomorphic nodes. Having isomorphic nodes is different than containing a large fraction of isomorphic nodes.
The results in Figure 3 show that most nodes in real-world graphs tend to be non-isomorphic for reasonably large graphs (in particular ppa). In what follows we show that random graph models generally do not contain isomorphic nodes with high probability.
Theoretical results on random graphs. Regarding isomorphic nodes on random graphs, we can prove the following result:
Corollary 3. Consider a random graph G = (V, E) with N given nodes so that all possible 2(N2 ) graphs should have the same probability to be chosen. Then, as N → ∞ all nodes in G are non-isomorphic, regardless whether we take the nodes in G to be attributed or unattributed.

Proof. The proof for unattributed G is a direct consequence of Erdos and Rényi [18, Theorem 2]. Adding node attributes cannot make two non-isomorphic nodes be isomorphic, which concludes our proof.

Erdos and Rényi [18, Theorem 3] (see also Kim et al. [27, Theorem 3.1]) shows that the statement in Corollary 3 is also true for G(N, p) graphs with p satisfying (ln N )/N ≤ p ≤ 1 − (ln N )/N . Kim et al. [27, Theorem 3.1] shows a similar result for random d-regular graph on N vertices with 3 ≤ d ≤ n − 4. Luczak et al. [41] has shown similar results for preferential-attachment graphs, where in each step a new node with m ≥ 3 edges is added. In what follows we show a similar result for SBMs.
2https://ogb.stanford.edu/docs/graphprop/ 3https://pallini.di.uniroma1.it/ 4https://snap.stanford.edu/data/index.html

24

Proposition 1. Consider a random graph G = (V, E) with N given nodes, generated by the SBM in Deﬁnition 6, where within-block and inter-block probabilities in S lie in the interval (p, 1 − p), with (ln N )/N ≤ p ≤ 1 − (ln N )/N . Then, as N → ∞ all nodes in G are non-isomorphic, regardless whether we take the nodes in G to be attributed or unattributed.

Proof. Let S have r > 0 blocks. Consider generating the G ﬁrst by sampling the within-block edges.

Let Ga be the induced subgraph of all nodes that belong to a single block a ∈ {1, . . . , r}. By the

results in Erdos and Rényi [18, Theorem 3], as N → ∞, Ga has no isomorphic nodes. The above is

true for all within-block edges. Now consider sampling the between-block edges of two i, j ∈ V

nodes in G. The event of i and j being isomorphic (if considering just their edges to Ga) is the same

as they connecting to the same nodes in Ga (since each node on a block is non-isomorphic, if they

connect to different nodes they would no longer be isomorphic). The probability of this event is at

most (1 − )αN , for = min(p, 1 − p), where α > 0 is the fraction of nodes in Ga (which is not a

function of N ). As N → ∞, by the union bound, the probability that this will happen with any pair

of edges is at most

N 2

(1 −

)αN , which goes to zero.

W.l.o.g. now assume a is the block with the least number of nodes (which is also diverging as N → ∞). The only alternative for i and j to be isomorphic is to do so by connecting to nodes

in distinct blocks. For instance, we could imagine r copies of Ga: Even though i and j did not

connect to the same nodes in the same graphs, they connected to their isomorphic equivalent nodes in

different copies. But since there are only r blocks, and r does not depend on N , this event must have

probability at most (1 −

)αN/r. As N → ∞, by the union bound, the probability

N 2

(1 −

)αN/r

goes to zero. Replacing the copies of Ga with the actual sampled blocks only makes this probability

smaller, since the subgraphs of the other blocks are larger and may contain different topologies than

Ga (making their nodes distinct from the nodes in Ga). Finally, adding node attributes cannot make

two non-isomorphic nodes be isomorphic, which concludes our proof.

E Proof results for Theorem 1

In what follows we provide the elements to prove Theorem 1.

First we prove the following lemma of the difference between a graph-graphon (N-normalized) sum aggregation and a sampled-graph (N-normalized) sum aggregation in Deﬁnition 13. Using the same assumptions as Maskey et al. [45, Lemma B.3]:

Lemma 3. Let (χ, d, µ) be a metric-measure space and W be a graphon s.t. Assumptions 1.1-2. are satisﬁed. Let Φ : R2F → RH be Lipschitz continuous with Lipschitz constant LΦ(Mtr), with Mtr as in Assumption 1 item 4. Consider a metric-space signal f : χ → RF with f ∞ < ∞. Suppose that X1, . . . , XN are drawn i.i.d. from µ on χ, and let p ∈ (0, 1/H). Let x ∈ χ, and deﬁne the random
variable

1N

1N

Tx = N

A(x, Xi)Φ f (x), f (Xi)

− N

W (x, Xi)Φ f (x), f (Xi)

i=1

i=1

on the sample space X N × [0, 1]N . Then, with probability at least 1 − Hp, we have

Tx

∞

≤

√ 2

(LΦ(Mtr)

f

∞

+

Φ(0, √

0)

∞)

log 2/p .

(5)

N

Proof. The proof of the bound is the same as the proof in Maskey et al. [45, Lemma B.3], even though Tx is a different quantity than the quantity used in Maskey et al. [45, Lemma B.3].
Combining Maskey et al. [45, Lemma B.3] and Lemma 3, we can use the triangle inequality to prove the following lemma about concentration of error between a sampled-graph (N-normalized) sum aggregation in Deﬁnition 13 and the continuous integral aggregation in Deﬁnition 12, which is used in Deﬁnitions 3 and 5. Using the same assumption as Maskey et al. [45, Lemma B.4]:
25

Lemma 4. Let (χ, d, µ) be a metric-measure space and W be a graphon s.t. Assumptions 1.1-2. are satisﬁed. Let Φ : R2F → RH be Lipschitz continuous with Lipschitz constant LΦ(Mtr). Consider a metric-space signal f : χ → RF with f ∞ < ∞. Suppose that X1, . . . , XN are drawn i.i.d. from
µ on χ, and let p ∈ (0, 1/(2H)). Let x ∈ χ, and deﬁne the random variable

1N

Rx = N A(x, Xi)Φ f (x), f (Xi)
i=1

−

W (x, y)Φ f (x), f (y) dµ(y)
χ

on the sample space χN × [0, 1]N . Then, with probability at least 1 − 2Hp, we have

Rx

∞

≤

√ 22

(LΦ

(Mtr)

f

∞

+

Φ(0, √

0)

∞)

log 2/p .

(6)

N

Proof. Use the triangle inequality, the results from Maskey et al. [45, Lemma B.3] and Lemma 3.

Deﬁne

Yx

=

1 N

N i=1

W

(x,

Xi)Φ

f (x), f (Xi)

−

χ W (x, y)Φ f (x), f (y) dµ(y).

Rx ∞ = Tx + Yx ∞ ≤ Tx ∞ + Yx ∞.

From Maskey et al. w.p. 1 − Hp and

[45, Yx

Lemma B.3] and Lemma 3, Tx

∞

≤

√ 2

(LΦ

(Mtr

)

f

∞

+

Φ(0,0) √

N

∞√≤

√ 2

(LΦ

(Mtr

)

f

∞

+

Φ(0,0) √

N

√
∞) log 2/p

∞) log 2/p w.p. 1 − Hp. We have with

probability at least 1 − 2Hp using the union bound of the two events,

Rx

∞

≤

√ 22

(LΦ(Mtr)

f

∞

+

Φ(0, √

0)

∞)

log 2/p .

N

Based on Lemma 4, we can prove the following corollary about the maximum concentration error

(bMetwW• e)efnosraamllpthleedn-gordaepshin(Nth-neosrammaplliezdedg)rsaupmh Gag. gUresginagtiothne(sMamA•e)

and continuous overall framing

integral aggregation as [45, Lemma B.3]:

Corollary 4. Consider (χ, d, µ) a metric-measure space and graphon W satisfying items 1 and 2 of Assumption 1. Let Φ : R2F → RH be Lipschitz continuous with Lipschitz constant LΦ(Mtr), and a metric-space signal f : χ → RF with f ∞ < ∞. Deﬁne X1, . . . , XN as drawn i.i.d. from µ on χ, and then edges Ai,j ∼ Ber(W (Xi, Xj)) i.i.d sampled. Let p ∈ (0, 1/2H), and deﬁne the random
variable

1N

RXi = N

A(Xi, Xj)Φ f (Xi), f (Xj)

j=1

−

W (Xi, y)Φ f (Xi), f (y) dµ(y)
χ

on the sample space χN × [0, 1]N . Then, with probability at least 1 − 2Hp, we have

max
i=1,...,N

(MA• − MW• )

Φ(f, f )

(Xi)

∞ = max
i=1,...,N

RXi

∞

≤

√ 22

(LΦ(Mtr)

f

∞+

Φ(0, 0) √

∞)

log(2N/p) .

(7)

N

Proof.

Using the result from Lemma 3 we have with probability 1 −

Hp N

,

1N

1N

N

A(x, Xi)Φ f (x), f (Xi)

− N

W (x, Xi)Φ f (x), f (Xi) ∞

i=1

i=1

≤

√ 2

(LΦ(Mtr)

f

∞+

Φ(0, 0) √

∞)

log(2N/p) .

N

26

Using the union bound of the N events that the above equations happens for x = X1, ..., XN , with probability at least 1 − Hp, we have

1N

1N

max i=1,...,N N

A(Xi, Xj)Φ f (Xi), f (Xj)

− N

W (Xi, Xj)Φ f (Xi), f (Xj) ∞

j=1

j=1

≤

√ 2

(LΦ(Mtr)

f

∞+

Φ√(0, 0) ∞)

log(2N/p) .

N

The same logic can be applied to YXi , ∀i ∈ {1, ..., N }. Thus, using the triangle inequality, and the union bound of the two events, we have with probability at least 1 − 2Hp,

max
i=1,...,N

RXi

∞

≤

√ 22

(LΦ(Mtr)

f

∞+

Φ(0, 0) √

∞)

N

log(2N/p) .

Now the layer-wise error between a cMPNN• and gMPNN• can be bounded as follows:

Corollary 5. Consider (χ, d, µ) a metric-measure space and graphon W consistent with items 1

and 2 of Assumption 1. Let Φ : R2F → RH and Ψ : RF +H → RF be Lipschitz continuous

with Lipschitz constants LΦ(Mtr) and LΨ(Mtr). Consider a metric-space signal f : χ → RF with

f

∞

<

∞.

Let

p

∈

(0,

1 2(H +1)

).

Suppose

that

X1, . . . , XN

are

drawn

i.i.d.

from

µ

in

χ,

and

then

edges Ai,j ∼ Ber(W (Xi, Xj)) i.i.d sampled. Then with probability at least 1 − 2Hp,

max
i=1,...,N

Ψ f (·), MA• Φ(f, f ) (Xi)

− Ψ f (·), MW• Φ(f, f ) (Xi)

∞

≤ LΨ(Mtr)

√ 22

(LΦ(Mtr)

f

∞+

Φ(0, 0) √
N

∞)

log(2N/p) .

(8)

Proof. The proof is the same as Maskey et al. [45, Lemma B.6]. The different result comes from the different bound between Corollary 4 and Maskey et al. [45, Lemma B.5].

E.1 Proof of Theorem 1

Following [45, Appendix B.2], they ﬁrst bound the layer-wise error as Corollary 5, and derive the ﬁnal bound through a recurrence relation. The only difference is on the layer-wise bound Corollary 6 and Maskey et al. [45, Corollary B.6]. We will omit the middle parts. Hence, ﬁnally, we can prove Theorem 1 by slightly adpating the proof in Maskey et al. [45, Theorem B.14] to our setting.

Theorem 1 (OOD convergence without in-distribution convergence). For a random graph model

(W, f ) satisfying Deﬁnition 1, let N tr be a random variable deﬁning the distribution of graph

sizes in training. Deﬁne the test distribution (Gte, F te) ∼ (W, f ) through the causal graph in

Figure 1 as an interventional change to obtain larger test graph sizes where min(supp(N te)) Mtr = max(supp(N tr)) (which means any test graph is much larger than the largest possible training

graph). Let Θ = ((Φ(l))Tl=1, (Ψ(l))Tl=1) be a MPNN as in Deﬁnition 2 with T layers such that Φ(l) : R2Fl−1 → RHl−1 and Ψ(l) : RFl−1+Hl−1 → RFl are learned from the training distribution

and are Lipschitz continuous with Lipschitz constants L(Φl)(Mtr) and L(Ψl)(Mtr) that depend on Mtr.

Let gMPNN• Θ•A(T ) and cMPNN• Θ•W(T ) be as in Deﬁnitions 3 and 5. Let X1te, ..., XNte te and Ate be

as in Deﬁnition 1. Let p ∈ (0,

T l=1

1

).

2(Hl +1)

√

Then,

if

√

N te

42

≥,

(1)

log (2N te/p) dmin

we have with probability at least 1 −

T l=1

2(Hl

+

1)p,

δA•-W

:=

max
i=1,...,N te

ΘA•(tTe )(F te)i − Θ•W(T )(f )(Xite) ∞ ≤ (C1 + C2 f

∞)

log(2N te/p)

√

,

N te

where the constants C1 and C2 are deﬁned in the Appendix and depend on {L(Φl)(Mtr), L(Ψl)(Mtr)}Tl=1 and the distribution of (Gtr, F tr).

27

Proof. In this case, Φ(l)(0, 0) ∞, Ψ(l)(0, 0) ∞ can be determined by (Gtr, F tr), N tr if the MPNN Θ has been trained on the training graph (Gtr, F tr).

Following the procedure of Maskey et al. [45, Appendix B.2] with Corollary 5, we can derive similarly, with probability at least 1 − Tl=1(2Hl + 1)p,

T

δA•-W ≤

L(Ψl)(Mtr)

l=1

√ 22

(LΦ(Mtr)(l)

f •(l)

∞

+

Φ(l)(0, √

0)

N te

∞)

log(2N te/p)

(9)

T

((L(Ψl )(Mtr))2 + 2(L(Φl )(Mtr))2(L(Ψl )(Mtr))2),

l =l+1

Using the same proof in Maskey et al. [45, Lemma B.9], we can derive

||f •(l)||∞ ≤ B1(l) + B2(l)||f ||∞,

where B1(l), B2(l) are independent of f , and

l

B1(l) =

L(Ψk)(Mtr) Φ(k)(0, 0) ∞ + Ψ(k)(0, 0) ∞

l
L(Ψl )(Mtr) 1 + L(Φl )(Mtr)

k=1

l =k+1

(10)

and

l

B2(l) =

L(Ψk)(Mtr) 1 + L(Φk)(Mtr) .

(11)

k=1

Now we can decompose the summation in Equation (9). First, we deﬁnce C1 as

C1 = T L(Ψl)(Mtr) 2√2(L(Φl)(Mtr) B1(l) + Φ(l+1)(0, 0) ∞)
l=1

T

×

((L(Ψl )(Mtr))2 + 2(L(Φl )(Mtr))2(L(Ψl )(Mtr))2),

l =l+1

(12)

Then we can deﬁne C2 as

C2 = T L(Ψl)(Mtr) 2√2L(Φl)(Mtr)B2(l)

T
((L(Ψl )(Mtr))2 + 2(L(Φl )(Mtr))2(L(Ψl )(Mtr))2),

l=1

l =l+1

(13)

It is clear to see we can rewrite Equation (9) as

δA•-W ≤ (C1 + C2 f ∞)

log(2N te/p)

√

.

N te

(14)

Thus C1 and C2 depends on {L(Φl)(Mtr)}Tl=1 and {L(Ψl)(Mtr)}Tl=1.

F Proof of theoretical results for hardness of link prediction

In this section, we prove the results for Θ•A(T ) and Θ•W(T ) based on Theorem 1. Now we can prove Corollary 1.

Corollary 1. Let Θ = ((Φ(l))Tl=1, (Ψ(l))Tl=1), ΘA•(T ), ΘW •(T ), p, (W, f ), (Gtr, F tr), (Gte, F te), N tr,

N te, Ate, and X1te, ..., XNte te be as in Theorem 1. If there exists i, j ∈ V te, i = j, s.t. Θ•W(T )(Xi) =

ΘW •(T )(Xj) and Equation (1) is satisﬁed, then, with C1 and C2 as in Theorem 1, we have that with

probability at least 1 −

T l=1

2(Hl

+

1)p,

Θ•A(tTe )(F te)i − ΘA•(tTe )(F te)j

∞ ≤ (C1 + C2

f

2 ∞)

log(2N te/p)

√

.

N te

28

Proof. The proof follows Theorem 1 by using the triangle inequality.

From Theorem 1, we know with proba√bility at least 1 − 2 Tl=1(Hl + 1)p, Θ•A(tTe )(F te)i −

Θ•W(T )(f )(Xite) ∞√ ≤ (C1 + C2 f ∞)

log 2N te/p √ N te

and

Θ•A(tTe )(F te)j − Θ•W(T )(f )(Xjte) ∞ ≤

(C1 + C2 f ∞)

log √

2N

te

/p

.

N te

Then

ΘA•(tTe )(F te)i − ΘA•(tTe )(F te)j ∞ ≤ ΘA•(tTe )(F te)i − ΘW •(T )(f )(Xite) ∞ + Θ•W(T )(f )(Xite) − Θ•A(tTe )(F te)j ∞ = ΘA•(tTe )(F te)i − ΘW •(T )(f )(Xite) ∞ + Θ•W(T )(f )(Xjte) − Θ•A(tTe )(F te)j ∞

2 log(2N te/p)

≤ (C1 + C2 f ∞)

√ N te

.

The ﬁrst inequality holds by traingle inequality, and the second equation holds since Θ•W(T )(f )(Xite) = Θ•W(T )(f )(Xjte).

Then we are able to prove Lemma 1 by induction. By our Deﬁnition 7, we can also claim tk − tk−1 = tπ(k) − tπ(k)−1, ∀k ∈ {1, ..., r}.
Lemma 1. Let Θ = ((Φ(l))Tl=1, (Ψ(l))Tl=1) be a MPNN as in Deﬁnition 2, and Θ•W(T ) as in Deﬁnition 5. For the SBM model (W, f ) in Deﬁnition 6 with N te nodes X1, . . . , XNte . If there exists i, j ∈ V te such that Xite, Xjte are nodes that belong to isomorphic SBM blocks (Deﬁnition 7), then Θ•W(T )(f )(Xite) = Θ•W(T )(f )(Xjte).

Proof. We prove the lemma by induction. We assume in layer l, f •(l)(Xite) = f •(l)(Xjte), 1 ≤ l ≤ T − 1, f •(l) outputs the same value within each block B(l), and B(l) = π ◦ B(l). By Deﬁnitions 6 and 7, we know the assumption holds for l = 1. First,
f •(l+1)(Xite) = Ψ(l+1) f •(l)(Xite), MW• Φ(l+1)(f •(l), f •(l)) (Xite) .
Since f •(l)(Xite) = f •(l)(Xjte), we only need to show MW• Φ(l+1)(f •(l), f •(l)) (Xite) = MW• Φ(l+1)(f •(l), f •(l)) (Xjte).

MW• Φ(l+1)(f •(l), f •(l)) (Xite)

=

W (Xite, y)Φ(l+1)(f •(l)(Xite), f •(l)(y))dy

[0,1]

r

=

W (Xite, y)Φ(l+1)(f •(l)(Xite), f •(l)(y))dy

k=1 [tk−1,tk)

r

=

Φ(l+1)(Ba(l), Bk(l))

W (Xite, y)dy

k=1

[tk−1 ,tk )

r

=

Φ(l+1)(Ba(l), Bk(l))(tk − tk−1)Si,k

k=1

r

=

Φ(l+1)(Ba(l), Bk(l))(tk − tk−1)Sπ(i),π(k)

k=1

(15)

29

r

=

Φ(l+1)(Bπ(l()a), Bπ(l()k))(tπ(k) − tπ(k)−1)Sπ(i),π(k)

k=1

r

=

Φ(l+1)(Bb(l), Bπ(l()k))(tπ(k) − tπ(k)−1)Sj,π(k)

k=1

r

=

Φ(l+1)(Bb(l), Bk(l))(tk − tk−1)Sj,k

k=1

r

=

Φ(l+1)(Bb(l), Bk(l))

W (Xjte, y)dy

k=1

[tk−1 ,tk )

=

W (Xjte, y)Φ(l+1)(f •(l)(Xjte), f •(l)(y))dy

[0,1]

= MW• Φ(l+1)(f •(l), f •(l)) (Xjte)

(16)

Here we use the fact that f •(l) f •(l) outputs the same value within each block, and Bk(l) = Bπ(l()k), ∀k ∈ {1, ..., r}.
We have shown f •(l+1)(Xite) = f •(l+1)(Xjte). And this proof applies for all Xite ∈ [ta−1, ta) (in block a), and the same conclusion holds. So f •(l+1) still outputs the same value within each block. Furthermore, Ba(l+1) = Bπ(l(+a1)) using the same proof technique. And this implies π ◦ B(l+1) = B(l+1).
Thus, Θ•W(T )(Xite) = f •(T )(Xite) = f •(T )(Xjte) = Θ•W(T )(Xjte).

Then we are ready to prove Corollary 2 by applying Corollary 1.

Corollary 2. Let Θ = ((Φ(l))Tl=1, (Ψ(l))Tl=1) be the MPNN with T layers and Θ•A(T ), Θ•W(T ) as in Theorem 1. Let η• : RFT × RFT → [0, 1] be as in Deﬁnition 8. Consider the SBM (W, f ) in Deﬁnition 6 with isomorphic blocks (Deﬁnition 7). Let (Gtr, F tr) ∼ (W, f ) and (Gte, F te) ∼
(W, f ) be the training and test graphs with N tr and N te nodes, respectively. Consider any two test
nodes i, j ∈ {1, ..., N te}, i = j, for which we can make a link prediction decision with η• (i.e.,

η•(Θ•A(tTe )(F te)i, Θ•A(T )(F te)j) = τ ). Let Gte be large enough to satisfy both Equation (1) and

√

N te >

2(C1 + C2 f ∞)

,

log(2N te/p) |η•(Θ•A(tTe )(F te)i, Θ•A(tTe )(F te)j ) − τ |/L•η(Mtr)

where p, C1, and C2 are as given in Corollary 1. Then, if i and j belong to isomorphic blocks (i.e.,

Θ•W(T )(f )(Xite) = Θ•W(T )(f )(Xjte)), with probability at least 1 −

T l=1

2(Hl

+

1)p

the

link

prediction

method in Deﬁnition 8 will make the same link prediction regardless of the SBM probability matrix

S (Deﬁnition 6) and whether i and j are in the same block or distinct isomorphic blocks.

Proof. To prove the corollary, we assume we have two nodes j and j , such that i and j are in the same block while i and j are in distinct isomorphic blocks. In the proof, we will show that the link prediction between i and j and the prediction between i and j will be the same.

First, from Corollary 1, since nodes j and j are in distinct isomorphic SBM blocks, when Equation (1)

is satisﬁed, we have with probability at least 1 − 2

T l=1

(Hl

+

1)p

Θ•A(tTe )(F te)j − Θ•A(tTe )(F te)j

2 ∞ ≤ (C1 + C2 f ∞)

log 2N te/p

√

.

N te

30

Then when the requirement for N te is satisﬁed,

η•(ΘA•(tTe )(F te)i, ΘA•(tTe )(F te)j ) − η•(Θ•A(tTe )(F te)i, Θ•A(tTe )(F te)j ) ∞

≤ L•η(Mtr) ΘA•(tTe )(F te)j − Θ•A(tTe )(F te)j

∞ ≤ L•η(Mtr)(C1 + C2

f

2 ∞)

< η•(ΘA•(tTe )(F te)i, Θ•A(tTe )(F te)j ) − τ ∞

log 2N te/p √
N te

If η•(ΘA•(tTe )(F te)i, Θ•A(tTe )(F te)j ) > τ , then
η•(Θ•A(tTe )(F te)i, ΘA•(tTe )(F te)j ) ≥ η•(ΘA•(tTe )(F te)i, ΘA•(tTe )(F te)j ) − |η•(Θ•A(tTe )(F te)i, Θ•A(tTe )(F te)j ) − η•(Θ•A(tTe )(F te)i, Θ•A(tTe )(F te)j )| > η•(ΘA•(tTe )(F te)i, ΘA•(tTe )(F te)j ) − |η•(Θ•A(tTe )(F te)i, Θ•A(tTe )(F te)j ) − τ | = η•(Θ•A(tTe )(F te)i, ΘA•(tTe )(F te)j ) − (η•(Θ•A(tTe )(F te)i, Θ•A(tTe )(F te)j ) − τ ) = τ

If η•(ΘA•(tTe )(F te)i, ΘA•(tTe )(F te)j ) < τ , then
η•(ΘA•(tTe )(F te)i, Θ•A(tTe )(F te)j ) ≤ η•(Θ•A(tTe )(F te)i, ΘA•(tTe )(F te)j ) + |η•(Θ•A(tTe )(F te)i, Θ•A(tTe )(F te)j ) − η•(Θ•A(tTe )(F te)i, Θ•A(tTe )(F te)j )| < η•(ΘA•(tTe )(F te)i, ΘA•(tTe )(F te)j ) + |η•(Θ•A(tTe )(F te)i, Θ•A(tTe )(F te)j ) − τ | = η•(ΘA•(tTe )(F te)i, ΘA•(tTe )(F te)j ) − (η•(Θ•A(tTe )(F te)i, ΘA•(tTe )(F te)j ) − τ ) = τ
So whether i and j are in the same block, or in distinct isomorphic SBM blocks, their prediction will be the same (both links have predictions larger than τ or less).

G Proof for pairwise gMPNN•• and cMPNN••

First we prove Lemma 2 showing W (x, y) is a stationary point in cMPNN••.
Lemma 2. If Φ(x, y) = y and Ψ(x, y) = x/y, then f ••(t)(x, y) = W (x, y), ∀x, y ∈ X is a stationary point in the cMPNN••, i.e. if f ••(t−1)(x, y) = W (x, y), then f ••(t)(x, y) = W (x, y), ∀x, y ∈ X.

Proof. If f ••(t−1)(x, y) = W (x, y), then

MW••(Φ(t)(f (t−1)))(x, y)

=

1 2

( W (y, z) Φ(t)(f ••(t−1)(x, y), f ••(t−1)(x, z)) X cW (x, y)

+ W (x, z) Φ(t)(f ••(t−1)(x, y), f ••(t−1)(y, z)))dµ(z) cW (x, y)

1 W (y, z)

W (x, z)

=(

W (x, z) +

W (y, z))dµ(z)

2 X cW (x, y)

cW (x, y)

1

=

W (x, z)W (y, z)dµ(z)

cW (x, y) X

= cW (x, y) = 1 cW (x, y)

Thus f ••(t)(x, y)

=

Ψ(t)(f ••(t−1)(x, y), MW••(Φ(t)(f ••(t−1), f ••(t−1)))(x, y))

=

W (x,y) 1

=

W (x, y).

We ﬁnish proving W (x, y) is a stanionary point in cMPNN••. There are inﬁnity choices of Φ and Ψ such that W (x, y) is a stanionary point.

Then we aim to prove Theorem 2, and the prove procedure should be very similar as Theorem 1.

31

G.1 Preparation

Following Maskey et al. [45, Lemma B.3], we propose the following lemma for cMPNN••. Using the same overall framing as Maskey et al. [45, Lemma B.3],
Lemma 5. Let (χ, d, µ) be a metric-measure space and W be a graphon s.t. Assumptions 1.1-3. are satisﬁed. Let Φ : R2F → RH be Lipschitz continuous with Lipschitz constant LΦ(Mtr). Consider a metric-space signal f •• : χ × χ → RF with f •• ∞ < ∞. Suppose that X1, . . . , XN are drawn i.i.d. from µ on χ, and let p ∈ (0, 1/H). Let x, y ∈ χ, and deﬁne the random variable

Yx•,•y

=

1 2N

N

W (y, Xi)Φ(f ••(x, y), f ••(x, Xi)) + W (x, Xi)Φ(f ••(x, y), f ••(y, Xi))

i=1

1 −

W (y, z)Φ(f ••(x, y), f ••(x, z))) + W (x, z)Φ(f ••(x, y), f ••(y, z)) dµ(z)

2X

on the sample space χN . Then, with probability at least 1 − Hp, we have

Yx•,•y

∞

≤

√ 2

(LΦ(Mtr)

f ••

∞

+ √

Φ(0,

0)

N

∞)

log 2/p .

(17)

Proof. The proof of the bound is the same as the proof in Maskey et al. [45, Lemma B.3], even though Yx,y is a different quantity than the quantity used in Maskey et al. [45, Lemma B.3].

Lemma 6. Let (χ, d, µ) be a metric-measure space and W be a graphon s.t. Assumptions 1.1-3. are satisﬁed. Let Φ : R2F → RH be Lipschitz continuous with Lipschitz constant LΦ(Mtr). Consider a metric-space signal f •• : χ × χ → RF with f •• ∞ < ∞. Suppose that X1, . . . , XN are drawn
i.i.d. from µ on χ, and let p ∈ (0, 1/H). Let x, y ∈ χ, and deﬁne the random variable

Tx•,•y

=

1 2N

N

A(y, Xi)Φ(f ••(x, y), f ••(x, Xi)) + A(x, Xi)Φ(f ••(x, y), f ••(y, Xi))

i=1

1N −
2N

W (y, Xi)Φ(f ••(x, y), f ••(x, Xi)) + W (x, Xi)Φ(f ••(x, y), f ••(y, Xi))

i=1

on the sample space X N × [0, 1]2N . Then, with probability at least 1 − Hp, we have

Tx•,•y

∞

≤

√ 2

(LΦ(Mtr)

f ••

∞

+ √

Φ(0,

0)

N

∞)

log 2/p .

(18)

Proof. The proof procedure is the same as Maskey et al. [45, Lemma B.3] where we use E[A(y, Xi)] = W (y, Xi) and E[A(x, Xi)] = W (x, Xi).

Lemma 7. Let (χ, d, µ) be a metric-measure space and W be a graphon s.t. Assumptions 1.1-3. are satisﬁed. Let Φ : R2F → RH be Lipschitz continuous with Lipschitz constant LΦ(Mtr). Consider a metric-space signal f •• : χ × χ → RF with f •• ∞ < ∞. Suppose that X1, . . . , XN are drawn
i.i.d. from µ on χ, and let p ∈ (0, 1/(2H)). Let x, y ∈ χ, and deﬁne the random variable

Rx••,y

=

1 2N

N

A(y, Xi)Φ(f ••(x, y), f ••(x, Xi)) + A(x, Xi)Φ(f ••(x, y), f ••(y, Xi))

i=1

1 −

W (y, z)Φ(f ••(x, y), f ••(x, z))) + W (x, z)Φ(f ••(x, y), f ••(y, z)) dµ(z)

2X

on the sample space χN × [0, 1]2N . Then, with probability at least 1 − 2Hp, we have

Rx••,y

∞

≤

√ 22

(LΦ

(Mtr)

f ••

∞

+ √

Φ(0,

0)

N

∞)

log 2/p .

(19)

32

Proof. Use the triangle inequality and the results from Lemmas 5 and 6.

Rx••,y ∞ = Tx•,•y + Yx•,•y ∞ ≤ Tx•,•y ∞ + Yx•,•y ∞.

From Yx•,•y

Lemmas 5 and 6, Tx•,•y ∞ ≤

∞

≤

√ 2

(LΦ

(Mtr

)

f ••

∞

+ √

Φ(0,0)

N

√ 2

(LΦ

(Mtr

)

f ••

√

∞

+ √

Φ(0,0)

∞)

log 2/p

w.p.

1 − Hp and

√

N

∞) log 2/p w.p. 1 − Hp. With probability at least 1 − 2Hp,

by intersecting the two events, we have

Rx••,y

∞

≤

√ 22

(LΦ(Mtr

)

f ••

∞

+ √

Φ(0,

0)

N

∞)

log 2/p .

Based on Lemma 7, we can prove the following corollary about the maximum concentration error for all pairs of nodes in the sampled graph G. Using the same overall framing as Maskey et al. [45, Lemma B.3],
Corollary 6. Let (χ, d, µ) be a metric-measure space and W be a graphon s.t. Assumptions 1.1-3. are satisﬁed. Let Φ : R2F → RH be Lipschitz continuous with Lipschitz constant LΦ(Mtr). Consider a metric-space signal f •• : χ × χ → RF with f •• ∞ < ∞. Suppose that X1, . . . , XN are drawn i.i.d. from µ on χ, and then edges Ai,j ∼ Ber(W (Xi, Xj)) i.i.d sampled. Let p ∈ (0, 1/2H), and deﬁne the random variable

RX••i ,Xj

=

1 2N

N

A(Xj, Xz)Φ(f ••(Xi, Xj), f ••(Xi, Xz))

z=1

+ A(Xi, Xz)Φ(f ••(Xi, Xj), f ••(Xj, Xz))

1 −
2

X

W (Xj, z)Φ(f ••(Xi, Xj), f ••(Xi, z)))

+ W (Xi, z)Φ(f ••(Xi, Xj), f ••(Xj, z)) dµ(z)

on the sample space χN × [0, 1]2N . Then, with probability at least 1 − 2Hp, we have

max
i,j=1,...,N

RX••i ,Xj

∞

≤

√ 22

(LΦ(Mtr)

f ••

∞+

Φ(0, 0) √

∞)

N

log(2N 2/p) .

(20)

Proof.

Using the result from Lemma 6, we have with probability 1 −

H N

p
2

,

TX••i ,Xj

∞

≤

√ 2

(LΦ(Mtr)

f ••

∞+

Φ(0, 0) √

∞)

N

log(2N 2/p) .

Using the union bound of the N 2 events that the above equations happens for x = X1, ..., XN and y = X1, ..., XN , with probability at least 1 − Hp, we have

max
i,j=1,...,N

TX••i ,Xj

∞

≤

√ 2

(LΦ(Mtr)

f ••

∞+

Φ(0, 0) √

∞)

N

log(2N 2/p) .

The same logic can be applied to YX••i,Xj , ∀i ∈ {1, ..., N }. Thus, using the triangle inequality, and the union bound of the two events, we have with probability at least 1 − 2Hp,

max
i,j=1,...,N

RX••i ,Xj

∞

≤

√ 22

(LΦ

(Mtr)

f ••

∞+

Φ(0, 0) √

∞)

N

log(2N 2/p) .

Following Maskey et al. [45, Lemma B.2], we also bound the maximum sampled-graph fraction of common neighbors cA(·, ·) under a condition of the sample size N . Using a same assumption as Maskey et al. [45, Lemma B.2],
33

Lemma 8. Let (χ, d, µ) be a metric-measure space and W be a graphon s.t. Assumptions 1.1-3. are satisﬁed. Suppose that X1, . . . , XN are drawn i.i.d. from µ on χ, and then edges Ai,j ∼ Ber(W (Xi, Xj)) i.i.d sampled. And let p ∈ (0, 1). If N ∈ N satisfy

√

√ log (2N 2/p)

N ≥4 2

.

(21)

dcmin

Then, with probability at least 1 − 2p, we have

√ log (2N 2/p)

max cA(Xi, Xj) − cW (Xi, Xj) ∞ ≤ 2 2 √

,

i,j=1,...,N

N

and

min cA(Xi, Xj)
i,j=1,...,N

≥

dcmin . 2

(22)

Proof. For given x, y ∈ X , deﬁne the random variable

1N

1N

cA(x, y) − cX (x, y) = N A(x, Xi)A(y, Xi) − N W (x, Xi)W (y, Xi)

i=1

i=1

on the sample space χN × [0, 1]2N . Using the same proof technique in Lemmas 5 to 7, we can prove with probability at least 1 − 2p, we have

√ log (2N 2/p)

max cA(Xi, Xj) − cW (Xi, Xj) ∞ ≤ 2 2 √

,

i,j=1,...,N

N

Since cW (Xi, Xj) ≥ dcmin, then with probability at least 1 − 2p, when Equation (21) is satisﬁed, we

have

min
i,j=1,...,N

cA(Xi, Xj )

≥

dcmin 2

Based on Lemma 8, we can prove a modiﬁed version of Maskey et al. [45, Lemma B.5]. Using a same overall framing as Maskey et al. [45, Lemma B.5],

Lemma 9. Let (χ, d, µ) be a metric-measure space and W be a graphon s.t. Assumptions 1.1-

3. are satisﬁed. Let Φ : R2F → RH be Lipschitz continuous with Lipschitz constant LΦ(Mtr).

Consider a metric-space signal f •• : χ × χ → RF with

f ••

∞

<

∞.

Let

p

∈

(0,

1 2(H +1)

),

and

let N ∈ N satisfy (21). Suppose that X1, . . . , XN are drawn i.i.d. from µ in χ, and then edges

Ai,j ∼ Ber(W (Xi, Xj)) i.i.d sampled. Then, condition (22) together with (23) below are satisﬁed in

probability at least 1 − 2(H + 1)p:

max
i,j=1,...,N

(MA•• − MW••) Φ(f ••, f ••) (Xi, Xj ) ∞

√

2 ≤4
√

log(2N 2/P )

√ N d2cmin

(LΦ(Mtr)

f ••

∞+

Φ(0, 0) ∞)

(23)

2 +

2(LΦ(Mtr) f •• ∞ +

Φ(0, 0) √

∞)

log(2N 2/P ) ,

dcmin N

Proof. The proof is the same as Maskey et al. [45, Lemma B.5]. The only difference is on the difference between Lemma 8 and Maskey et al. [45, Lemma B.2], and the difference between Corollary 6 and Maskey et al. [45, Lemma B.4].
Same as Maskey et al. [45, Lemma B.6], the layer-wise error for cMPNN•• and a gMPNN•• can be bounded. Using the same overall framing as Maskey et al. [45, Lemma B.6],

34

Corollary 7. Let (χ, d, µ) be a metric-measure space and W be a graphon s.t. Assumptions 1.1-

3. are satisﬁed. Let Φ : R2F → RH and Ψ : RF +H → RF be Lipschitz continuous with

Lipschitz constants LΦ(Mtr) and LΨ(Mtr). Consider a metric-space signal f •• : χ × χ → RF with

f ••

∞

<

∞.

Let p

∈

(0,

1 2(H +1)

),

and

let

N

∈

N satisfy (21).

Suppose that X1, . . . , XN

are

drawn i.i.d. from µ in χ, and then edges Ai,j ∼ Ber(W (Xi, Xj)) i.i.d sampled. Then, condition (22)

together with (24) below are satisﬁed in probability at least 1 − 2(H + 1)p,

max
i,j=1,...,N

Ψ f ••(·, ·), MA•• Φ(f ••, f ••) (Xi, Xj )

− Ψ f ••(·, ·), MW•• Φ(f ••, f ••) (Xi, Xj )

∞

√

≤ LΨ(Mtr) 4 √

2 log(2N 2/p)

√ N d2cmin

(LΦ(Mtr)

f ••

∞+

Φ(0, 0)

∞)

2 +

2(LΦ(Mtr) f ••

∞+

Φ(0, 0) √

∞)

log(2N 2/p) ,

dcmin N

(24)

Proof. The proof is the same as Maskey et al. [45, Lemma B.6]. The difference comes from the different bound in our Lemma 9 and the bound used by Maskey et al. [45, Lemma B.5].

G.2 Proof for Theorem 2

Finally, we can prove Theorem 2. The proof closely follows that of Maskey et al. [45, Theorem B.14], adapted to our setting. Using the same overall framing as Maskey et al. [45, Theorem B.14].

Theorem 2 (OOD convergence without in-distribution convergence). For a random graph model

(W, f ) satisfying Deﬁnition 1, let N tr be a random variable deﬁning the distribution of graph

sizes in training. Deﬁne the test distribution (Gte, F te) ∼ (W, f ) through the causal graph in

Figure 1 as an interventional change to obtain larger test graph sizes where min(supp(N te)) Mtr = max(supp(N tr)) (which means any test graph is much larger than the largest possible training

graph). Let Θ = ((Φ(l))Tl=1, (Ψ(l))Tl=1) be a MPNN as in Deﬁnition 2 with T layers such that Φ(l) and Ψ(l) that are learned from the training data and are Lipschitz continuous with Lipschitz constants

L(Φl)(Mtr) and L(Ψl)(Mtr). Let gMPNN•• Θ•W•(T ) and cMPNN•• Θ•W•(T ) be as in Deﬁnitions 9 and 10.

For a random graph model (W, f ) as in Deﬁnition 1 with√dcmin > 0. Let√X1te, ..., XNte te and Ate be as

in Deﬁnition 1. Let p ∈ (0,

T l=1

1 2(Hl

+1)

).

Then,

if

√

N te

log (2(N te)2/p)

≥

, 4 2
dcmin

we

have

with

probability

at least 1 −

T l=1

2(Hl

+

1)p,

δA•-•W

=
i,j

max
=1,...,N

te

Θ•A•(T )(F ••)i,j −Θ•W•(T )(f ••)(Xite, Xjte) ∞ ≤ (C3 +C4

f ••

∞)

log(2(N te)2/p)

√

,

N te

where the constants C3 and C4 are deﬁned in the Appendix and depend on {L(Φl)(Mtr), L(Ψl)(Mtr)}Tl=1 and the distribution of (Gtr, F tr).

Proof. In this case, Φ(l)(0, 0) ∞, Ψ(l)(0, 0) ∞ can be determined by (Gtr, F tr), N tr if the MPNN Θ has been trained on the training graph (Gtr, F tr).

Following the procedure of Maskey et al. [45, Appendix B.2] with Corollary 7, we can derive

similarly, with probability at least 1 − Tl=1(2Hl + 1)p,

δA••-W ≤

T

L(Ψl)(Mtr)

√ 2
4

l=1

log(2(N te)2/p) √
N ted2cmin

(L(Φl)

(Mtr)

f ••(l)

∞+

Φ(l)(0, 0) ∞)

+ 2√2(L(Φl)(Mtr) f ••(l) ∞ +

Φ(l)(0, 0) √

∞)

log(2(N te)2/p)

(25)

dcmin N te

l

T
((L(Ψl
=l+1

)(Mtr))2

+

8 d2cmin

(L(Φl

)(Mtr))2(L(Ψl

)(Mtr))2),

35

Using the same proof in Maskey et al. [45, Lemma B.9], we can derive

||f ••(l)||∞ ≤ B1••(l) + B2••(l)||f ||∞, where B1••(l), B2••(l) are independent of f ••, and

l+1
B1••(l+1) =
k=1

L(Ψk)(Mtr

)

1 dcmin

Φ(k)(0, 0)

∞+

l+1
L(Ψl )(Mtr)
l =k+1

1+

1 dcmin

L(Φl

)(Mtr)

Ψ(k)(0, 0) ∞

(26)

and

l+1

B2••(l+1) =

L(Ψk)(Mtr)

k=1

1

+

1 dcmin

L(Φk)

(Mtr)

.

(27)

Now we can decompose the summation in Equation (25). First, we deﬁnce C3 as

T

C3 =

L(Ψl)(Mtr)

l=1

√

4

2 d2cmin

(L(Φl)(Mtr)B1••(l)

+

Φ(l)(0, 0) ∞)

+ 2√2(L(Φl)(Mtr)B1••(l) + Φ(l)(0, 0) ∞)

dcmin

(28)

l

T
((L(Ψl
=l+1

)(Mtr))2

+

8 d2cmin

(L(Φl

)(Mtr))2(L(Ψl

)(Mtr))2),

Then we can deﬁne C4 as

T

C4 =

L(Ψl)(Mtr)

l=1

√

4

2 d2cmin

L(Φl)(Mtr)B2••(l)

+

2√2L(Φl)(Mtr)B2••(l) dcmin

l

T
((L(Ψl
=l+1

)(Mtr))2

+

8 d2cmin

(L(Φl

)(Mtr))2(L(Ψl

)(Mtr))2),

(29)

It is clear to see we can rewrite Equation (25) as

δA••-W ≤ (C3 + C4 f •• ∞)

log(2(N te)2/p)

√

.

N te

(30)

Thus C3 and C4 depends on {L(Φl)(Mtr)}Tl=1 and {L(Ψl)(Mtr)}Tl=1 and possibly on (Gtr, F tr) and N tr.

36

