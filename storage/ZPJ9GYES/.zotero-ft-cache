Certified Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing

arXiv:2002.03421v2 [cs.CR] 15 Sep 2020

Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong
Duke University {jinyuan.jia,binghui.wang,xiaoyu.cao,neil.gong}@duke.edu

ABSTRACT
Community detection plays a key role in understanding graph structure. However, several recent studies showed that community detection is vulnerable to adversarial structural perturbation. In particular, via adding or removing a small number of carefully selected edges in a graph, an attacker can manipulate the detected communities. However, to the best of our knowledge, there are no studies on certifying robustness of community detection against such adversarial structural perturbation. In this work, we aim to bridge this gap. Specifically, we develop the first certified robustness guarantee of community detection against adversarial structural perturbation. Given an arbitrary community detection method, we build a new smoothed community detection method via randomly perturbing the graph structure. We theoretically show that the smoothed community detection method provably groups a given arbitrary set of nodes into the same community (or different communities) when the number of edges added/removed by an attacker is bounded. Moreover, we show that our certified robustness is tight. We also empirically evaluate our method on multiple real-world graphs with ground truth communities.
KEYWORDS
Community Detection; Certified Robustness
ACM Reference Format: Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong. 2020. Certified Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing. In Proceedings of The Web Conference 2020 (WWW ’20), April 20–24, 2020, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3366423.3380029
1 INTRODUCTION
Graph is a powerful tool to represent many complex systems. For example, an online social network can be viewed as a graph, where nodes are users and edges represent friendships or interactions between users. Community detection is a basic tool to understand the structure of a graph and has many applications. For instance, communities in a social graph may represent users with common interests, locations, occupations, etc.. Therefore, many community detection methods (e.g., [2, 14, 16, 22–24, 29, 40]) have been proposed by various fields such as network science, applied physics, and
The first two authors made equal contributions.
This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. WWW ’20, April 20–24, 2020, Taipei, Taiwan © 2020 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License. ACM ISBN 978-1-4503-7023-3/20/04. https://doi.org/10.1145/3366423.3380029

bioinformatics. Roughly speaking, a community detection method divides the nodes in a graph into groups such that nodes in the same group are densely connected and nodes in different groups are sparsely connected.
However, multiple recent studies showed that community detection is vulnerable to adversarial structural perturbations [8, 9, 13, 28, 36]. Specifically, via adding or removing a small number of carefully selected edges in a graph, an attacker can manipulate the detected communities. For example, an attacker can spoof a community detection method to split a set of nodes, which are originally detected as in the same community, into different communities. An attacker can also spoof a community detection method to merge a set of nodes, which are originally detected as in different communities, into the same community. We call these two attacks splitting attack and merging attack, respectively. However, to the best of our knowledge, there are no studies to certify robustness of community detection against such adversarial structural perturbation. We note that several heuristic defenses [9, 28] were proposed to enhance the robustness of community detection against structural perturbation. However, these defenses lack formal guarantees and can often be defeated by strategic attacks that adapt to them.
In this work, we aim to bridge this gap. In particular, we aim to develop certified robustness of community detection against structural perturbation. Given an arbitrary community detection method, our techniques transform the method to a robust community detection method that provably groups a given arbitrary set of nodes into the same community (against splitting attacks) or into different communities (against merging attacks) when the number of edges added/removed by the attacker is no larger than a threshold. We call the threshold certified perturbation size.
Our robustness guarantees are based on a recently proposed technique called randomized smoothing [11, 20, 25], which is the state-of-the-art method to build provably robust machine learning methods. Specifically, given an arbitrary function f , which takes x as an input and outputs a categorial value. Randomized smoothing constructs a smoothed function д via adding random noise to the input x. Moreover, the output of the smoothed function is the function f ’s output that has the largest probability when adding random noise to the input x. Suppose an attacker can add a perturbation to the input x. The smoothed function provably has the same output once the perturbation added to the input x is bounded.
We propose to certify robustness of community detection using randomized smoothing. Specifically, given a graph, an arbitrary community detection method, and an arbitrarily set of nodes in the graph, we construct a function f , which takes the graph as an input and outputs 1 if the community detection method groups the set of nodes into the same community, otherwise the function f outputs 0. Then, we build a smoothed function д via adding random noise

WWW ’20, April 20–24, 2020, Taipei, Taiwan

Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong

to the graph structure, i.e., randomly adding or removing edges in the graph. Finally, we certify the robustness of the smoothed function д against adversarial structural perturbation.
However, existing randomized smoothing methods are insufficient to certify robustness of community detection. Specifically, they assume the input x is continuous and add Gaussian or Laplacian noise to it. However, graph structure is binary data, i.e., a pair of nodes can be connected or unconnected; and Gaussian or Laplacian noise is not semantically meaningful for such binary data. To address the challenge, we develop randomized smoothing for binary data. We theoretically derive a certified perturbation size via addressing several technical challenges. For instance, we prove a variant of the Neyman-Pearson Lemma [30] for binary data; and we divide the graph structure space into regions in a novel way such that we can apply the variant of the Neyman-Pearson Lemma to certify robustness of community detection. Moreover, we prove that our certified perturbation size is tight if no assumptions on the community detection method are made. Our certified perturbation size is the solution to an optimization problem. Therefore, we further design an algorithm to solve the optimization problem.
We empirically evaluate our method using multiple real-world graph datasets with ground-truth communities including Email, DBLP, and Amazon datasets. We choose the efficient community detection method called Louvain’s method [2]. We study the impact of various parameters on the certified robustness.
In summary, our key contributions are as follows:
• We develop the first certified robustness of community detection against adversarial structural perturbation. Moreover, we show that our certified robustness is tight.
• Our certified perturbation size is the solution to an optimization problem and we develop an algorithm to solve the optimization problem.
• We evaluate our method on multiple real-world datasets.
2 BACKGROUND 2.1 Community Detection
Suppose we are given an undirected graph G = (V , E), where V is the set of nodes and E is the set of edges. A community detection method divides the nodes in the graph into groups, which are called communities. In non-overlapping community detection, a node only belongs to one community, while in overlapping community detection, a node may belong to multiple communities. Formally, a community detection algorithm A takes a graph as an input and produces a set of communities C = {C1, C2, · · · , Ck }, where V = ∪ki=1Ci and Ci is the set of nodes that are in the ith community. For simplicity, we represent the graph structure as a binary vector x, where an entry of the vector represents the connection status of the corresponding pair of nodes. Specifically, an entry xi = 1 if the corresponding pair of nodes are connected, otherwise xi = 0. Moreover, we denote by n the length of the binary vector x. Therefore, we can represent community detection as C = A(x).
2.2 Attacks to Community Detection
Adversarial structural perturbation: We consider an attacker can manipulate the graph structure, i.e., adding or removing some

edges in the graph. In particular, an attacker may have control of
some nodes in the graph and can add or remove edges among them.
For instance, in a social graph, the attacker-controlled nodes may
be fake users created by the attacker or normal users compromised by the attacker. We denote by a binary vector δ the attacker’s perturbation to the graph, where δi = 1 if and only if the attacker changes the connection status of the corresponding pair of nodes. x ⊕ δ is the perturbed graph structure, where the operator ⊕ is the XOR between two binary variables. Moreover, we use ||δ ||0 to measure the magnitude of the perturbation because ||δ ||0 has semantic interpretations. In particular, ||δ ||0 is the number of edges added or removed by the attacker.
Two attacks: An attacker can manipulate the detected communities via adversarial structural perturbation [8, 9, 13, 28, 36]. Specifi-
cally, there are two types of attacks to community detection:
• Splitting attack. Given a set of nodes (called victim nodes) Γ = {u1, u2, · · · , uc } that are in the same community. A splitting attack aims to perturb the graph structure such that a community detection method divides the nodes in Γ into different communities. Formally, we have communities C′ = {C1, C2, · · · , Ck′ } = A(x ⊕ δ ′) after the attacker adds perturbation δ ′ to the graph structure, but there does not exist a community Ci such that Γ ⊂ Ci .
• Merging attack. Given a set of victim nodes Γ that are in different communities. A merging attack aims to perturb the
graph structure such that a community detection method groups the nodes in Γ into the same community. Formally, we have communities C′′ = {C1, C2, · · · , Ck′′ } = A(x ⊕ δ ′′) after the attacker adds perturbation δ ′′ to the graph structure, and there exists a community Ci such that Γ ⊂ Ci .
We aim to develop certified robustness of community detection
against the splitting and merging attacks.

2.3 Randomized Smoothing

Randomized smoothing is state-of-the-art method to build provably
secure machine learning methods [6, 11]. Suppose we are given a function f , which takes xˆ as an input and outputs a categorical value in a domain {1, 2, · · · , d}. Randomized smoothing aims to construct a smoothed function д via adding random noise ϵˆ to the input xˆ. Moreover, the output of the smoothed function is the output of the function f that has the largest probability when adding random noise to the input xˆ. Formally, we have:

д(xˆ) = argmax Pr(f (xˆ + ϵˆ) = yˆ),

(1)

yˆ ∈ {1,2, ··· ,d }

where ϵˆ is random noise drawn from a certain distribution. Suppose an attacker can add perturbation δˆ to the input xˆ. Existing studies [11, 20, 25] assume xˆ is continuous data. Moreover, they showed that, when the random noise is drawn from a Gaussian
distribution or Laplacian distribution, the smoothed function provably has the same output when the L2-norm or L1-norm of the perturbation δˆ is bounded. However, in our problem, the graph structure is binary data. Gaussian or Laplacian noise is not seman-
tically meaningful for such binary data. To address the challenge,
we will develop randomized smoothing for binary data and apply
it to certify robustness against the splitting and merging attacks.

Certified Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing

WWW ’20, April 20–24, 2020, Taipei, Taiwan

3 CERTIFIED ROBUSTNESS
3.1 Randomized Smoothing on Binary Data
We first construct a function f to model the splitting and merging
attacks. Specifically, given a graph whose structure we represent as a binary vector x, a community detection algorithm A, and a set of victim nodes denoted as Γ, the function f outputs 1 if the nodes in Γ are grouped into the same community detected by A and outputs 0 otherwise. Formally, we define f as follows:

f (x) = 1, if ∃i, Γ ⊂ Ci , where Ci ∈ A(x)

(2)

0, otherwise.

We simply use x as an input for the function f because we study
structural perturbation and other parameters–such as the commu-
nity detection algorithm A and the set of victim nodes Γ–can be assumed to be constants. An attacker adds a perturbation vector δ to the graph structure, i.e., x⊕δ is the perturbed structure. When the nodes in Γ are in the same community before attack (i.e., f (x) = 1) and f (x ⊕ δ) produces 0, a splitting attack succeeds. When the nodes in Γ are in different communities before attack (i.e., f (x) = 0) and f (x ⊕ δ) produces 1, a merging attack succeeds.
We construct a smoothed function д via adding random noise to the graph structure x. Specifically, we define a noise distribution in the discrete space {0, 1}n as follows:

Pr(ϵi = 0) = β, Pr(ϵi = 1) = 1 − β, ∀i ∈ {1, 2, · · · , n}, (3)
where n is the length of x and ϵi is the random binary noise added to the ith entry of x. Formally, x ⊕ ϵ is the noisy graph structure.
Our random noise means that the connection status (connected or
unconnected) of a pair of nodes is preserved with a probability β and changed with a probability 1 − β.
We note that the detected communities C = A(x⊕ϵ) are random since ϵ is random. Therefore, the output f (x⊕ϵ) is also random. The smoothed function д outputs the value that has a larger probability.
Formally, we have:

д(x) = argmax Pr(f (x ⊕ ϵ) = y)
y ∈ {0,1}

= 1, if Pr(f (x ⊕ ϵ) = 1) > 0.5

(4)

0, otherwise.

Certifying robustness against a splitting attack is to certify that
д(x ⊕ δ) = 1 for all ||δ ||0 ≤ L1, while certifying robustness against a merging attack is to certify that д(x ⊕ δ) = 0 for all ||δ ||0 ≤ L2. In other words, we aim to certify that д(x ⊕ δ) = y for all ||δ ||0 ≤ L, where y ∈ {0, 1} and L is called certified perturbation size.

3.2 Deriving Certified Perturbation Size
In this section, we derive the certified perturbation size of the smoothed function д theoretically for a given graph, community detection algorithm, and a set of victim nodes. In the next section, we will design algorithms to compute the certified perturbation size in practice. Our results can be summarized in the following two theorems.
Theorem 1 (Certified Perturbation Size). Given a graphstructure binary vector x, a community detection algorithm A, and a set of victim nodes Γ. The function f , random noise ϵ, and smoothed

function д are defined in Equation 2, 3, and 4, respectively. Assume there exists p ∈ [0, 1] such that:

Pr(f (x ⊕ ϵ) = y) ≥ p > 0.5,

(5)

where p is a lower bound of the probability p = Pr(f (x ⊕ ϵ) = y) that f outputs y under the random noise ϵ. Then, we have:

д(x ⊕ δ) = y, ∀||δ ||0 ≤ L,

(6)

where L is called certified perturbation size and is the solution to the following optimization problem:

L = argmax l,

(7)

s.t. ||δ ||0 = l,

(8)

µ −1

Pr(x ⊕ δ ⊕ ϵ ∈ Hi )

i =1

+ (p

−

µ −1
Pr(x
i =1

⊕ϵ

∈

Hi )) ·

Pr(x ⊕ δ ⊕ ϵ Pr(x ⊕ ϵ ∈

∈ Hµ ) Hµ )

>

0.5,

(9)

where we define region H (e)

=

{z

∈

{0, 1}n

:

Pr(x⊕ϵ =z) Pr(x⊕δ ⊕ϵ =z)

=

β 1−β

e } and density ratio h(e)

=

β 1−β

e , where e

=

−n, −n +

1, · · · , n − 1, n. We rank the regions H (−n), H (−n + 1), · · · , H (n) in

a descending order with respect to the density ratios h(−n), h(−n +

1), · · · , h(n). Moreover, we denote the ranked regions as H1, H2, · · · ,

H2n+1. Furthermore, µ is defined as follows:

µ′

µ = argmin µ′, s.t . Pr(x ⊕ ϵ ∈ Hi ) ≥ p

µ′ ∈ {1,2, ··· ,2n+1}

i =1

Proof. See Appendix B.

□

Next, we show that our certified perturbation size is tight.
Theorem 2 (Tightness of the Certified Perturbation Size). For any perturbation δ with ||δ ||0 > L, there exists a community detection algorithm A∗ (and thus a function f ∗) consistent with Equation 5 such that д(x ⊕ δ) y or there exists ties.

Proof. See Appendix C.

□

We have the following observations from our two theorems:
• Our certified perturbation size can be applied to any community detection method.
• Our certified perturbation size depends on p and β. When the probability lower bound p is tighter, our certified perturbation size is larger. We use the probability lower bound p instead of its exact value p because it is challenging to compute the exact value.
• When using the noise distribution defined in Equation 3 and no further assumptions are made on the community
detection algorithm, it is impossible to certify a perturbation size that is larger than L.

WWW ’20, April 20–24, 2020, Taipei, Taiwan

3.3 Computing Certified Perturbation Size
Given a graph-structure binary vector x, a community detection algorithm A, and a set of victim nodes Γ, we aim to compute the certified perturbation size in practice. We face two challenges. The first challenge is to estimate y and obtain the probability lower bound p.

The second challenge is how to solve the optimization problem in

Equation 7. To address the first challenge, we first estimate a value of y, and then use the one-sided Clopper-Pearson method [5] to

estimate the probability bound with probabilistic guarantees. To

address the second challenge, we develop an efficient algorithm to

solve the optimization problem.

Estimating y and p: We leverage a Monte-Carlo method to esti-

mate y and p with probabilistic guarantees. Specifically, we first

randomly sample N noise, and we use ϵ1, ϵ2, · · · , ϵN to denote

them. Then, we compute the frequency of the output 0 and 1

for the function f , i.e., m0 =

N i =1

I( f

(x

⊕

ϵi )

=

0)

and m1

=

N i =1

I(f (x

⊕

ϵi )

=

1),

where

I

is

an

indicator

function.

We

esti-

mate yˆ = argmaxi ∈{0,1} mi . Then, we estimate p by leveraging the

one-sided Clopper-Pearson method. Estimating p can be viewed

as estimating the parameter of a Binomial distribution. In partic-
ular, myˆ can be viewed as a sample from a Binomial distribution Bin(N , p), where myˆ is the frequency of the value yˆ and Bin(N , p) denotes a Binomial distribution with parameters N and p. Therefore, we can estimate p by leveraging the one-sided Clopper-Pearson

method. Specifically, we have:

p = B(α ; myˆ, N − myˆ + 1),

(10)

where 1−α represents the confidence level and B(α ; myˆ, N −myˆ +1) denotes the αth quantile of the beta distribution with parameters myˆ and N − myˆ + 1.
Solving the optimization problem: After obtaining the probability bound p, we solve the optimization problem in Equation 7
to obtain L. The key to solve the optimization problem is to compute Pr(x ⊕ ϵ ∈ H (e)) and Pr(x ⊕ δ ⊕ ϵ ∈ H (e)) for each e ∈ {−n, −n + 1, · · · , n} when ||δ ||0 = l. Specifically, we have:

min{n,n+e }

Pr(x ⊕ ϵ ∈ H (e)) =

βn−(i−e)(1 − β)(i−e) · θ (e, i) (11)

i=max{0,e }

min{n,n+e }

Pr(x ⊕ δ ⊕ ϵ ∈ H (e)) =

βn−i (1 − β)i · θ (e, i), (12)

i=max{0,e }

where θ (e, i) is defined as follows:

0,  θ (e, i) = 0,

if (e + l ) mod 2 0,

if 2i − e < l,

(13)

 n−l

 

2i −e −l

l l −e

,

otherwise

2

2

The calculation details can be found in Appendix D. Once we can

compute the probabilities Pr(x⊕ϵ ∈ H (e)) and Pr(x⊕δ ⊕ϵ ∈ H (e)),

we can iteratively find the largest l such that the constraint in

Equation 9 is satisfied. Such largest l is our certified perturbation

size L.

Complete certification algorithm: Algorithm 1 shows our complete certification algorithm. The function SampleUnderNoise randomly samples N noise from the noise distribution defined in Equation 3, adds each noise to the graph structure, and computes the

Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong
Algorithm 1: Certify Input: f , β, x, N , α. Output: ABSTAIN or (yˆ, L).
1 m0, m1 = SampleUnderNoise(f , β, x, N ) 2 yˆ = argmaxi ∈{0,1} mi 3 p = B(α ; myˆ, N − myˆ + 1) 4 if p > 0.5 then 5 L = CertifiedPerturbationSize(p) 6 return (yˆ, L) 7 else 8 return ABSTAIN

Table 1: Dataset statistics.

Dataset Email DBLP Amazon

#Nodes 1,005 317,080 334,863

#Edges 25,571 1,049,866 925,872

#Communities 42
13,477 75,149

frequency of the function f ’s output 0 and 1. Then, our algorithm estimates yˆ and p. Based on p, the function CertifiedPerturba-
tionSize computes the certified perturbation size by solving the optimization problem in Equation 7. Our algorithm returns (yˆ, L) if p > 0.5 and ABSTAIN otherwise. The following proposition shows
the probabilistic guarantee of our certification algorithm.

Proposition 1. With probability at least 1 − α over the randomness in Algorithm 1, if the algorithm returns an output value yˆ and a certified perturbation size L (i.e., does not ABSTAIN), then we have д(x ⊕ δ) = yˆ, ∀||δ ||0 ≤ L.

Proof. See Appendix E.

□

4 EVALUATION 4.1 Experimental Setup
Datasets: We consider three undirected graph datasets with “groundtruth” communities, i.e., Email, DBLP, and Amazon. Table 1 shows the datasets and their statistics. We obtained the datasets from SNAP (http://snap.stanford.edu/).
Email. Email dataset describes the communications between members in a research institution. The graph consists of 1,005 nodes, each of which represents a member in the institution; and 25,571 edges, indicating the email communications between members. The 42 departments in the institution are considered as the ground-truth communities and each node belongs to exactly one of them.
DBLP. DBLP dataset contains 317,080 nodes and 1,049,866 edges. Each node represents a researcher in the computer science field. If two researchers co-authored at least one paper, then there is an edge between them in the graph. Every connected component with no less than 3 nodes is marked as a community, which results in a total of 13,477 distinct communities.
Amazon. Amazon dataset is a network representing relations between products on Amazon website. The graph has 334,863 nodes and 925,872 edges. The nodes represent different products and

Certified Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing

WWW ’20, April 20–24, 2020, Taipei, Taiwan

Certiﬁed Accuracy

1.0

1.0

1.0

|Γ|=2

|Γ|=2

|Γ|=2

|Γ|=4

|Γ|=4

|Γ|=4

0.8

|Γ|=6

0.8

|Γ|=6

0.8

|Γ|=6

|Γ|=8

|Γ|=8

|Γ|=8

0.6

|Γ|=10

0.6

|Γ|=10

0.6

|Γ|=10

Certiﬁed Accuracy

Certiﬁed Accuracy

0.4

0.4

0.4

0.2

0.2

0.2

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(a) Email

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(b) DBLP

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(c) Amazon

Figure 1: Impact of the number of victim nodes |Γ| on defending against splitting attacks.

Certiﬁed Accuracy

1.0

1.0

1.0

0.8

0.8

0.8

Certiﬁed Accuracy

Certiﬁed Accuracy

0.6

0.4

|Γ|=2

|Γ|=4

|Γ|=6

0.2

|Γ|=8

|Γ|=10

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size

(a) Email

0.6

0.4

|Γ|=2

|Γ|=4

|Γ|=6

0.2

|Γ|=8

|Γ|=10

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size

(b) DBLP

0.6

0.4

|Γ|=2

|Γ|=4

|Γ|=6

0.2

|Γ|=8

|Γ|=10

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size

(c) Amazon

Figure 2: Impact of the number of victim nodes |Γ| on defending against merging attacks.

two products are connected if they are frequently bought together.
Overall, 75,149 communities are considered, each consisting of a
connected component within the same category of products.
Community detection algorithm: We use the popular Louvain’s method [2] to detect communities. The method optimizes modu-
larity in a heuristic and iterative way. We note that the method
produces communities in multiple hierarchical levels, and we take
the last level since in which the maximum of the modularity is attained. We use a publicly available implementation.1
Evaluation metric: We use certified accuracy as the metric to evaluate our certification method. We take defending against the
splitting attack as an example to illustrate certified accuracy. Suppose we are given M sets of victim nodes Γ1, Γ2, · · · , ΓM . The nodes in each victim set Γi are in the same ground-truth community. The goal of a splitting attack is to perturb the graph structure such that the Louvain’s method groups the victim nodes in a set Γi into at least two communities. Our certification algorithm in Algorithm 1 produces an output yi and a certified perturbation size Li for each victim set Γi . yi = 1 means that we can provably guarantee that the nodes in Γi are grouped into the same community. Given a perturbation size l, we define the certified accuracy CA(l) at the perturbation size l as the fraction of sets of victim nodes whose
1 https://sites.google.com/site/findcommunities/

output yi = 1 and certified perturbation size is at least l. Our certified accuracy CA(l) is the fraction of sets of victim nodes that our
method can provably detect as in the same community when an attacker adds or removes at most l edges in the graph. Formally,
we have:

Certified Accuracy for Defending against Splitting Attacks:

CA(l) =

M i =1

I(yi

= 1)I(Li M

≥

l) ,

(14)

where I is an indicator function. For merging attacks, the nodes in a victim set Γi are in different ground-truth communities. The
goal of a merging attack is to perturb the graph structure such
that the Louvain’s method groups the victim nodes in a set Γi into the same community. Given a perturbation size l, we define the certified accuracy CA(l) at the perturbation size l as the fraction of sets of victim nodes whose output yi = 0 and certified perturbation size is at least l. Our certified accuracy CA(l) is the fraction of sets

of victim nodes that our method can provably detect as in more than one communities when an attacker adds or removes at most l

edges in the graph. Formally, we have:

Certified Accuracy for Defending against Merging Attacks:

CA(l) =

M i =1

I(yi

=

0)I(Li

≥ l) .

M

(15)

WWW ’20, April 20–24, 2020, Taipei, Taiwan

Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong

Certiﬁed Accuracy

1.0

1.0

1.0

β=0.7

β=0.7

β=0.7

0.8

β=0.8 β=0.9

0.8

β=0.8 β=0.9

0.8

β=0.8 β=0.9

0.6

0.6

0.6

Certiﬁed Accuracy

Certiﬁed Accuracy

0.4

0.4

0.4

0.2

0.2

0.2

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(a) Email

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(b) DBLP

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(c) Amazon

Figure 3: Impact of the noise parameter β on defending against splitting attacks.

Certiﬁed Accuracy

1.0

1.0

1.0

N =1K

0.8

N =5K N =10K

0.8

0.8

0.6

0.6

0.6

Certiﬁed Accuracy

Certiﬁed Accuracy

0.4
0.2
0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(a) Email

0.4

N =1K

0.2

N =5K

N =10K

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size

(b) DBLP

0.4

N =1K

0.2

N =5K

N =10K

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size

(c) Amazon

Figure 4: Impact of the number of noise samples N on defending against splitting attacks.

Parameter setting: Our method has the following parameters: the noise parameter β, the confidence level 1 − α, and the number of samples N . Unless otherwise mentioned, we use the following default parameters: β = 0.7, 1 − α = 0.999, and N = 10, 000. To estimate certified accuracy for defending against splitting attacks, we randomly sample two sets of |Γ| nodes from each ground-truth community whose size is larger than |Γ|, and we treat them as victim sets. To estimate certified accuracy for defending against
merging attacks, we randomly sample 1,000 victim sets, each of which includes nodes randomly sampled from |Γ| different communities. By default, we assume each set of victim nodes includes 2 nodes, i.e., |Γ| = 2. We also study the impact of each parameter, including β, 1 − α, N , and |Γ|. When studying the impact of one parameter, we fix the remaining parameters to be their default values.
We randomly pick 100 nodes as attacker-controlled nodes for each
dataset, and the attacker perturbs the edges between them.
4.2 Experimental Results
Impact of the number of victim nodes |Γ|: Figure 1 shows the certified accuracy vs. perturbation size for defending against split-
ting attacks with different number of victim nodes on the three
datasets, while Figure 2 shows the results for defending against
merging attacks. We observe that as the number of victim nodes

increases, the curve of the certified accuracy becomes lower for splitting attacks and higher for merging attacks. This is because it is harder to provably guarantee that a larger set of nodes are detected as in the same community (defending against splitting attacks); and it is easier to provably guarantee that a larger set of nodes are detected as in more than one communities (defending against merging attacks).
Impact of the noise parameter β: Figure 3 shows the certified accuracy vs. perturbation size for defending against splitting attacks with different noise parameter β on the three datasets. We observe that β provides a tradeoff between normal accuracy without attacks and robustness. Specifically, when β is larger, the normal accuracy, i.e., certified accuracy at perturbation size 0, is larger, while the certified accuracy decreases more quickly as the perturbation size increases. We also have similar observations of the certified accuracy vs. perturbation size for defending against merging attacks, and thus we omit the results for simplicity.
Impact of the number of sampled noise N : Figure 4 shows the certified accuracy vs. perturbation size for defending against splitting attacks with different numbers of sampled noise N on the three datasets. We observe that the curve is higher as N increases. This is because a larger N makes the estimated probability bound p tighter and thus the certified perturbation size is also larger. We also

Certified Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing

WWW ’20, April 20–24, 2020, Taipei, Taiwan

Certiﬁed Accuracy Certiﬁed Accuracy Certiﬁed Accuracy

1.0

1.0

1.0

1 − α =99.0%

0.8

1 − α =99.9% 1 − α =99.99%

0.8

0.8

0.6

0.6

0.6

0.4
0.2
0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(a) Email

0.4

1 − α =99.0%

0.2

1 − α =99.9%

1 − α =99.99%

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size

(b) DBLP

0.4

1 − α =99.0%

0.2

1 − α =99.9%

1 − α =99.99%

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size

(c) Amazon

Figure 5: Impact of the confidence level 1 − α on defending against splitting attacks.

have similar observations of the certified accuracy vs. perturbation size for defending against merging attacks, and thus we omit the results for simplicity.
Impact of the confidence level 1−α: Figure 5 shows the certified accuracy vs. perturbation size for defending against splitting attacks with different confidence levels 1 − α on the three datasets. We observe that as the confidence level 1 − α increases, the curve of the certified accuracy becomes lower. The reason is that a higher confidence level causes a looser estimated probability bound p and thus the certified perturbation size is smaller. However, we note that the differences of the certified accuracies between different confidence levels are negligible when the confidence levels are large enough. We also have similar observations of the certified accuracy vs. perturbation size for defending against merging attacks, and thus we omit the results for simplicity.
5 RELATED WORK
Adversarial attacks to non-graph data and their defenses: For non-graph data, adversarial example is a well-known adversarial attack. Specifically, an attacker adds a carefully crafted perturbation to an input example such that a machine learning classifier makes predictions for the perturbed example as the attacker desires. The input example with carefully crafted perturbation is called adversarial example [17, 34]. Various empirical defenses (e.g., [17, 27, 31]) have been proposed to defend against adversarial examples. However, these defenses were often soon broken by adaptive attacks [1, 7].
In response, various certified defenses (e.g., [10, 15, 32, 33, 37]) against adversarial examples have been developed. Among these methods, randomized smoothing is state-of-the-art. Randomized smoothing turns an arbitrary classifier/function into a robust one via adding random noise to the input. Randomized smoothing was initially proposed as empirical defenses [6, 26] without formal certified robustness guarantees. For example, Cao & Gong [6] proposed to use uniform noise sampled from a hypercube centered at an input. Lecuyer et al. [20] derived the first certified robustness guarantee for randomized smoothing with Gaussian or Laplacian noise by utilizing differential privacy techniques. Li et al. [25] further improved the certified robustness bound by using techniques in information theory. In particular, they applied data processing inequality to

Rényi divergence and obtained certified robustness guarantee for Gaussian noise. Cohen et al. [11] proved the first tight bound for certified robustness under isotropic Gaussian noise. Jia et al. [18] derived a tight certified robustness of top-k predictions for randomized smoothing with Gaussian noise. In particular, they showed that a label is among the top-k labels predicted by the smoothed classifier when the adversarial perturbation is bounded. Our work uses randomized smoothing. However, different from the existing randomized smoothing methods, which assume continuous input and add continuous noise, we propose randomized smoothing on binary data and leverage it to certify robustness of community detection against splitting and merging attacks. We note that a concurrent work [21] generalized randomized smoothing to discrete data. The major difference between our approach and [21] is that we leverage a variant of the Neyman-Pearson Lemma to derive the certified perturbation size.
Adversarial attacks to graph data and their defenses: Compared to non-graph data, adversarial attacks to graph data and their defenses are much less studied. Adversarial structural perturbation is a recently proposed attack to graph data. For instance, several recent studies [3, 12, 35, 42, 43] showed that Graph Neural Networks (GNNs) are vulnerable to adversarial structural perturbations. Specifically, an attacker can slightly perturb the graph structure and/or node features to mislead the predictions made by GNNs. Some empirical defenses [38, 39, 41] were proposed to defend against such attacks. However, these methods do not have certified robustness guarantees. Zügner & Günnemann [44] developed the first certified robustness guarantee against node-feature perturbations for graph convolutional network [19]. Bojchevski & Günnemann [4] proposed the first method for verifying certifiable (non-)robustness of graph convolutional network against structural perturbations. These work is different from ours as we focus on certifying robustness of community detection.
Multiple studies [8, 9, 13, 28, 36] have shown that community detection is vulnerable to adversarial structural perturbation. Several heuristic defenses [9, 28] were proposed to enhance the robustness of community detection against adversarial structural perturbations. However, these defenses lack formal guarantees. Our work is the first certified robustness guarantee of community detection against adversarial structural perturbations.

WWW ’20, April 20–24, 2020, Taipei, Taiwan
6 DISCUSSION AND LIMITATIONS
Given a set of nodes that are in the same ground-truth community (or in different ground-truth communities), our certified robustness guarantees that the nodes are provably detected as in the same community (or in different communities) when the number of added or removed edges in the graph is at most a certain threshold (called certified perturbation size). We note that when we add or remove enough edges in a graph, the “ground-truth” communities may change, and thus we may expect the set of nodes to be detected as in different communities (or in the same community). Therefore, the certified perturbation size should not be too large. We believe it is an interesting future work to explore what certified perturbation size should be expected for a particular application scenario.
Our work shows that we can provably guarantee that a set of nodes are or are not in the same community when an attacker adds or deletes a bounded number of edges in the graph. However, it is still an open question on how to obtain communities from our smoothed community detection method. One possible way to obtain communities is as follows: we first randomly pick a node as the initial community C. For each remaining node, we compute the probability of the node being clustered into the same community with each node in C under randomized smoothing. Then, we compute the average probability and if it is larger than a threshold, we add the node to C. When no more nodes can be added to C, we randomly pick another node from the remaining nodes and repeat the above process until all nodes are clustered into certain communities. We believe it is an interesting future work to explore how to derive communities from the smoothed method. We note that the communities derived from the smoothed community detection method may be less accurate than those derived from the base community detection method. In other words, there may be a tradeoff between accuracy and robustness.
7 CONCLUSION
In this work, we develop the first certified robustness guarantee of community detection against adversarial structural perturbations. Specifically, our results show that a set of nodes can be provably detected as in the same community (against splitting attacks) or in different communities (against merging attacks) when the number of edges added or removed by an attacker is no larger than a threshold. Moreover, we show that our derived threshold is tight when randomized smoothing with our discrete noise is used. Our method can turn any community detection method to be provably robust against adversarial structural perturbation to defend against splitting and merging attacks. We also empirically demonstrate the effectiveness of our method using three real-world graph datasets with ground-truth communities. Interesting future work includes leveraging the information of the community detection algorithm to further improve the certified robustness guarantees and exploring what certified perturbation size should be expected for a particular application scenario. ACKNOWLEDGMENTS We thank the anonymous reviewers for insightful reviews. This work was supported by NSF grant No. 1937787 and No. 1937786.

Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong
REFERENCES
[1] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated Gradients
Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. In International Conference on Machine Learning. 274–283. [2] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment 2008, 10 (2008), P10008. [3] Aleksandar Bojchevski and Stephan Günnemann. 2019. Adversarial Attacks on Node Embeddings via Graph Poisoning. In ICML. [4] Aleksandar Bojchevski and Stephan Günnemann. 2019. Certifiable Robustness to Graph Perturbations. In Advances in Neural Information Processing Systems. 8317–8328.
[5] Lawrence D Brown, T Tony Cai, and Anirban DasGupta. 2001. Interval estimation for a binomial proportion. Statistical science (2001), 101–117.
[6] Xiaoyu Cao and Neil Zhenqiang Gong. 2017. Mitigating evasion attacks to deep neural networks via region-based classification. In Proceedings of the 33rd Annual Computer Security Applications Conference. ACM, 278–287.
[7] Nicholas Carlini and David Wagner. 2017. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, 3–14.
[8] Jinyin Chen, Lihong Chen, Yixian Chen, Minghao Zhao, Shanqing Yu, Qi Xuan, and Xiaoniu Yang. 2019. GA-Based Q-Attack on Community Detection. IEEE Transactions on Computational Social Systems 6, 3 (2019), 491–503.
[9] Yizheng Chen, Yacin Nadji, Athanasios Kountouras, Fabian Monrose, Roberto
Perdisci, Manos Antonakakis, and Nikolaos Vasiloglou. 2017. Practical attacks against graph-based clustering. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. ACM, 1125–1142. [10] Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. 2017. Maximum resilience of artificial neural networks. In ATVA. [11] Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. 2019. Certified adversarial robustness via randomized smoothing. In ICML. [12] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. 2018. Adversarial attack on graph structured data. In ICML. [13] Valeria Fionda and Giuseppe Pirro. 2017. Community deception or: How to stop fearing community detection algorithms. IEEE Transactions on Knowledge and Data Engineering 30, 4 (2017), 660–673. [14] Santo Fortunato and Marc Barthelemy. 2007. Resolution limit in community detection. Proceedings of the national academy of sciences 104, 1 (2007), 36–41. [15] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin Vechev. 2018. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In IEEE S & P. [16] Michelle Girvan and Mark EJ Newman. 2002. Community structure in social and biological networks. Proceedings of the national academy of sciences 99, 12 (2002), 7821–7826.
[17] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and harnessing adversarial examples. In International Conference on Learning Representations.
[18] Jinyuan Jia, Xiaoyu Cao, Binghui Wang, and Neil Zhenqiang Gong. 2020. Cer-
tified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing. In International Conference on Learning Representations. [19] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. In ICLR. [20] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
Jana. 2019. Certified robustness to adversarial examples with differential privacy. In IEEE S & P. [21] Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi Jaakkola. 2019. Tight certificates of adversarial robustness for randomly smoothed classifiers. In Advances in Neural Information Processing Systems. 4911–4922. [22] Jure Leskovec, Kevin J Lang, Anirban Dasgupta, and Michael W Mahoney. 2008.
Statistical properties of community structure in large social and information networks. In Proceedings of the 17th international conference on World Wide Web. ACM, 695–704.
[23] Jure Leskovec, Kevin J Lang, Anirban Dasgupta, and Michael W Mahoney. 2009.
Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters. Internet Mathematics 6, 1 (2009), 29–123. [24] Jure Leskovec, Kevin J Lang, and Michael Mahoney. 2010. Empirical comparison of algorithms for network community detection. In Proceedings of the 19th international conference on World wide web. ACM, 631–640. [25] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. 2019. Second-order adversarial attack and certifiable robustness. NeurIPS (2019). [26] Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. 2018. Towards robust neural networks via random self-ensemble. In Proceedings of the European Conference on Computer Vision (ECCV). 369–385. [27] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations.

Certified Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing

WWW ’20, April 20–24, 2020, Taipei, Taiwan

[28] Shishir Nagaraja. 2010. The impact of unlinkability on adversarial community detection: effects and countermeasures. In International Symposium on Privacy Enhancing Technologies Symposium. Springer, 253–272.
[29] Mark EJ Newman. 2006. Modularity and community structure in networks. Proceedings of the national academy of sciences 103, 23 (2006), 8577–8582.
[30] Jerzy Neyman and Egon Sharpe Pearson. 1933. IX. On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 231, 694-706 (1933), 289–337.
[31] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
2016. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 582–597. [32] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018. Certified defenses against adversarial examples. In ICLR. [33] Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker. 2015. Towards Verification of Artificial Neural Networks.. In MBMV. [34] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks. In ICLR. [35] Binghui Wang and Neil Zhenqiang Gong. 2019. Attacking Graph-based Classification via Manipulating the Graph Structure. In CCS. [36] Marcin Waniek, Tomasz P Michalak, Michael J Wooldridge, and Talal Rahwan. 2018. Hiding individuals and communities in a social network. Nature Human Behaviour 2, 2 (2018), 139. [37] Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane
Boning, Inderjit S Dhillon, and Luca Daniel. 2018. Towards fast computation of certified robustness for relu networks. In ICML. [38] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming
Zhu. 2019. Adversarial Examples on Graph Data: Deep Insights into Attack and Defense. In IJCAI. [39] Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong,
and Xue Lin. 2019. Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective. In IJCAI. [40] Jaewon Yang and Jure Leskovec. 2015. Defining and evaluating network communities based on ground-truth. Knowledge and Information Systems 42, 1 (2015), 181–213.
[41] Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2019. Robust Graph Convolutional Networks Against Adversarial Attacks. In KDD.
[42] Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann. 2018. Adversarial attacks on neural networks for graph data. In KDD.
[43] Daniel Zügner and Stephan Günnemann. 2019. Adversarial attacks on graph neural networks via meta learning. ICLR (2019).
[44] Daniel Zügner and Stephan Günnemann. 2019. Certifiable Robustness and Robust Training for Graph Convolutional Networks. In KDD.

A A VARIANT OF THE NEYMAN-PEARSON LEMMA
We prove a variant of the Neyman-Pearson Lemma [30] that is used to derive our certified perturbation size.

Lemma 1. Assume X and Y are two random variables in the discrete

space {0, 1}n with probability distributions Pr(X ) and Pr(Y ), respec-

tively. Let ψ : {0, 1}n → {0, 1} be a random or deterministic function.

Let T1

=

{z

∈

{0, 1}n

:

Pr(X =z) Pr(Y =z)

>

t } and T2

=

{z

∈

{0, 1}n

:

Pr(X =z) Pr(Y =z)

=

t } for some t

>

0. Assume T3

⊆

T2

and T

= T1

T3. If

Pr(ψ (X ) = 1) ≥ Pr(X ∈ T ), then Pr(ψ (Y ) = 1) ≥ Pr(Y ∈ T ).

Proof. In general, we assume that ψ is a random function. We
use ψ (0|z) to denote the probability that ψ (z) = 1 and ψ (0|z) to denote the probability that ψ (z) = 0, respectively. We use T c to represent the complement of T , i.e., T c = {0, 1}n \ T .

Pr(ψ (Y ) = 1) − Pr(Y ∈ T )

(16)

=

ψ (1|z)Pr(Y = z) − Pr(Y = z)

(17)

z∈ {0,1}n

z ∈T

=

ψ (1|z)Pr(Y = z) + ψ (1|z)Pr(Y = z)

(18)

z∈T c

z ∈T

− ψ (1|z)Pr(Y = z) + ψ (0|z)Pr(Y = z)

(19)

z ∈T

z ∈T

= ψ (1|z)Pr(Y = z) − ψ (0|z)Pr(Y = z)

(20)

z∈T c

z ∈T

1

≥t

ψ (1|z)Pr(X = z) − ψ (0|z)Pr(X = z)

z∈T c

z ∈T

(21)

1

=t

ψ (1|z)Pr(X = z) + ψ (1|z)Pr(X = z)

z∈T c

z ∈T

(22)

− ψ (1|z)Pr(X = z) + ψ (0|z)Pr(X = z)

(23)

z ∈T

z ∈T

1

=t

ψ (1|z)Pr(X = z) − Pr(X = z)

z∈ {0,1}n

z ∈T

(24)

1

= t Pr(ψ (X ) = 1) − Pr(X ∈ T )

(25)

≥ 0.

(26)

We obtain Equation 21 from 20 because for any z ∈ T , we have

Pr(X =z) Pr(Y =z)

≥ t, and for any z

∈ T c , we have

Pr(X =z) Pr(Y =z)

≤

t. We have

Equation 26 from 25 because Pr(ψ (X ) = 1) ≥ Pr(X ∈ T ).

□

B PROOF OF THEOREM 1

We define the following two random variables:

X =x⊕ϵ

(27)

Y = x ⊕ δ ⊕ ϵ,

(28)

where the random variables X and Y denote random samples by
adding discrete noise to the graph-structure binary vector x and its perturbed version x + δ, respectively. Our goal is to find the maximum perturbation size ||δ ||0 such that the following holds:

Pr(f (Y ) = y) > 0.5

(29)

First, we define region Q such that Pr(X ∈ Q) = p. Specifically, we
gradually add the regions H1, H2, · · · , H2n+1 to Q until Pr(X ∈ Q) = p. In particular, we define µ as follows:

µ′

µ = argmin µ′, s.t . Pr(X ∈ Hi ) ≥ p (30)

µ′ ∈ {1,2, ··· ,2n+1}

i =1

Moreover, we define Hµ as any subregion of Hµ such that:

µ −1

Pr(X ∈ Hµ ) = p − Pr(X ∈ Hi )

(31)

i =1

Then, we define the region Q as follows:

µ −1

Q = Hi ∪ Hµ

(32)

i =1

Based on the conditions in Equation 5, we have:

Pr(f (X ) = y) ≥ p = Pr(X ∈ Q)

(33)

We define a function ψ (z) = I(f (z) = y). Then, we have:

Pr(ψ (X ) = y) = Pr(f (X ) = y) ≥ Pr(X ∈ Q).

(34)

WWW ’20, April 20–24, 2020, Taipei, Taiwan

Moreover,

we

have

Pr(X =z) Pr(Y =z)

>

hµ

if

and only if z

∈

µ −1 j =1

Hj

,

and

Pr(X =z) Pr(Y =z)

=

hµ

for

any

z

∈

Hµ ,

where hµ

is

the

probability

density

ratio in the region Hµ . Note that Q =

µ −1 i =1

Hi

∪

Hµ .

Therefore,

according to Lemma 1, we have:

Pr(f (Y ) = y) ≥ Pr(Y ∈ Q).

(35)

Therefore, to reach our goal, it is sufficient to have:

Pr(Y ∈ Q) > 0.5 ⇐⇒

(36)

Pr(Y ∈ Q)

(37)

µ −1

=Pr(Y ∈ Hi ∪ Hµ )

(38)

i =1

µ −1

µ −1

= Pr(Y ∈ Hi ) + (p − Pr(X ∈ Hi ))/hµ

(39)

i =1

i =1

>0.5,

(40)

where hµ is the probability density ratio in the region Hµ . Our certified perturbation size L is the largest perturbation size ||δ ||0
that makes the above inequality hold.

C PROOF OF THEOREM 2
Our idea is to construct a base function f ∗ consistent with the con-
ditions in Equation 5, but the smoothed function is not guaranteed
to predict y. Let region Q be defined as in the proof of Theorem 1 We denote Qc as the complement of Q, i.e., Qc = {0, 1}n \ Q. Given region Q, we construct the following function:

f ∗(z) = y 1−y

if z ∈ Q if z ∈ Qc

(41)

By construction, we have Pr(f ∗(X ) = y) = p and Pr(f ∗(X ) = 1 − y) = 1 − p, which are consistent with Equation 5. From our proof of Theorem 1, we know that when ||δ ||0 > L, we have:

Pr(Y ∈ Q) ≤ 0.5,

(42)

Moreover, based on the definition of f ∗, we have:

Pr(f ∗(Y ) = y) ≤ Pr(f ∗(Y ) = 1 − y)

(43)

Therefore, we have either д(x ⊕ δ) y or there exists ties when ||δ ||0 > L.

D COMPUTING Pr(x ⊕ ϵ ∈ H (e)) AND Pr(x ⊕ δ ⊕ ϵ ∈ H (e))

We first define the following regions:

H (a, b) = {z ∈ {0, 1}n : ||z − x||0 = a and

(44)

||z − x ⊕ δ ||0 = b},

(45)

for a, b ∈ {0, 1, · · · , n}. Intuitively, H (a, b) includes the binary vectors that are a bits different from x and b bits different from x ⊕δ. Next, we compute the size of the region H (a, b) when ||δ ||0 = l. Without loss of generality, we assume x = [0, 0, · · · , 0] as a zero vector and x ⊕ δ = [1, 1, · · · , 1, 0, 0, · · · , 0], where the first l entries are 1 and the remaining n − l entries are 0. We construct a binary vector z ∈ H (a, b). Specifically, suppose we flip i zeros in the last n−l zeros in both x and x⊕δ. Then, we flip a −i of the first l bits of x

Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong

and flip the rest l −a +i bits of the first l bits of x ⊕δ. In order to have z ∈ H (a, b), we need l −a +i +i = b, i.e., i = (a +b −l)/2. Therefore, we have the size |H (a, b)| of the region H (a, b) as follows:

0,   |H (a, b)| = 0,

if (a + b − l) mod 2 0,

if a + b < l,

(46)

 n−l

  

a +b −l

l
a −b +l

,

otherwise

2

2



Moreover, for each z ∈ H (a, b), we have Pr(x ⊕ ϵ = z) = βn−a (1 −

β)a and Pr(x ⊕ δ ⊕ ϵ = z) = βn−b (1 − β)b . Therefore, we have:

Pr(x ⊕ ϵ ∈ H (a, b)) = βn−a (1 − β)a · |H (a, b)| (47)

Pr(x ⊕ δ ⊕ ϵ ∈ H (a, b)) = βn−b (1 − β)b · |H (a, b)| (48)

Note that H (e) = ∪b−a=e H (a, b). Therefore, we have:

Pr(x ⊕ ϵ ∈ H (e))

(49)

=Pr(x ⊕ ϵ ∈ ∪b−a=e H (a, b))

(50)

=Pr(x ⊕ ϵ ∈ ∪im=imn(anx,(n0+,ee))H (i − e, i))

(51)

min(n, n +e )

=

Pr(x ⊕ ϵ ∈ H (i − e, i))

(52)

i =max(0, e )

min(n, n +e )

=

βn−(i−e)(1 − β)i−e · |H (i − e, i)|

(53)

i =max(0, e )

min(n, n +e )

=

βn−(i−e)(1 − β)i−e · θ (e, i),

(54)

i =max(0, e )

where θ (e, i) = |H (i − e, i)|.

Similarly, we have

Pr(x ⊕ δ ⊕ ϵ ∈ H (e))

(55)

min(n, n +e )

=

βn−i (1 − β)i · θ (e, i).

(56)

i =max(0, e )

E PROOF OF PROPOSITION 1

Based on the Clopper-Pearson method, we know the probability that the following inequality holds is at least 1 − α over the randomness in sampling the noise:

Pr(f (x + ϵ) = yˆ) ≥ p

(57)

Therefore, with the estimated probability lower bound p, we can invoke Theorem 1 to obtain the robustness guarantee if p > 0.5.

Note that otherwise our algorithm Certify abstains.

