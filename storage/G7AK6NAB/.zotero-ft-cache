Expressive and Interpretable Graph Neural Networks
Pan Li
10/03/2022 Talk at FastML workshop
1

Collaborators

Siqi Miao

Daniel F. Guerrero

Mia Liu

Jacobo Konigsberg Zhenbin Wu

Tianchun Li

Shikun Liu

Yongbin Feng

Lisa Paspalaki

Nhan Tran

Yanbang Wang

Hongwei Wang

Jure Leskovec

Yunan Luo
2

Deep Learning on Graphs in Science
â€¢ Protein folding
[Senior et al., Nature 2019] [Jumper et al., Nature 2021]
â€¢ Simulation of glass dynamics
[Baspt et al, Nature Physics 2021]
â€¢ Molecular Property Prediction â€¢ Jet Tagging in HEP

[Duvenaud et al., NeurIPS 2015]

Refined based on [Qu, Li, Qian, 2022] 3

Graph Neural Networks
Graph Data ð´, ð‘‹ : the adjacency matrix ð´, possibly with node attributes ð‘‹.

â„Ž)(#)

â„Ž'(#)

0.1 0.6 â€¦

e

Node (feature) representation
â€¢ Transformation of node attributes

d â„Ž!(#) a

f â„Ž&(#)

â„Ž%(#) c

b â„Ž((#)

4

Graph Neural Networks
Graph Data ð´, ð‘‹ : the adjacency matrix ð´, possibly with node attributes ð‘‹.

â„Ž)(#) d

â„Ž'(#)

0.1 0.6 â€¦

e

â„Ž!(#) a

f â„Ž&(#)

â„Ž%(#) c

b â„Ž((#)

Update (A feed-forward NN)
â„Ž((*)b

ð‘“+,)!-'(â€¦ )

ð‘“!..(â€¦ ) Sum or

â€¦

mean or

max

â„Ž!(#) a â„Ž%(#) c f

Graph neural network: one layer
Aggregation (sum, mean, max pooling, attention, etc.)

â„Ž!(#$%) = ð‘“'()*#+ â„Ž!# , ð‘“*,, {â„Ž'# ð‘¢ âˆˆ ð‘!} , where ð‘! denotes the set of the neighbors of node ð‘£.
5

Graph Neural Networks

Graph Data ð´, ð‘‹ : the adjacency matrix ð´, possibly with node attributes ð‘‹.

â„Ž)(*) d

â„Ž'(*)

0.2 0.5 â€¦

e

â„Ž!(*) a

f â„Ž&(*)

â„Ž%(*) c

b â„Ž((*)

â„Ž((,) a

â„Ž((*) a

â„Ž.(/) b

ce

â„Ž-(,) c f a b d a f â„Ž0(,)

Make prediction
1. [node level] Use node representations separately to predict node labels
2. [graph level] Aggregate all node representations to predict the graph label
â„Ž- = POOL â„Ž!(.) âˆ£ ð‘£ âˆˆ ð‘‰

â„Ž!(#$%) = ð‘“'()*#+ â„Ž!# , ð‘“*,, {â„Ž'# ð‘¢ âˆˆ ð‘!} , where ð‘! denotes the set of the neighbors of node ð‘£.
6

Limitations of GNNs in Science
â€¢ Limited Expressive Power â€¢ Fail to represent some relations between input features and labels
â€¢ Hard to Interpret â€¢ Complicated architectures â€¢ Capture spurious correlations not effective patterns
â€¢ Subpar Generalization â€¢ Performance drop due to distribution shifts (simulation-based training -> real-data testing)
7

Limitations of GNNs in Science

â€¢ Limited Expressive Power â€¢ Fail to represent some relations between input features and labels

â€¢ Hard to Interpret â€¢ Complicated architectures â€¢ Capture spurious correlations not effective patterns

â€¢ Subpar Generalization

â€¢ Performance drop due to distribution shifts (simulation-based training -> real-data testing)

Image source: HOW CMS WEEDS OUT PARTICLES THAT PILE UP
Observed in

Please check this with Shikun Liu on Wednesday.

pileup mitigation

8 Li et al., Semi-supervised Graph Neural Networks for Pileup Noise Removal, Neurips AI4Science workshop, 2021

Limitations of GNNs in Science
â€¢ Limited Expressive Power â€¢ Fail to represent some relations between input features and labels
â€¢ Hard to Interpret â€¢ Complicated architectures â€¢ Capture spurious correlations not effective patterns
â€¢ Subpar Generalization â€¢ Performance drop due to distribution shifts (simulation-based training -> real-data testing)
9

Expressive Power

â€¢ The target function ð‘“: ð’³ â†’ ð’´ --- unknown â€¢ A model ð‘“!: ð’³ â†’ ð’´ --- ðœƒ denotes the parameters
Can we expect sup |ð‘“ ð‘‹ âˆ’ ð‘“! ð‘‹ | to be small for some ðœƒ?
"âˆˆð’³
For regular inputs ð’³ = â„) and a fully-connected feedforward ð‘“/ For graph inputs ð’³ = ð’¢ = {0,1}0Ã—0 and a GNN ð‘“/

Hornik et al., Multilayer Feedforward Networks are Universal Approximators, Neural Networks, 1989

Xu et al., How powerful are graph neural networks? ICLR 2019,

10

Expressive Power for Graph-level Tasks
GNNs predict graph-level properties:

â€¢ Solubility

â€¢ Toxicity

GNN

â€¢ HOMO-LUMO
energy gap

â€¢ effectiveness to certain disease

An illustration showing the different stages involved in developing a drug. Image credit: Genome Research Limited

11

Expressive Power for Graph-level Tasks
GNNs fail in many cases. E.g., fail to give predictions of any different properties regarding the following molecules

Decalin

GNNs yield same prediction

limited expressive power

Bicyclopentyl

12

Expressive Power for Graph-level Tasks
Node attributes
â€œone carbon atom with two hydrogen atomsâ€ as a node
13

Expressive Power for Graph-level Tasks
â„Ž"($%&) = ð‘“()*+$, â„Ž"$ , ð‘“+-- {â„Ž($ ð‘¢ âˆˆ ð‘"}
Iteration I Iteration II
14

Expressive Power for Graph-level Tasks

â„Ž"($%&) = ð‘“()*+$, â„Ž"$ , ð‘“+-- {â„Ž($ ð‘¢ âˆˆ ð‘"}
Iteration I

Iteration II

â€¦
Predict based on â„Ž- = POOL â„Ž!(.) âˆ£ ð‘£ âˆˆ ð‘‰

The multi-sets of colors (node representations) on two graphs keep the same

No valid predictionsâ€¦

15

Expressive Power for Graph-level Tasks
â€¢ Too symmetric? This is not an extreme caseâ€¦ â€¢ Consider ð´ âˆˆ â„%Ã—%, ð‘“ ð´ = ð‘¡ð‘Ÿð‘Žð‘ð‘’(ð´)) â€¢ If let ð´ âˆˆ 0,1 %Ã—% represent a graph, ð´'( = 1 if (ð‘¢, ð‘£) is an edge, ð‘“ ð´ outputs the number of 3-cycles.
16

Expressive Power for Graph-level Tasks
â€¢ Too symmetric? This is not an extreme caseâ€¦ â€¢ Consider ð´ âˆˆ â„%Ã—%, ð‘“ ð´ = ð‘¡ð‘Ÿð‘Žð‘ð‘’(ð´)) â€¢ If let ð´ âˆˆ 0,1 %Ã—% represent a graph, ð´'( = 1 if (ð‘¢, ð‘£) is an edge, ð‘“ ð´ outputs the number of 3-cycles. â€¢ Consider a GNN ð‘“!(â‹…).
Have different numbers of 3-cycles while GNNs give them the same prediction
v.s.
17

Expressive Power for Graph-level Tasks

â€¢ Too symmetric? This is not an extreme caseâ€¦
â€¢ Consider ð´ âˆˆ â„%Ã—%, ð‘“ ð´ = ð‘¡ð‘Ÿð‘Žð‘ð‘’(ð´))
â€¢ If let ð´ âˆˆ 0,1 %Ã—% represent a graph, ð´'( = 1 if (ð‘¢, ð‘£) is an edge, ð‘“ ð´ outputs the number of 3-cycles.
â€¢ Consider a GNN ð‘“!(â‹…). ð‘“!(â‹…) cannot approximate ð‘“ â‹… â€¢ A lot of input ð´â€™s may cause such errors

Error for ð´ âˆˆ â„0Ã—0 : |ð‘“/ ð´ âˆ’ ð‘“ ð´ |

Error is expanded ð´

Note that ð‘“. is continuous

18

Solutions for Graph-level Expressive Power
19

Solutions for Graph-level Expressive Power
Let us consider the 0-1 case: ð´ âˆˆ 0,1 %Ã—% (a graph without weights on edges)

One key idea: Injecting structural (e.g., distance) features

u

u v

v
For any node u, there is at most one node v whose shortest path distance to u is 5.

v
There exists a node u such that there are two nodes whose shortest path distance to u are 5.

20 Li et al., Distance encoding: Design provably more powerful neural networks for graph representation learning, NeurIPS 2020

Solutions for Graph-level Expressive Power

Let us consider the 0-1 case: ð´ âˆˆ 0,1 %Ã—%

u

u

â€¢ Build a new fully connected graphs (transformers) â€¢ Use distance over the original graph as edge features
on the new graph
Graphomer achieves top-1 in KDD Cup's 2021 to predict molecular properties
21 Ying et al., Do Transformers Really Perform Bad for Graph Representation? NeurIPS 2021

Solutions for Graph-level Expressive Power

How about the case when ð´ âˆˆ â„%Ã—%?

u

u

[ ð´+ (", ð´+/ (", ð´+0 (", â€¦ ] Use as extra edge features

â€¢ Build a new fully connected graph (transformers)

â€¢

Use [

ð´?

,
'(

ð´?*

,
'(

ð´?)

,
'(

â€¦

]

as

edge

features

for

(u,v)

ð´+: Adding some row/column normalization is good for numerical stability

A more general structural feature
22 Chen et al., on the equivalence between graph isomorphism testing and function approximation with gnns. NeurIPS 2019

Solutions for Graph-level Expressive Power

How about the case when ð´ âˆˆ â„%Ã—%?

u

u

[ ð´+ (", ð´+/ (", ð´+0 (", â€¦ ] Use as extra edge features
v

For complexity consideration, this can be removed.
â€¢ Build fully connected graphs (transformers)

â€¢

Use [

ð´?

,
'(

ð´?*

,
'(

ð´?)

'( â€¦ ] as edge features

ð´+: Adding some row/column normalization is good for numerical stability

23

Solutions for Graph-level Expressive Power
â€¢ Higher-order tensors: Computation complexity is high
Maron et al., Provably powerful graph networks, NeurIPS 2019
â€¢ Add random node features: Training is hard to converge
Sato et al., Random features strengthen graph neural networks, SDM 2021 Abboud et al., The surprising power of graph neural networks with random node initialization, IJCAI 2021
24

Limitations of GNNs in Science
â€¢ Limited Expressive Power â€¢ Fail to represent some relations between input features and labels
â€¢ Hard to Interpret â€¢ Complicated architectures â€¢ Capture spurious correlations not effective patterns
â€¢ Subpar Generalization â€¢ Performance drop due to distribution shifts (simulation-based training -> real-data testing)
25

ðœ â†’ 3Î¼ Detection

q Motivation
Â§ Physics beyond the Standard Model
â€¢ Search for charged lepton flavor violating decays â€¢ ðœ â†’ 3Î¼ is the cleanest signature
Â§ Extremely small branching ratio
â€¢ Though may be enhanced by BSM physics â€¢ BR(ðœ â†’ 3ðœ‡) âˆ¼ ð‘‚ 10!"
q Given an ML model, we want
Â§ High trigger efficiency Â§ Low trigger rate

The Large Hadron Collider

26

GNNs give super performance
q We use muon hits left in the muon stations to make prediction. Traditional methods (pattern matching)
The three ðœ‡â€™s
GNN-based methods
92% efficiency @ 10kHz rate Can we trust this performance?
27

Problem: Spurious Correlations
â€¢ Positive samples: Only use the endcap (a half of space Eta>0 or Eta< 0 ) without true signals
â€¢ Negative samples: Randomly choose one endcap (a half of space Eta>0 or Eta< 0 )
The three ðœâ€™s
Now, we get 87% efficiency @ 10kHz rate
28

Problem: Spurious Correlations
â€¢ Positive samples: Only use the endcap (a half of space Eta>0 or Eta< 0 ) without true signals
â€¢ Negative samples: Randomly choose one endcap (a half of space Eta>0 or Eta< 0 )

The three ðœâ€™s

Why can it happenï¼Ÿ
Either the simulator or pre-processing injects spurious correlations.

Now, we get 87% efficiency @ 10kHz rate

Two endcaps: 92% efficiency @ 10kHz rate Traditional: 24% efficiency @ 77kHz rate29

Design Interpretable and Trustworthy GNNs
Can we check patterns learned by GNNs to see if we can trust them?

We expect GNNs to tell us

We expect GNNs to tell us

30

Solution: Learnable Randomness Injection
Constrain the amount of information that the model can use from the data

Miao et al., Interpretable geometric deep learning via learnable randomness injection, in submission

31

Miao et al., Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism, ICML 2022

Solution: Learnable Randomness Injection
â€¢ A trainable model output the probabilities to drop/keep points

Add randomness

0.7

0.7

0.7

0.7

0.7 0.7

keeping prob.

a trainable model

(graph construction + GNN)

32

Miao et al., Interpretable geometric deep learning via learnable randomness injection, in submission

Solution: Learnable Randomness Injection
â€¢ A trainable model output the probabilities to drop/keep points â€¢ Another trainable model encodes the perturbed data to predict labels

Add randomness

0.7 0.7

0.7 0.7
0.7 0.7

Driven by Classification
Loss
Another trainable model

0.7 0.7

0.7 1
1 1

keeping prob.

a trainable model

(graph construction + GNN)

33

Miao et al., Interpretable geometric deep learning via learnable randomness injection, in submission

Solution: Learnable Randomness Injection
â€¢ A trainable model output the probabilities to drop/keep points â€¢ Another trainable model encodes the perturbed data to predict labels â€¢ Rank the probabilities to provide important patterns

Add randomness

0.7 0.7

0.7 0.7
0.7 0.7

Driven by Classification
Loss
Another trainable model

0.7 0.7

0.7 1
1 1

keeping prob.

a trainable model

(graph construction + GNN)

Provide interpretation for checking
34

Miao et al., Interpretable geometric deep learning via learnable randomness injection, in submission

Solution: Learnable Randomness Injection

â€¢ A trainable model output the probabilities to drop/keep points
â€¢ Another trainable model encodes the perturbed data to predict labels
â€¢ Rank the probabilities to provide important patterns â€¢ The detected points by our methods match the ðœ â†’ 3Î¼ signals with
80% ROC AUC

Add randomness

0.7 0.7

0.7 0.7
0.7 0.7

Driven by Classification
Loss
Another trainable model

0.7 0.7

0.7 1
1 1

keeping prob.

a trainable model

(graph construction + GNN)

Provide interpretation for checking
35

Miao et al., Interpretable geometric deep learning via learnable randomness injection, in submission

Check Papers and Code
More Applications

Point cloud part is under review at ICLR 2023

Applied to 2-D molecules

Code is online:
https://github.com/Graph-COM/GSAT

Miao et al., Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism, ICML 2022

36

Miao et al., Interpretable geometric deep learning via learnable randomness injection, in submission

Takeaways

Three problems of GNNs in scientific applicationsâ€¦

â€¢ Limited Expressive Power

Adding structural features, e.g., [

ð´8

,
#$

ð´8%

,
#$

ð´8&

,
#$

â€¦

]

as

edge

features

â€¢ Hard to Interpret
Constraining information during the model training by adding randomness

â€¢ Subpar Generalization

37

Takeaways

Three problems of GNNs in scientific applicationsâ€¦

â€¢ Limited Expressive Power

Adding structural features, e.g., [

ð´8

,
#$

ð´8%

,
#$

ð´8&

,
#$

â€¦

]

as

edge

features

â€¢ Hard to Interpret and trust
Constraining information during the model training by adding randomness

â€¢ Subpar Generalization

Thank you!
38

