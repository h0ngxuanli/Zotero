é¦–å‘äº Miracleyoo's
å†™æ–‡ç« 
ç‚¹å‡»æ‰“å¼€è¦çˆ†äº†çš„ä¸»é¡µ
Pytorch Lightning å®Œå…¨æ”»ç•¥
Pytorch Lightning å®Œå…¨æ”»ç•¥
Takanashi
Takanashi
å°½é‡æœ‰è¶£çš„CS PhD
â€‹ å…³æ³¨ä»–
1,104 äººèµåŒäº†è¯¥æ–‡ç« 
å†™åœ¨å‰é¢

Pytorch-Lightningè¿™ä¸ªåº“æˆ‘â€œå‘ç°â€è¿‡ä¸¤æ¬¡ã€‚ç¬¬ä¸€æ¬¡å‘ç°æ—¶ï¼Œæ„Ÿè§‰å®ƒå¾ˆé‡å¾ˆéš¾å­¦ï¼Œè€Œä¸”ä¼¼ä¹è‡ªå·±ä¹Ÿç”¨ä¸ä¸Šã€‚ä½†æ˜¯åé¢éšç€åšçš„é¡¹ç›®å¼€å§‹å‡ºç°äº†ä¸€äº›ç¨å¾®é«˜é˜¶çš„è¦æ±‚ï¼Œæˆ‘å‘ç°æˆ‘æ€»æ˜¯ä¸æ–­åœ°åœ¨ç›¸ä¼¼å·¥ç¨‹ä»£ç ä¸ŠèŠ±è´¹å¤§é‡æ—¶é—´ï¼ŒDebugä¹Ÿæ˜¯è¿™äº›ä»£ç èŠ±çš„æ—¶é—´æœ€å¤šï¼Œè€Œä¸”æ¸æ¸äº§ç”Ÿäº†ä¸€ä¸ªçŸ›ç›¾ä¹‹å¤„ï¼šå¦‚æœæƒ³è¦æ›´å¤šæ›´å¥½çš„åŠŸèƒ½ï¼Œå¦‚TensorBoardæ”¯æŒï¼ŒEarly Stopï¼ŒLR Schedulerï¼Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œå¿«é€Ÿæµ‹è¯•ç­‰ï¼Œä»£ç å°±æ— å¯é¿å…åœ°å˜å¾—è¶Šæ¥è¶Šé•¿ï¼Œçœ‹èµ·æ¥ä¹Ÿè¶Šæ¥è¶Šä¹±ï¼ŒåŒæ—¶æ ¸å¿ƒçš„è®­ç»ƒé€»è¾‘ä¹Ÿæ¸æ¸è¢«è¿™äº›å·¥ç¨‹ä»£ç ç›–è¿‡ã€‚é‚£ä¹ˆæœ‰æ²¡æœ‰æ›´å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œç”šè‡³èƒ½ä¸€é”®è§£å†³æ‰€æœ‰è¿™äº›é—®é¢˜å‘¢ï¼Ÿ

äºæ˜¯æˆ‘ç¬¬äºŒæ¬¡å‘ç°äº†Pytorch-Lightningã€‚

çœŸé¦™ã€‚


ä½†æ˜¯é—®é¢˜è¿˜æ˜¯æ¥äº†ã€‚è¿™ä¸ªæ¡†æ¶å¹¶æ²¡æœ‰å› ä¸ºé¦™è€Œå˜å¾—æ›´åŠ æ˜“å­¦ã€‚å®˜ç½‘çš„æ•™ç¨‹å¾ˆä¸°å¯Œï¼Œå¯ä»¥çœ‹å‡ºæ¥å¼€å‘è€…ä»¬åœ¨åŠªåŠ›åšäº†ã€‚ä½†æ˜¯å¾ˆå¤šç›¸è¿çš„çŸ¥è¯†ç‚¹éƒ½è¢«åˆ†å¸ƒåœ¨äº†ä¸åŒçš„ç‰ˆå—é‡Œï¼Œè¿˜æœ‰ä¸€äº›æ ¸å¿ƒçš„ç†è§£è¦ç‚¹å¹¶æ²¡æœ‰è¢«å¼ºè°ƒå‡ºæ¥ï¼Œè€Œæ˜¯å°å­—å¸¦è¿‡ï¼Œè¿™è®©æˆ‘æƒ³åšä¸€ä¸ªæ™®æƒ çš„æ•™ç¨‹ï¼ŒåŒ…å«æ‰€æœ‰æˆ‘åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è®¤ä¸ºé‡è¦çš„æ¦‚å¿µï¼Œå¥½ç”¨çš„å‚æ•°ï¼Œä¸€äº›æ³¨æ„ç‚¹ã€å‘ç‚¹ï¼Œå¤§é‡çš„ç¤ºä¾‹ä»£ç æ®µå’Œä¸€äº›æ ¸å¿ƒé—®é¢˜çš„é›†ä¸­è®²è§£ã€‚

æœ€åï¼Œç¬¬ä¸‰éƒ¨åˆ†æä¾›äº†ä¸€ä¸ªæˆ‘æ€»ç»“å‡ºæ¥çš„æ˜“ç”¨äºå¤§å‹é¡¹ç›®ã€å®¹æ˜“è¿ç§»ã€æ˜“äºå¤ç”¨çš„æ¨¡æ¿ï¼Œæœ‰å…´è¶£çš„å¯ä»¥å» GitHub è¯•ç”¨ã€‚
æ ¸å¿ƒ

    Pytorch-Lighting çš„ä¸€å¤§ç‰¹ç‚¹æ˜¯æŠŠæ¨¡å‹å’Œç³»ç»Ÿåˆ†å¼€æ¥çœ‹ã€‚æ¨¡å‹æ˜¯åƒResnet18ï¼Œ RNNä¹‹ç±»çš„çº¯æ¨¡å‹ï¼Œ è€Œç³»ç»Ÿå®šä¹‰äº†ä¸€ç»„æ¨¡å‹å¦‚ä½•ç›¸äº’äº¤äº’ï¼Œå¦‚GANï¼ˆç”Ÿæˆå™¨ç½‘ç»œä¸åˆ¤åˆ«å™¨ç½‘ç»œï¼‰ã€Seq2Seqï¼ˆEncoderä¸Decoderç½‘ç»œï¼‰å’ŒBertã€‚åŒæ—¶ï¼Œæœ‰æ—¶å€™é—®é¢˜åªæ¶‰åŠä¸€ä¸ªæ¨¡å‹ï¼Œé‚£ä¹ˆè¿™ä¸ªç³»ç»Ÿåˆ™å¯ä»¥æ˜¯ä¸€ä¸ªé€šç”¨çš„ç³»ç»Ÿï¼Œç”¨äºæè¿°æ¨¡å‹å¦‚ä½•ä½¿ç”¨ï¼Œå¹¶å¯ä»¥è¢«å¤ç”¨åˆ°å¾ˆå¤šå…¶ä»–é¡¹ç›®ã€‚
    Pytorch-Lighting çš„æ ¸å¿ƒè®¾è®¡æ€æƒ³æ˜¯â€œè‡ªç»™è‡ªè¶³â€ã€‚æ¯ä¸ªç½‘ç»œä¹ŸåŒæ—¶åŒ…å«äº†å¦‚ä½•è®­ç»ƒã€å¦‚ä½•æµ‹è¯•ã€ä¼˜åŒ–å™¨å®šä¹‰ç­‰å†…å®¹ã€‚

æ¨èä½¿ç”¨æ–¹æ³•

è¿™ä¸€éƒ¨åˆ†æ”¾åœ¨æœ€å‰é¢ï¼Œå› ä¸ºå…¨æ–‡å†…å®¹å¤ªé•¿ï¼Œå¦‚æœæ”¾åé¢å®¹æ˜“å¿½ç•¥æ‰è¿™éƒ¨åˆ†ç²¾åã€‚

Pytorch-Lightning æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åº“ï¼Œæˆ–è€…è¯´æ˜¯pytorchçš„æŠ½è±¡å’ŒåŒ…è£…ã€‚å®ƒçš„å¥½å¤„æ˜¯å¯å¤ç”¨æ€§å¼ºï¼Œæ˜“ç»´æŠ¤ï¼Œé€»è¾‘æ¸…æ™°ç­‰ã€‚ç¼ºç‚¹ä¹Ÿå¾ˆæ˜æ˜¾ï¼Œè¿™ä¸ªåŒ…éœ€è¦å­¦ä¹ å’Œç†è§£çš„å†…å®¹è¿˜æ˜¯æŒºå¤šçš„ï¼Œæˆ–è€…æ¢å¥è¯è¯´ï¼Œå¾ˆé‡ã€‚å¦‚æœç›´æ¥æŒ‰ç…§å®˜æ–¹çš„æ¨¡æ¿å†™ä»£ç ï¼Œå°å‹projectè¿˜å¥½ï¼Œå¦‚æœæ˜¯å¤§å‹é¡¹ç›®ï¼Œæœ‰å¤æ•°ä¸ªéœ€è¦è°ƒè¯•éªŒè¯çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œé‚£å°±ä¸å¤ªå¥½åŠï¼Œç”šè‡³æ›´åŠ éº»çƒ¦äº†ã€‚ç»è¿‡å‡ å¤©çš„æ‘¸ç´¢å’Œè°ƒè¯•ï¼Œæˆ‘æ€»ç»“å‡ºäº†ä¸‹é¢è¿™æ ·ä¸€å¥—å¥½ç”¨çš„æ¨¡æ¿ï¼Œä¹Ÿå¯ä»¥è¯´æ˜¯å¯¹Pytorch-Lightningçš„è¿›ä¸€æ­¥æŠ½è±¡ã€‚

æ¬¢è¿å¤§å®¶å°è¯•è¿™ä¸€å¥—ä»£ç é£æ ¼ï¼Œå¦‚æœç”¨ä¹ æƒ¯çš„è¯è¿˜æ˜¯ç›¸å½“æ–¹ä¾¿å¤ç”¨çš„ï¼Œä¹Ÿä¸å®¹æ˜“åŠé“é€€å‘ã€‚

 root- |-data |-__init__.py |-data_interface.py |-xxxdataset1.py |-xxxdataset2.py |-... |-model |-__init__.py |-model_interface.py |-xxxmodel1.py |-xxxmodel2.py |-... |-main.py 

å¦‚æœå¯¹æ¯ä¸ªæ¨¡å‹ç›´æ¥ä¸Šplmoduleï¼Œå¯¹äºå·²æœ‰é¡¹ç›®ã€åˆ«äººçš„ä»£ç ç­‰çš„è½¬æ¢å°†ç›¸å½“è€—æ—¶ã€‚å¦å¤–ï¼Œè¿™æ ·çš„è¯ï¼Œä½ éœ€è¦ç»™æ¯ä¸ªæ¨¡å‹éƒ½åŠ ä¸Šä¸€äº›ç›¸ä¼¼çš„ä»£ç ï¼Œå¦‚ training_step ï¼Œ validation_step ã€‚æ˜¾ç„¶ï¼Œè¿™å¹¶ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼Œå¦‚æœçœŸçš„è¿™æ ·åšï¼Œä¸ä½†ä¸æ˜“äºç»´æŠ¤ï¼Œåè€Œå¯èƒ½ä¼šæ›´åŠ æ‚ä¹±ã€‚åŒç†ï¼Œå¦‚æœæŠŠæ¯ä¸ªæ•°æ®é›†ç±»éƒ½ç›´æ¥è½¬æ¢æˆplçš„DataModuleï¼Œä¹Ÿä¼šé¢ä¸´ç›¸ä¼¼çš„é—®é¢˜ã€‚åŸºäºè¿™æ ·çš„è€ƒé‡ï¼Œæˆ‘å»ºè®®ä½¿ç”¨ä¸Šè¿°æ¶æ„ï¼š

    ä¸»ç›®å½•ä¸‹åªæ”¾ä¸€ä¸ª main.py æ–‡ä»¶ã€‚
    data å’Œ modle ä¸¤ä¸ªæ–‡ä»¶å¤¹ä¸­æ”¾å…¥ __init__.py æ–‡ä»¶ï¼ŒåšæˆåŒ…ã€‚è¿™æ ·æ–¹ä¾¿å¯¼å…¥ã€‚ä¸¤ä¸ª init æ–‡ä»¶åˆ†åˆ«æ˜¯ï¼š
        from .data_interface import DInterface
        from .model_interface import MInterface 
    åœ¨ data_interface ä¸­å»ºç«‹ä¸€ä¸ª class DInterface(pl.LightningDataModule): ç”¨ä½œæ‰€æœ‰æ•°æ®é›†æ–‡ä»¶çš„æ¥å£ã€‚ __init__() å‡½æ•°ä¸­importç›¸åº”Datasetç±»ï¼Œ setup() è¿›è¡Œå®ä¾‹åŒ–ï¼Œå¹¶è€è€å®å®åŠ å…¥æ‰€éœ€è¦çš„çš„ train_dataloader , val_dataloader , test_dataloader å‡½æ•°ã€‚è¿™äº›å‡½æ•°å¾€å¾€éƒ½æ˜¯ç›¸ä¼¼çš„ï¼Œå¯ä»¥ç”¨å‡ ä¸ªè¾“å…¥argsæ§åˆ¶ä¸åŒçš„éƒ¨åˆ†ã€‚
    åŒç†ï¼Œåœ¨ model_interface ä¸­å»ºç«‹ class MInterface(pl.LightningModule): ç±»ï¼Œä½œä¸ºæ¨¡å‹çš„ä¸­é—´æ¥å£ã€‚ __init__() å‡½æ•°ä¸­importç›¸åº”æ¨¡å‹ç±»ï¼Œç„¶åè€è€å®å®åŠ å…¥ configure_optimizers , training_step , validation_step ç­‰å‡½æ•°ï¼Œç”¨ä¸€ä¸ªæ¥å£ç±»æ§åˆ¶æ‰€æœ‰æ¨¡å‹ã€‚ä¸åŒéƒ¨åˆ†ä½¿ç”¨è¾“å…¥å‚æ•°æ§åˆ¶ã€‚
    main.py å‡½æ•°åªè´Ÿè´£ï¼š
        å®šä¹‰parserï¼Œæ·»åŠ parseé¡¹ã€‚
        é€‰å¥½éœ€è¦çš„ callback å‡½æ•°ä»¬ã€‚
        å®ä¾‹åŒ– MInterface , DInterface , Trainer ã€‚

å®Œäº‹ã€‚

å®Œå…¨ç‰ˆæ¨¡æ¿å¯ä»¥åœ¨ GitHub æ‰¾åˆ°ã€‚
Lightning Module
ç®€ä»‹

ä¸»é¡µé¢

    ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š
    æ¨¡å‹
    ä¼˜åŒ–å™¨
    Train/Val/Testæ­¥éª¤
    æ•°æ®æµä¼ªä»£ç ï¼š

 outs = [] for batch in data : out = training_step ( batch ) outs . append ( out ) training_epoch_end ( outs )  

ç­‰ä»·Lightningä»£ç ï¼š

 def training_step ( self , batch , batch_idx ): prediction = ... return prediction def training_epoch_end ( self , training_step_outputs ): for prediction in predictions : # do something with these  

æˆ‘ä»¬éœ€è¦åšçš„ï¼Œå°±æ˜¯åƒå¡«ç©ºä¸€æ ·ï¼Œå¡«è¿™äº›å‡½æ•°ã€‚
ç»„ä»¶ä¸å‡½æ•°

APIé¡µé¢

    ä¸€ä¸ªPytorch-Lighting æ¨¡å‹å¿…é¡»å«æœ‰çš„éƒ¨ä»¶æ˜¯ï¼š
    init : åˆå§‹åŒ–ï¼ŒåŒ…æ‹¬æ¨¡å‹å’Œç³»ç»Ÿçš„å®šä¹‰ã€‚
    training_step(self, batch, batch_idx) : å³æ¯ä¸ªbatchçš„å¤„ç†å‡½æ•°ã€‚
    å‚æ•°ï¼š
        batch ( Tensor | ( Tensor , â€¦) | [ Tensor , â€¦]) â€“ The output of your DataLoader . A tensor, tuple or list.
        batch_idx ( int ) â€“ Integer displaying index of this batch
        optimizer_idx ( int ) â€“ When using multiple optimizers, this argument will also be present.
        hiddens ( Tensor ) â€“ Passed in if truncated_bptt_steps > 0.

è¿”å›å€¼ï¼šAny of.

        Tensor - The loss tensor
        dict - A dictionary. Can include any keys, but must include the key 'loss'
        None - Training will skip to the next batch


è¿”å›å€¼æ— è®ºå¦‚ä½•ä¹Ÿéœ€è¦æœ‰ä¸€ä¸ªlossé‡ã€‚å¦‚æœæ˜¯å­—å…¸ï¼Œè¦æœ‰è¿™ä¸ªkeyã€‚æ²¡lossè¿™ä¸ªbatchå°±è¢«è·³è¿‡äº†ã€‚ä¾‹ï¼š

 def training_step ( self , batch , batch_idx ): x , y , z = batch out = self . encoder ( x ) loss = self . loss ( out , x ) return loss # Multiple optimizers (e.g.: GANs) def training_step ( self , batch , batch_idx , optimizer_idx ): if optimizer_idx == 0 : # do training_step with encoder if optimizer_idx == 1 : # do training_step with decoder # Truncated back-propagation through time def training_step ( self , batch , batch_idx , hiddens ): # hiddens are the hidden states from the previous truncated backprop step ... out , hiddens = self . lstm ( data , hiddens ) ... return { 'loss' : loss , 'hiddens' : hiddens }  

configure_optimizers : ä¼˜åŒ–å™¨å®šä¹‰ï¼Œè¿”å›ä¸€ä¸ªä¼˜åŒ–å™¨ï¼Œæˆ–æ•°ä¸ªä¼˜åŒ–å™¨ï¼Œæˆ–ä¸¤ä¸ªListï¼ˆä¼˜åŒ–å™¨ï¼ŒSchedulerï¼‰ã€‚å¦‚ï¼š

 # most cases def configure_optimizers ( self ): opt = Adam ( self . parameters (), lr = 1e-3 ) return opt # multiple optimizer case (e.g.: GAN) def configure_optimizers ( self ): generator_opt = Adam ( self . model_gen . parameters (), lr = 0.01 ) disriminator_opt = Adam ( self . model_disc . parameters (), lr = 0.02 ) return generator_opt , disriminator_opt # example with learning rate schedulers def configure_optimizers ( self ): generator_opt = Adam ( self . model_gen . parameters (), lr = 0.01 ) disriminator_opt = Adam ( self . model_disc . parameters (), lr = 0.02 ) discriminator_sched = CosineAnnealing ( discriminator_opt , T_max = 10 ) return [ generator_opt , disriminator_opt ], [ discriminator_sched ] # example with step-based learning rate schedulers def configure_optimizers ( self ): gen_opt = Adam ( self . model_gen . parameters (), lr = 0.01 ) dis_opt = Adam ( self . model_disc . parameters (), lr = 0.02 ) gen_sched = { 'scheduler' : ExponentialLR ( gen_opt , 0.99 ), 'interval' : 'step' } # called after each training step dis_sched = CosineAnnealing ( discriminator_opt , T_max = 10 ) # called every epoch return [ gen_opt , dis_opt ], [ gen_sched , dis_sched ] # example with optimizer frequencies # see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1 # https://arxiv.org/abs/1704.00028 def configure_optimizers ( self ): gen_opt = Adam ( self . model_gen . parameters (), lr = 0.01 ) dis_opt = Adam ( self . model_disc . parameters (), lr = 0.02 ) n_critic = 5 return ( { 'optimizer' : dis_opt , 'frequency' : n_critic }, { 'optimizer' : gen_opt , 'frequency' : 1 } )  

    å¯ä»¥æŒ‡å®šçš„éƒ¨ä»¶æœ‰ï¼š
        forward : å’Œæ­£å¸¸çš„ nn.Module ä¸€æ ·ï¼Œç”¨äºinferenceã€‚å†…éƒ¨è°ƒç”¨æ—¶ï¼š y=self(batch)
        training_step_end : åªåœ¨ä½¿ç”¨å¤šä¸ªnodeè¿›è¡Œè®­ç»ƒä¸”ç»“æœæ¶‰åŠå¦‚softmaxä¹‹ç±»éœ€è¦å…¨éƒ¨è¾“å‡ºè”åˆè¿ç®—çš„æ­¥éª¤æ—¶ä½¿ç”¨è¯¥å‡½æ•°ã€‚åŒç†ï¼Œ validation_step_end / test_step_end ã€‚
        training_epoch_end :
            åœ¨ä¸€ä¸ªè®­ç»ƒepochç»“å°¾å¤„è¢«è°ƒç”¨ã€‚
            è¾“å…¥å‚æ•°ï¼šä¸€ä¸ªListï¼ŒListçš„å†…å®¹æ˜¯å‰é¢ training_step() æ‰€è¿”å›çš„æ¯æ¬¡çš„å†…å®¹ã€‚
            è¿”å›ï¼šNone
        validation_step(self, batch, batch_idx) / test_step(self, batch, batch_idx) :
            æ²¡æœ‰è¿”å›å€¼é™åˆ¶ï¼Œä¸ä¸€å®šéè¦è¾“å‡ºä¸€ä¸ª val_loss ã€‚
        validation_epoch_end / test_epoch_end 
    å·¥å…·å‡½æ•°æœ‰ï¼š
        freeze ï¼šå†»ç»“æ‰€æœ‰æƒé‡ä»¥ä¾›é¢„æµ‹æ—¶å€™ä½¿ç”¨ã€‚ä»…å½“å·²ç»è®­ç»ƒå®Œæˆä¸”åé¢åªæµ‹è¯•æ—¶ä½¿ç”¨ã€‚
        print ï¼šå°½ç®¡è‡ªå¸¦çš„ print å‡½æ•°ä¹Ÿå¯ä»¥ä½¿ç”¨ï¼Œä½†å¦‚æœç¨‹åºè¿è¡Œåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿæ—¶ï¼Œä¼šæ‰“å°å¤šæ¬¡ã€‚è€Œä½¿ç”¨ self.print() åˆ™åªä¼šæ‰“å°ä¸€æ¬¡ã€‚
        log ï¼šåƒæ˜¯TensorBoardç­‰logè®°å½•å™¨ï¼Œå¯¹äºæ¯ä¸ªlogçš„æ ‡é‡ï¼Œéƒ½ä¼šæœ‰ä¸€ä¸ªç›¸å¯¹åº”çš„æ¨ªåæ ‡ï¼Œå®ƒå¯èƒ½æ˜¯batch numberæˆ–epoch numberã€‚è€Œ on_step å°±è¡¨ç¤ºæŠŠè¿™ä¸ªlogå‡ºå»çš„é‡çš„æ¨ªåæ ‡è¡¨ç¤ºä¸ºå½“å‰batchï¼Œè€Œ on_epoch åˆ™è¡¨ç¤ºå°†logçš„é‡åœ¨æ•´ä¸ªepochä¸Šè¿›è¡Œç´¯ç§¯ålogï¼Œæ¨ªåæ ‡ä¸ºå½“å‰epochã€‚
        | LightningMoule Hook | on_step | on_epoch | prog_bar | logger | | --------------------- | ------- | -------- | -------- | ------ | | training_step | T | F | F | T | | training_step_end | T | F | F | T | | training_epoch_end | F | T | F | T | | validation_step | F | T | F | T | | validation_step_end | F | T | F | T | | validation_epoch_end* | F | T | F | T |
        * also applies to the test loop

    å‚æ•°
    name ( str ) â€“ key name
    value ( Any ) â€“ value name
    prog_bar ( bool ) â€“ if True logs to the progress bar
    logger ( bool ) â€“ if True logs to the logger
    on_step ( Optional [ bool ]) â€“ if True logs at this step. None auto-logs at the training_step but not validation/test_step
    on_epoch ( Optional [ bool ]) â€“ if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step
    reduce_fx ( Callable ) â€“ reduction function over step values for end of epoch. Torch.mean by default
    tbptt_reduce_fx ( Callable ) â€“ function to reduce on truncated back prop
    tbptt_pad_token ( int ) â€“ token to use for padding
    enable_graph ( bool ) â€“ if True, will not auto detach the graph
    sync_dist ( bool ) â€“ if True, reduces the metric across GPUs/TPUs
    sync_dist_op ( Union [ Any , str ]) â€“ the op to sync across GPUs/TPUs
    sync_dist_group ( Optional [ Any ]) â€“ the ddp group

        log_dict ï¼šå’Œ log å‡½æ•°å”¯ä¸€çš„åŒºåˆ«å°±æ˜¯ï¼Œ name å’Œ value å˜é‡ç”±ä¸€ä¸ªå­—å…¸æ›¿æ¢ã€‚è¡¨ç¤ºåŒæ—¶logå¤šä¸ªå€¼ã€‚å¦‚ï¼š
        python values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values)
        save_hyperparameters ï¼šå‚¨å­˜ init ä¸­è¾“å…¥çš„æ‰€æœ‰è¶…å‚ã€‚åç»­è®¿é—®å¯ä»¥ç”± self.hparams.argX æ–¹å¼è¿›è¡Œã€‚åŒæ—¶ï¼Œè¶…å‚è¡¨ä¹Ÿä¼šè¢«å­˜åˆ°æ–‡ä»¶ä¸­ã€‚
    å‡½æ•°å†…å»ºå˜é‡ï¼š
        device ï¼šå¯ä»¥ä½¿ç”¨ self.device æ¥æ„å»ºè®¾å¤‡æ— å…³å‹tensorã€‚å¦‚ï¼š z = torch.rand(2, 3, device=self.device) ã€‚
        hparams ï¼šå«æœ‰æ‰€æœ‰å‰é¢å­˜ä¸‹æ¥çš„è¾“å…¥è¶…å‚ã€‚
        precision ï¼šç²¾ç¡®åº¦ã€‚å¸¸è§32å’Œ16ã€‚

è¦ç‚¹

    å¦‚æœå‡†å¤‡ä½¿ç”¨DataParallelï¼Œåœ¨å†™ training_step çš„æ—¶å€™éœ€è¦è°ƒç”¨forwardå‡½æ•°ï¼Œ z=self(x) 

æ¨¡æ¿

 class LitModel ( pl . LightningModule ): def __init__ ( ... ): def forward ( ... ): def training_step ( ... ) def training_step_end ( ... ) def training_epoch_end ( ... ) def validation_step ( ... ) def validation_step_end ( ... ) def validation_epoch_end ( ... ) def test_step ( ... ) def test_step_end ( ... ) def test_epoch_end ( ... ) def configure_optimizers ( ... ) def any_extra_hook ( ... )  

Trainer
åŸºç¡€ä½¿ç”¨

 model = MyLightningModule () trainer = Trainer () trainer . fit ( model , train_dataloader , val_dataloader )  

å¦‚æœè¿ validation_step éƒ½æ²¡æœ‰ï¼Œé‚£ val_dataloader ä¹Ÿå°±ç®—äº†ã€‚
ä¼ªä»£ç ä¸hooks

Hooksé¡µé¢

 def fit ( ... ): on_fit_start () if global_rank == 0 : # prepare data is called on GLOBAL_ZERO only prepare_data () for gpu / tpu in gpu / tpus : train_on_device ( model . copy ()) on_fit_end () def train_on_device ( model ): # setup is called PER DEVICE setup () configure_optimizers () on_pretrain_routine_start () for epoch in epochs : train_loop () teardown () def train_loop (): on_train_epoch_start () train_outs = [] for train_batch in train_dataloader (): on_train_batch_start () # ----- train_step methods ------- out = training_step ( batch ) train_outs . append ( out ) loss = out . loss backward () on_after_backward () optimizer_step () on_before_zero_grad () optimizer_zero_grad () on_train_batch_end ( out ) if should_check_val : val_loop () # end training epoch logs = training_epoch_end ( outs ) def val_loop (): model . eval () torch . set_grad_enabled ( False ) on_validation_epoch_start () val_outs = [] for val_batch in val_dataloader (): on_validation_batch_start () # -------- val step methods ------- out = validation_step ( val_batch ) val_outs . append ( out ) on_validation_batch_end ( out ) validation_epoch_end ( val_outs ) on_validation_epoch_end () # set up for train model . train () torch . set_grad_enabled ( True )  

æ¨èå‚æ•°

å‚æ•°ä»‹ç»ï¼ˆé™„è§†é¢‘ï¼‰

ç±»å®šä¹‰ä¸é»˜è®¤å‚æ•°

    default_root_dir ï¼šé»˜è®¤å­˜å‚¨åœ°å€ã€‚æ‰€æœ‰çš„å®éªŒå˜é‡å’Œæƒé‡å…¨éƒ¨ä¼šè¢«å­˜åˆ°è¿™ä¸ªæ–‡ä»¶å¤¹é‡Œé¢ã€‚æ¨èæ˜¯ï¼Œæ¯ä¸ªæ¨¡å‹æœ‰ä¸€ä¸ªç‹¬ç«‹çš„æ–‡ä»¶å¤¹ã€‚æ¯æ¬¡é‡æ–°è®­ç»ƒä¼šäº§ç”Ÿä¸€ä¸ªæ–°çš„ version_x å­æ–‡ä»¶å¤¹ã€‚
    max_epochs ï¼šæœ€å¤§è®­ç»ƒå‘¨æœŸæ•°ã€‚ trainer = Trainer(max_epochs=1000)
    min_epochs ï¼šè‡³å°‘è®­ç»ƒå‘¨æœŸæ•°ã€‚å½“æœ‰Early Stopæ—¶ä½¿ç”¨ã€‚
    auto_scale_batch_size ï¼šåœ¨è¿›è¡Œä»»ä½•è®­ç»ƒå‰è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„batch sizeã€‚

 # default used by the Trainer (no scaling of batch size) trainer = Trainer ( auto_scale_batch_size = None ) # run batch size scaling, result overrides hparams.batch_size trainer = Trainer ( auto_scale_batch_size = 'binsearch' ) # call tune to find the batch size trainer . tune ( model )  

    auto_select_gpus ï¼šè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„GPUã€‚å°¤å…¶æ˜¯åœ¨æœ‰GPUå¤„äºç‹¬å æ¨¡å¼æ—¶å€™ï¼Œéå¸¸æœ‰ç”¨ã€‚
    auto_lr_find ï¼šè‡ªåŠ¨æ‰¾åˆ°åˆé€‚çš„åˆå§‹å­¦ä¹ ç‡ã€‚ä½¿ç”¨äº†è¯¥ è®ºæ–‡ çš„æŠ€æœ¯ã€‚å½“ä¸”ä»…å½“æ‰§è¡Œ trainer.tune(model) ä»£ç æ—¶å·¥ä½œã€‚

 # run learning rate finder, results override hparams.learning_rate trainer = Trainer ( auto_lr_find = True ) # run learning rate finder, results override hparams.my_lr_arg trainer = Trainer ( auto_lr_find = 'my_lr_arg' ) # call tune to find the lr trainer . tune ( model )  

    precision ï¼šç²¾ç¡®åº¦ã€‚æ­£å¸¸æ˜¯32ï¼Œä½¿ç”¨16å¯ä»¥å‡å°å†…å­˜æ¶ˆè€—ï¼Œå¢å¤§batchã€‚

 # default used by the Trainer trainer = Trainer ( precision = 32 ) # 16-bit precision trainer = Trainer ( precision = 16 , gpus = 1 )  

    val_check_interval ï¼šè¿›è¡ŒValidationæµ‹è¯•çš„å‘¨æœŸã€‚æ­£å¸¸ä¸º1ï¼Œè®­ç»ƒ1ä¸ªepochæµ‹è¯•4æ¬¡æ˜¯0.25ï¼Œæ¯1000 batchæµ‹è¯•ä¸€æ¬¡æ˜¯1000ã€‚

    use (float) to check within a training epochï¼šæ­¤æ—¶è¿™ä¸ªå€¼ä¸ºä¸€ä¸ªepochçš„ç™¾åˆ†æ¯”ã€‚æ¯ç™¾åˆ†ä¹‹å¤šå°‘æµ‹è¯•ä¸€æ¬¡ã€‚
    use (int) to check every n steps (batches)ï¼šæ¯å¤šå°‘ä¸ªbatchæµ‹è¯•ä¸€æ¬¡ã€‚

 # default used by the Trainer trainer = Trainer ( val_check_interval = 1.0 ) # check validation set 4 times during a training epoch trainer = Trainer ( val_check_interval = 0.25 ) # check validation set every 1000 training batches # use this when using iterableDataset and your dataset has no length # (ie: production cases with streaming data) trainer = Trainer ( val_check_interval = 1000 )  

    gpus ï¼šæ§åˆ¶ä½¿ç”¨çš„GPUæ•°ã€‚å½“è®¾å®šä¸ºNoneæ—¶ï¼Œä½¿ç”¨cpuã€‚

 # default used by the Trainer (ie: train on CPU) trainer = Trainer ( gpus = None ) # equivalent trainer = Trainer ( gpus = 0 ) # int: train on 2 gpus trainer = Trainer ( gpus = 2 ) # list: train on GPUs 1, 4 (by bus ordering) trainer = Trainer ( gpus = [ 1 , 4 ]) trainer = Trainer ( gpus = '1, 4' ) # equivalent # -1: train on all gpus trainer = Trainer ( gpus =- 1 ) trainer = Trainer ( gpus = '-1' ) # equivalent # combine with num_nodes to train on multiple GPUs across nodes # uses 8 gpus in total trainer = Trainer ( gpus = 2 , num_nodes = 4 ) # train only on GPUs 1 and 4 across nodes trainer = Trainer ( gpus = [ 1 , 4 ], num_nodes = 4 )  

    limit_train_batches ï¼šä½¿ç”¨è®­ç»ƒæ•°æ®çš„ç™¾åˆ†æ¯”ã€‚å¦‚æœæ•°æ®è¿‡å¤šï¼Œæˆ–æ­£åœ¨è°ƒè¯•ï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªã€‚å€¼çš„èŒƒå›´ä¸º0~1ã€‚åŒæ ·ï¼Œæœ‰ limit_test_batches ï¼Œ limit_val_batches ã€‚

 # default used by the Trainer trainer = Trainer ( limit_train_batches = 1.0 ) # run through only 25% of the training set each epoch trainer = Trainer ( limit_train_batches = 0.25 ) # run through only 10 batches of the training set each epoch trainer = Trainer ( limit_train_batches = 10 )  

    fast_dev_run ï¼šboolé‡ã€‚å¦‚æœè®¾å®šä¸ºtrueï¼Œä¼šåªæ‰§è¡Œä¸€ä¸ªbatchçš„train, val å’Œ testï¼Œç„¶åç»“æŸã€‚ä»…ç”¨äºdebugã€‚

    Setting this argument will disable tuner, checkpoint callbacks, early stopping callbacks, loggers and logger callbacks like LearningRateLogger and runs for only 1 epoch

 # default used by the Trainer trainer = Trainer ( fast_dev_run = False ) # runs 1 train, val, test batch and program ends trainer = Trainer ( fast_dev_run = True ) # runs 7 train, val, test batches and program ends trainer = Trainer ( fast_dev_run = 7 )  

.fit()å‡½æ•°

Trainer.fit(model, train_dataloader=None, val_dataloaders=None, datamodule=None) ï¼šè¾“å…¥ç¬¬ä¸€ä¸ªé‡ä¸€å®šæ˜¯modelï¼Œç„¶åå¯ä»¥è·Ÿä¸€ä¸ªLigntningDataModuleæˆ–ä¸€ä¸ªæ™®é€šçš„Train DataLoaderã€‚å¦‚æœå®šä¹‰äº†Val stepï¼Œä¹Ÿè¦æœ‰Val DataLoaderã€‚

    å‚æ•°
    datamodule ( Optional [ LightningDataModule ]) â€“ A instance of LightningDataModule.
    model ( LightningModule ) â€“ Model to fit.
    train_dataloader ( Optional [ DataLoader ]) â€“ A Pytorch DataLoader with training samples. If the model has a predefined train_dataloader method this will be skipped.
    val_dataloaders ( Union [ DataLoader , List [ DataLoader ], None ]) â€“ Either a single Pytorch Dataloader or a list of them, specifying validation samples. If the model has a predefined val_dataloaders method this will be skipped

å…¶ä»–è¦ç‚¹

    .test() è‹¥éç›´æ¥è°ƒç”¨ï¼Œä¸ä¼šè¿è¡Œã€‚ trainer.test()
    .test() ä¼šè‡ªåŠ¨loadæœ€ä¼˜æ¨¡å‹ã€‚
    model.eval() and torch.no_grad() åœ¨è¿›è¡Œæµ‹è¯•æ—¶ä¼šè¢«è‡ªåŠ¨è°ƒç”¨ã€‚
    é»˜è®¤æƒ…å†µä¸‹ï¼Œ Trainer() è¿è¡ŒäºCPUä¸Šã€‚

ä½¿ç”¨æ ·ä¾‹

    æ‰‹åŠ¨æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼š

 from argparse import ArgumentParser def main ( hparams ): model = LightningModule () trainer = Trainer ( gpus = hparams . gpus ) trainer . fit ( model ) if __name__ == '__main__' : parser = ArgumentParser () parser . add_argument ( '--gpus' , default = None ) args = parser . parse_args () main ( args )  

    è‡ªåŠ¨æ·»åŠ æ‰€æœ‰ Trainer ä¼šç”¨åˆ°çš„å‘½ä»¤è¡Œå‚æ•°ï¼š

 from argparse import ArgumentParser def main ( args ): model = LightningModule () trainer = Trainer . from_argparse_args ( args ) trainer . fit ( model ) if __name__ == '__main__' : parser = ArgumentParser () parser = Trainer . add_argparse_args ( # group the Trainer arguments together parser . add_argument_group ( title = "pl.Trainer args" ) ) args = parser . parse_args () main ( args )  

    æ··åˆå¼ï¼Œæ—¢ä½¿ç”¨ Trainer ç›¸å…³å‚æ•°ï¼Œåˆä½¿ç”¨ä¸€äº›è‡ªå®šä¹‰å‚æ•°ï¼Œå¦‚å„ç§æ¨¡å‹è¶…å‚ï¼š

 from argparse import ArgumentParser import pytorch_lightning as pl from pytorch_lightning import LightningModule , Trainer def main ( args ): model = LightningModule () trainer = Trainer . from_argparse_args ( args ) trainer . fit ( model ) if __name__ == '__main__' : parser = ArgumentParser () parser . add_argument ( '--batch_size' , default = 32 , type = int ) parser . add_argument ( '--hidden_dim' , type = int , default = 128 ) parser = Trainer . add_argparse_args ( # group the Trainer arguments together parser . add_argument_group ( title = "pl.Trainer args" ) ) args = parser . parse_args () main ( args )  

æ‰€æœ‰å‚æ•°

    Trainer.``__init__ ( logger=True , checkpoint_callback=True , callbacks=None , default_root_dir=None , gradient_clip_val=0 , process_position=0 , num_nodes=1 , num_processes=1 , gpus=None , auto_select_gpus=False , tpu_cores=None , log_gpu_memory=None , progress_bar_refresh_rate=None , overfit_batches=0.0 , track_grad_norm=- 1 , check_val_every_n_epoch=1 , fast_dev_run=False , accumulate_grad_batches=1 , max_epochs=None , min_epochs=None , max_steps=None , min_steps=None , limit_train_batches=1.0 , limit_val_batches=1.0 , limit_test_batches=1.0 , limit_predict_batches=1.0 , val_check_interval=1.0 , flush_logs_every_n_steps=100 , log_every_n_steps=50 , accelerator=None , sync_batchnorm=False , precision=32 , weights_summary='top' , weights_save_path=None , num_sanity_val_steps=2 , truncated_bptt_steps=None , resume_from_checkpoint=None , profiler=None , benchmark=False , deterministic=False , reload_dataloaders_every_epoch=False , auto_lr_find=False , replace_sampler_ddp=True , terminate_on_nan=False , auto_scale_batch_size=False , prepare_data_per_node=True , plugins=None , amp_backend='native' , amp_level='O2' , distributed_backend=None , move_metrics_to_cpu=False , multiple_trainloader_mode='max_size_cycle' , stochastic_weight_avg=False )

Logå’Œreturn lossåˆ°åº•åœ¨åšä»€ä¹ˆ

To add a training loop use the training_step method

 class LitClassifier ( pl . LightningModule ): def __init__ ( self , model ): super () . __init__ () self . model = model def training_step ( self , batch , batch_idx ): x , y = batch y_hat = self . model ( x ) loss = F . cross_entropy ( y_hat , y ) return loss  

    æ— è®ºæ˜¯ training_step ï¼Œè¿˜æ˜¯ validation_step ï¼Œ test_step è¿”å›å€¼éƒ½æ˜¯ loss ã€‚è¿”å›çš„lossä¼šè¢«ç”¨ä¸€ä¸ªlistæ”¶é›†èµ·æ¥ã€‚

Under the hood, Lightning does the following (pseudocode):

 # put model in train mode model . train () torch . set_grad_enabled ( True ) losses = [] for batch in train_dataloader : # forward loss = training_step ( batch ) losses . append ( loss . detach ()) # backward loss . backward () # apply and clear grads optimizer . step () optimizer . zero_grad ()  

Training epoch-level metrics

If you want to calculate epoch-level metrics and log them, use the .log method

 def training_step ( self , batch , batch_idx ): x , y = batch y_hat = self . model ( x ) loss = F . cross_entropy ( y_hat , y ) # logs metrics for each training_step, # and the average across the epoch, to the progress bar and logger self . log ( 'train_loss' , loss , on_step = True , on_epoch = True , prog_bar = True , logger = True ) return loss  

    å¦‚æœåœ¨ x_step å‡½æ•°ä¸­ä½¿ç”¨äº† .log() å‡½æ•°ï¼Œé‚£ä¹ˆè¿™ä¸ªé‡å°†ä¼šè¢«é€æ­¥è®°å½•ä¸‹æ¥ã€‚æ¯ä¸€ä¸ª log å‡ºå»çš„å˜é‡éƒ½ä¼šè¢«è®°å½•ä¸‹æ¥ï¼Œæ¯ä¸€ä¸ª step ä¼šé›†ä¸­ç”Ÿæˆä¸€ä¸ªå­—å…¸dictï¼Œè€Œæ¯ä¸ªepochéƒ½ä¼šæŠŠè¿™äº›å­—å…¸æ”¶é›†èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå­—å…¸çš„listã€‚

The .log object automatically reduces the requested metrics across the full epoch. Hereâ€™s the pseudocode of what it does under the hood:

 outs = [] for batch in train_dataloader : # forward out = training_step ( val_batch ) # backward loss . backward () # apply and clear grads optimizer . step () optimizer . zero_grad () epoch_metric = torch . mean ( torch . stack ([ x [ 'train_loss' ] for x in outs ]))  

Train epoch-level operations

If you need to do something with all the outputs of each training_step, override training_epoch_end yourself.

 def training_step ( self , batch , batch_idx ): x , y = batch y_hat = self . model ( x ) loss = F . cross_entropy ( y_hat , y ) preds = ... return { 'loss' : loss , 'other_stuff' : preds } def training_epoch_end ( self , training_step_outputs ): for pred in training_step_outputs : # do something  

The matching pseudocode is:

 outs = [] for batch in train_dataloader : # forward out = training_step ( val_batch ) # backward loss . backward () # apply and clear grads optimizer . step () optimizer . zero_grad () training_epoch_end ( outs )  

DataModule

ä¸»é¡µé¢
ä»‹ç»

    é¦–å…ˆï¼Œè¿™ä¸ª DataModule å’Œä¹‹å‰å†™çš„Datasetå®Œå…¨ä¸å†²çªã€‚å‰è€…æ˜¯åè€…çš„ä¸€ä¸ªåŒ…è£…ï¼Œå¹¶ä¸”è¿™ä¸ªåŒ…è£…å¯ä»¥è¢«ç”¨äºå¤šä¸ªtorch Dataset ä¸­ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œå…¶æœ€å¤§çš„ä½œç”¨å°±æ˜¯æŠŠå„ç§train/val/teståˆ’åˆ†ã€DataLoaderåˆå§‹åŒ–ä¹‹ç±»çš„é‡å¤ä»£ç é€šè¿‡åŒ…è£…ç±»çš„æ–¹å¼å¾—ä»¥è¢«ç®€å•çš„å¤ç”¨ã€‚
    å…·ä½“ä½œç”¨é¡¹ç›®ï¼š
        Download instructionsï¼šä¸‹è½½
        Processing instructionsï¼šå¤„ç†
        Split instructionsï¼šåˆ†å‰²
        Train dataloaderï¼šè®­ç»ƒé›†Dataloader
        Val dataloader(s)ï¼šéªŒè¯é›†Dataloader
        Test dataloader(s)ï¼šæµ‹è¯•é›†Dataloader
    å…¶æ¬¡ï¼Œ pl.LightningDataModule ç›¸å½“äºä¸€ä¸ªåŠŸèƒ½åŠ å¼ºç‰ˆçš„torch Datasetï¼ŒåŠ å¼ºçš„åŠŸèƒ½åŒ…æ‹¬ï¼š
    prepare_data(self) ï¼š
        æœ€æœ€å¼€å§‹çš„æ—¶å€™ï¼Œè¿›è¡Œä¸€äº›æ— è®ºGPUæœ‰å¤šå°‘åªè¦æ‰§è¡Œä¸€æ¬¡çš„æ“ä½œï¼Œå¦‚å†™å…¥ç£ç›˜çš„ä¸‹è½½æ“ä½œã€åˆ†è¯æ“ä½œ(tokenize)ç­‰ã€‚
        è¿™é‡Œæ˜¯ä¸€åŠ³æ°¸é€¸å¼å‡†å¤‡æ•°æ®çš„å‡½æ•°ã€‚
        ç”±äºåªåœ¨å•çº¿ç¨‹ä¸­è°ƒç”¨ï¼Œä¸è¦åœ¨è¿™ä¸ªå‡½æ•°ä¸­è¿›è¡Œ self.x=y ä¼¼çš„èµ‹å€¼æ“ä½œã€‚
        ä½†å¦‚æœæ˜¯è‡ªå·±ç”¨è€Œä¸æ˜¯ç»™å¤§ä¼—åˆ†å‘çš„è¯ï¼Œè¿™ä¸ªå‡½æ•°å¯èƒ½å¹¶ä¸éœ€è¦è°ƒç”¨ï¼Œå› ä¸ºæ•°æ®æå‰å¤„ç†å¥½å°±å¥½äº†ã€‚
    setup(self, stage=None) ï¼š
        å®ä¾‹åŒ–æ•°æ®é›†ï¼ˆDatasetï¼‰ï¼Œå¹¶è¿›è¡Œç›¸å…³æ“ä½œï¼Œå¦‚ï¼šæ¸…ç‚¹ç±»æ•°ï¼Œåˆ’åˆ†train/val/testé›†åˆç­‰ã€‚
        å‚æ•° stage ç”¨äºæŒ‡ç¤ºæ˜¯å¤„äºè®­ç»ƒå‘¨æœŸ( fit )è¿˜æ˜¯æµ‹è¯•å‘¨æœŸ( test )ï¼Œå…¶ä¸­ï¼Œ fit å‘¨æœŸéœ€è¦æ„å»ºtrainå’Œvalä¸¤è€…çš„æ•°æ®é›†ã€‚
        setupå‡½æ•°ä¸éœ€è¦è¿”å›å€¼ã€‚åˆå§‹åŒ–å¥½çš„train/val/test setç›´æ¥èµ‹å€¼ç»™selfå³å¯ã€‚
    train_dataloader/val_dataloader/test_dataloader ï¼š
        åˆå§‹åŒ– DataLoader ã€‚
        è¿”å›ä¸€ä¸ªDataLoaderé‡ã€‚

ç¤ºä¾‹

 class MNISTDataModule ( pl . LightningDataModule ): def __init__ ( self , data_dir : str = './' , batch_size : int = 64 , num_workers : int = 8 ): super () . __init__ () self . data_dir = data_dir self . batch_size = batch_size self . num_workers = num_workers self . transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)) ]) # self.dims is returned when you call dm.size() # Setting default dims here because we know them. # Could optionally be assigned dynamically in dm.setup() self . dims = ( 1 , 28 , 28 ) self . num_classes = 10 def prepare_data ( self ): # download MNIST ( self . data_dir , train = True , download = True ) MNIST ( self . data_dir , train = False , download = True ) def setup ( self , stage = None ): # Assign train/val datasets for use in dataloaders if stage == 'fit' or stage is None : mnist_full = MNIST ( self . data_dir , train = True , transform = self . transform ) self . mnist_train , self . mnist_val = random_split ( mnist_full , [ 55000 , 5000 ]) # Assign test dataset for use in dataloader(s) if stage == 'test' or stage is None : self . mnist_test = MNIST ( self . data_dir , train = False , transform = self . transform ) def train_dataloader ( self ): return DataLoader ( self . mnist_train , batch_size = self . batch_size , num_workers = self . num_workers ) def val_dataloader ( self ): return DataLoader ( self . mnist_val , batch_size = self . batch_size , num_workers = self . num_workers ) def test_dataloader ( self ): return DataLoader ( self . mnist_test , batch_size = self . batch_size , num_workers = self . num_workers )  

è¦ç‚¹

    è‹¥åœ¨DataModuleä¸­å®šä¹‰äº†ä¸€ä¸ª self.dims å˜é‡ï¼Œåé¢å¯ä»¥è°ƒç”¨ dm.size() è·å–è¯¥å˜é‡ã€‚

Saving and Loading

ä¸»é¡µé¢
Saving

    ModelCheckpoint : è‡ªåŠ¨å‚¨å­˜çš„callback moduleã€‚é»˜è®¤æƒ…å†µä¸‹trainingè¿‡ç¨‹ä¸­åªä¼šè‡ªåŠ¨å‚¨å­˜æœ€æ–°çš„æ¨¡å‹ä¸ç›¸å…³å‚æ•°ï¼Œè€Œç”¨æˆ·å¯ä»¥é€šè¿‡è¿™ä¸ªmoduleè‡ªå®šä¹‰ã€‚å¦‚è§‚æµ‹ä¸€ä¸ª val_loss çš„é‡ï¼Œå¹¶å‚¨å­˜top 3å¥½çš„æ¨¡å‹ï¼Œä¸”åŒæ—¶å‚¨å­˜æœ€åä¸€ä¸ªepochçš„æ¨¡å‹ï¼Œç­‰ç­‰ã€‚ä¾‹ï¼š

 from pytorch_lightning.callbacks import ModelCheckpoint # saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt checkpoint_callback = ModelCheckpoint ( monitor = 'val_loss' , filename = 'sample-mnist-{epoch:02d}-{val_loss:.2f}' , save_top_k = 3 , mode = 'min' , save_last = True ) trainer = pl . Trainer ( gpus = 1 , max_epochs = 3 , progress_bar_refresh_rate = 20 , callbacks = [ checkpoint_callback ])  

    å¦å¤–ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å­˜å‚¨checkpoint: trainer.save_checkpoint("example.ckpt")
    ModelCheckpoint Callbackä¸­ï¼Œå¦‚æœ save_weights_only =True ï¼Œé‚£ä¹ˆå°†ä¼šåªå‚¨å­˜æ¨¡å‹çš„æƒé‡ï¼ˆç›¸å½“äº model.save_weights(filepath) ï¼‰ï¼Œåä¹‹ä¼šå‚¨å­˜æ•´ä¸ªæ¨¡å‹ï¼ˆç›¸å½“äº model.save(filepath) ï¼‰ã€‚

Loading

    loadä¸€ä¸ªæ¨¡å‹ï¼ŒåŒ…æ‹¬å®ƒçš„weightsã€biaseså’Œè¶…å‚æ•°ï¼š

 model = MyLightingModule . load_from_checkpoint ( PATH ) print ( model . learning_rate ) # prints the learning_rate you used in this checkpoint model . eval () y_hat = model ( x )  

loadæ¨¡å‹æ—¶æ›¿æ¢ä¸€äº›è¶…å‚æ•°ï¼š

 class LitModel ( LightningModule ): def __init__ ( self , in_dim , out_dim ): super () . __init__ () self . save_hyperparameters () self . l1 = nn . Linear ( self . hparams . in_dim , self . hparams . out_dim ) # if you train and save the model like this it will use these values when loading # the weights. But you can overwrite this LitModel ( in_dim = 32 , out_dim = 10 ) # uses in_dim=32, out_dim=10 model = LitModel . load_from_checkpoint ( PATH ) # uses in_dim=128, out_dim=10 model = LitModel . load_from_checkpoint ( PATH , in_dim = 128 , out_dim = 10 )  

    å®Œå…¨loadè®­ç»ƒçŠ¶æ€ï¼šloadåŒ…æ‹¬æ¨¡å‹çš„ä¸€åˆ‡ï¼Œä»¥åŠå’Œè®­ç»ƒç›¸å…³çš„ä¸€åˆ‡å‚æ•°ï¼Œå¦‚ model, epoch, step, LR schedulers, apex ç­‰

 model = LitModel () trainer = Trainer ( resume_from_checkpoint = 'some/path/to/my_checkpoint.ckpt' ) # automatically restores model, epoch, step, LR schedulers, apex, etc... trainer . fit ( model )  

Callbacks

    Callback æ˜¯ä¸€ä¸ªè‡ªåŒ…å«çš„ç¨‹åºï¼Œå¯ä»¥ä¸è®­ç»ƒæµç¨‹äº¤ç»‡åœ¨ä¸€èµ·ï¼Œè€Œä¸ä¼šæ±¡æŸ“ä¸»è¦çš„ç ”ç©¶é€»è¾‘ã€‚
    Callback å¹¶éåªä¼šåœ¨epochç»“å°¾è°ƒç”¨ã€‚pytorch-lightning æä¾›äº†æ•°åä¸ªhookï¼ˆæ¥å£ï¼Œè°ƒç”¨ä½ç½®ï¼‰å¯ä¾›é€‰æ‹©ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰callbackï¼Œå®ç°ä»»ä½•æƒ³å®ç°çš„æ¨¡å—ã€‚
    æ¨èä½¿ç”¨æ–¹å¼æ˜¯ï¼Œéšé—®é¢˜å’Œé¡¹ç›®å˜åŒ–çš„æ“ä½œï¼Œè¿™äº›å‡½æ•°å†™åˆ°lightning moduleé‡Œé¢ï¼Œè€Œç›¸å¯¹ç‹¬ç«‹ï¼Œç›¸å¯¹è¾…åŠ©æ€§çš„ï¼Œéœ€è¦å¤ç”¨çš„å†…å®¹åˆ™å¯ä»¥å®šä¹‰å•ç‹¬çš„æ¨¡å—ï¼Œä¾›åç»­æ–¹ä¾¿åœ°æ’æ‹”ä½¿ç”¨ã€‚

Callbacksæ¨è

å†…å»ºCallbacks

    EarlyStopping(monitor='early_stop_on', min_delta=0.0, patience=3, verbose=False, mode='min', strict=True) ï¼šæ ¹æ®æŸä¸ªå€¼ï¼Œåœ¨æ•°ä¸ªepochæ²¡æœ‰æå‡çš„æƒ…å†µä¸‹æå‰åœæ­¢è®­ç»ƒã€‚

    å‚æ•°ï¼š
    monitor ( str ) â€“ quantity to be monitored. Default: 'early_stop_on'.
    min_delta ( float ) â€“ minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. Default: 0.0.
    patience ( int ) â€“ number of validation epochs with no improvement after which training will be stopped. Default: 3.
    verbose ( bool ) â€“ verbosity mode. Default: False.
    mode ( str ) â€“ one of 'min', 'max'. In 'min' mode, training will stop when the quantity monitored has stopped decreasing and in 'max' mode it will stop when the quantity monitored has stopped increasing.
    strict ( bool ) â€“ whether to crash the training if monitor is not found in the validation metrics. Default: True.

ç¤ºä¾‹ï¼š

 from pytorch_lightning import Trainer from pytorch_lightning.callbacks import EarlyStopping early_stopping = EarlyStopping ( 'val_loss' ) trainer = Trainer ( callbacks = [ early_stopping ])  

    ModelCheckpoint ï¼šè§ä¸Šæ–‡ Saving and Loading .
    PrintTableMetricsCallback ï¼šåœ¨æ¯ä¸ªepochç»“æŸåæ‰“å°ä¸€ä»½ç»“æœæ•´ç†è¡¨æ ¼ã€‚

 from pl_bolts.callbacks import PrintTableMetricsCallback callback = PrintTableMetricsCallback () trainer = pl . Trainer ( callbacks = [ callback ]) trainer . fit ( ... ) # ------------------------------ # at the end of every epoch it will print # ------------------------------ # lossâ”‚train_lossâ”‚val_lossâ”‚epoch # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ # 2.2541470527648926â”‚2.2541470527648926â”‚2.2158432006835938â”‚0  

Logging

    Logging ï¼šLoggeré»˜è®¤æ˜¯TensorBoardï¼Œä½†å¯ä»¥æŒ‡å®šå„ç§ä¸»æµLogger æ¡†æ¶ ï¼Œå¦‚Comet.mlï¼ŒMLflowï¼ŒNetpuneï¼Œæˆ–ç›´æ¥CSVæ–‡ä»¶ã€‚å¯ä»¥åŒæ—¶ä½¿ç”¨å¤æ•°ä¸ªloggerã€‚

 from pytorch_lightning import loggers as pl_loggers # Default tb_logger = pl_loggers . TensorBoardLogger ( save_dir = os . getcwd (), version = None , name = 'lightning_logs' ) trainer = Trainer ( logger = tb_logger ) # Or use the same format as others tb_logger = pl_loggers . TensorBoardLogger ( 'logs/' ) # One Logger comet_logger = pl_loggers . CometLogger ( save_dir = 'logs/' ) trainer = Trainer ( logger = comet_logger ) # Save code snapshot logger = pl_loggers . TestTubeLogger ( 'logs/' , create_git_tag = True ) # Multiple Logger tb_logger = pl_loggers . TensorBoardLogger ( 'logs/' ) comet_logger = pl_loggers . CometLogger ( save_dir = 'logs/' ) trainer = Trainer ( logger = [ tb_logger , comet_logger ])  

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¯50ä¸ªbatch logä¸€æ¬¡ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´å‚æ•°

    å¦‚æœæƒ³è¦logè¾“å‡ºéscalarï¼ˆæ ‡é‡ï¼‰çš„å†…å®¹ï¼Œå¦‚å›¾ç‰‡ï¼Œæ–‡æœ¬ï¼Œç›´æ–¹å›¾ç­‰ç­‰ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨ self.logger.experiment.add_xxx() æ¥å®ç°æ‰€éœ€æ“ä½œã€‚

 def training_step ( ... ): ... # the logger you used (in this case tensorboard) tensorboard = self . logger . experiment tensorboard . add_image () tensorboard . add_histogram ( ... ) tensorboard . add_figure ( ... )  

    ä½¿ç”¨logï¼šå¦‚æœæ˜¯TensorBoardï¼Œé‚£ä¹ˆï¼š tensorboard --logdir ./lightning_logs ã€‚åœ¨Jupyter Notebookä¸­ï¼Œå¯ä»¥ä½¿ç”¨ï¼š

 # Start tensorboard. % load_ext tensorboard % tensorboard -- logdir lightning_logs /  

åœ¨è¡Œå†…æ‰“å¼€TensorBoardã€‚

    å°æŠ€å·§ï¼šå¦‚æœåœ¨å±€åŸŸç½‘å†…å¼€å¯äº†TensorBoardï¼ŒåŠ ä¸Šflag --bind_all å³å¯ä½¿ç”¨ä¸»æœºåè®¿é—®ï¼š

tensorboard --logdir lightning_logs --bind_all -> http://SERVER-NAME:6006/
Transfer Learning

ä¸»é¡µé¢

 import torchvision.models as models class ImagenetTransferLearning ( LightningModule ): def __init__ ( self ): super () . __init__ () # init a pretrained resnet backbone = models . resnet50 ( pretrained = True ) num_filters = backbone . fc . in_features layers = list ( backbone . children ())[: - 1 ] self . feature_extractor = nn . Sequential ( * layers ) # use the pretrained model to classify cifar-10 (10 image classes) num_target_classes = 10 self . classifier = nn . Linear ( num_filters , num_target_classes ) def forward ( self , x ): self . feature_extractor . eval () with torch . no_grad (): representations = self . feature_extractor ( x ) . flatten ( 1 ) x = self . classifier ( representations ) ...  

å…³äºdeviceæ“ä½œ

LightningModules know what device they are on! Construct tensors on the device directly to avoid CPU->Device transfer.

 # bad t = torch . rand ( 2 , 2 ) . cuda () # good (self is LightningModule) t = torch . rand ( 2 , 2 , device = self . device )  

For tensors that need to be model attributes, it is best practice to register them as buffers in the modulesâ€™ __init__ method:

 # bad self . t = torch . rand ( 2 , 2 , device = self . device ) # good self . register_buffer ( "t" , torch . rand ( 2 , 2 ))  

å‰é¢ä¸¤æ®µæ˜¯æ•™ç¨‹ä¸­çš„æ–‡æœ¬ã€‚ç„¶è€Œå®é™…ä¸Šæœ‰ä¸€ä¸ªæš—å‘ï¼š

å¦‚æœä½ ä½¿ç”¨äº†ä¸€ä¸ªä¸­ç»§çš„ pl.LightningModule ï¼Œè€Œè¿™ä¸ªmoduleé‡Œé¢å®ä¾‹åŒ–äº†æŸä¸ªæ™®é€šçš„ nn.Module ï¼Œè€Œè¿™ä¸ªæ¨¡å‹ä¸­åˆéœ€è¦å†…éƒ¨ç”Ÿæˆä¸€äº›tensorï¼Œæ¯”å¦‚å›¾ç‰‡æ¯ä¸ªé€šé“çš„meanï¼Œstdä¹‹ç±»ï¼Œé‚£ä¹ˆå¦‚æœä½ ä» pl.LightningModule ä¸­passä¸€ä¸ª self.device ï¼Œå®é™…ä¸Šåœ¨ä¸€å¼€å§‹è¿™ä¸ª self.device æ°¸è¿œæ˜¯ cpu ã€‚æ‰€ä»¥å¦‚æœä½ åœ¨è°ƒç”¨çš„ nn.Module çš„ __init__() ä¸­åˆå§‹åŒ–ï¼Œä½¿ç”¨ to(device) æˆ–å¹²è„†ä»€ä¹ˆéƒ½ä¸ç”¨ï¼Œç»“æœå°±æ˜¯å®ƒæ°¸è¿œéƒ½åœ¨ cpu ä¸Šã€‚

ä½†æ˜¯ï¼Œç»è¿‡å®éªŒï¼Œè™½ç„¶ pl.LightningModule åœ¨ __init__() é˜¶æ®µ self.device è¿˜æ˜¯ cpu ï¼Œå½“è¿›å…¥äº† training_step() ä¹‹åï¼Œå°±è¿…é€Ÿå˜ä¸ºäº† cuda ã€‚æ‰€ä»¥ï¼Œå¯¹äºå­æ¨¡å—ï¼Œæœ€ä½³æ–¹æ¡ˆæ˜¯ï¼Œä½¿ç”¨ä¸€ä¸ª forward ä¸­ä¼ å…¥çš„é‡ï¼Œå¦‚ x ï¼Œä½œä¸ºä¸€ä¸ªreferenceå˜é‡ï¼Œç”¨ type_as å‡½æ•°å°†åœ¨æ¨¡å‹ä¸­ç”Ÿæˆçš„tensoréƒ½æ”¾åˆ°å’Œè¿™ä¸ªå‚è€ƒå˜é‡ç›¸åŒçš„deviceä¸Šå³å¯ã€‚

 class RDNFuse ( nn . Module ): ... def init_norm_func ( self , ref ): self . mean = torch . tensor ( np . array ( self . mean_sen ), dtype = torch . float32 ) . type_as ( ref ) def forward ( self , x ): if not hasattr ( self , 'mean' ): self . init_norm_func ( x )  

Points

    pl.seed_everything(1234) ï¼šå¯¹æ‰€æœ‰ç›¸å…³çš„éšæœºé‡å›ºå®šç§å­ã€‚
    ä½¿ç”¨LR Scheduleræ—¶å€™ï¼Œä¸ç”¨è‡ªå·± .step() ã€‚å®ƒä¹Ÿè¢«Trainerè‡ªåŠ¨å¤„ç†äº†ã€‚ Optimization ä¸»é¡µé¢ 

 # Single optimizer for epoch in epochs : for batch in data : loss = model . training_step ( batch , batch_idx , ... ) loss . backward () optimizer . step () optimizer . zero_grad () for scheduler in schedulers : scheduler . step () # Multiple optimizers for epoch in epochs : for batch in data : for opt in optimizers : disable_grads_for_other_optimizers () train_step ( opt ) opt . step () for scheduler in schedulers : scheduler . step ()  

    å…³äºåˆ’åˆ†trainå’Œvalé›†åˆçš„æ–¹æ³•ã€‚ä¸PLæ— å…³ï¼Œä½†å¾ˆå¸¸ç”¨ï¼Œä¸¤ä¸ªä¾‹å­ï¼š
    random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))
    å¦‚ä¸‹ï¼š

 from torch.utils.data import DataLoader , random_split from torchvision.datasets import MNIST mnist_full = MNIST ( self . data_dir , train = True , transform = self . transform ) self . mnist_train , self . mnist_val = random_split ( mnist_full , [ 55000 , 5000 ])  

    Parametersï¼š
    dataset ( Dataset ) â€“ Dataset to be split
    lengths ( sequence ) â€“ lengths of splits to be produced
    generator ( Generator ) â€“ Generator used for the random permutation.

ç¼–è¾‘äº 2021-03-03 03:16
PyTorch
æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰
Python
â€‹ èµåŒ 1104 â€‹ â€‹ 56 æ¡è¯„è®º
â€‹ åˆ†äº«
â€‹ å–œæ¬¢ â€‹ æ”¶è— â€‹ ç”³è¯·è½¬è½½
â€‹
å‘å¸ƒä¸€æ¡å¸¦å›¾è¯„è®ºå§

56 æ¡è¯„è®º
é»˜è®¤
æœ€æ–°
vmvsci
vmvsci
æœ¬æ¥æˆ‘è§‰å¾—plå¤ªé‡äº†, ç»“æœè¦ç”¨åˆ†å¸ƒå¼çš„æˆ‘åˆå›æ¥[é£™æ³ªç¬‘]
2021-11-11
â€‹ å›å¤ â€‹ 10
å˜¿å“ˆ
å˜¿å“ˆ
z-via
æ²¡æœ‰å“
04-26
â€‹ å›å¤ â€‹ èµ
z-via
z-via
å˜¿å“ˆ

è¯·é—®è§£å†³äº†å—ï¼Œæˆ‘ä¹Ÿåœ¨æ‰¾å¦‚ä½•ç”¨plåšå¤šæœºå¤šå¡çš„åˆ†å¸ƒå¼è®­ç»ƒ
04-26
â€‹ å›å¤ â€‹ èµ
å±•å¼€å…¶ä»– 3 æ¡å›å¤ â€‹
é¸Ÿé›€å‘¼æ™´
é¸Ÿé›€å‘¼æ™´
dz, ä½ çš„è¿™ç¯‡æ–‡ç« è¢«æ¬è¿äº†è€Œä¸”æœªæ ‡æ³¨å‡ºå¤„, https:// zhuanlan.zhihu.com/p/46 9409009
2022-06-04
â€‹ å›å¤ â€‹ 5
Takanashi
Takanashi
ä½œè€…
è°¢è°¢ğŸ™å·²ä¸¾æŠ¥
2022-07-18
â€‹ å›å¤ â€‹ èµ
Sky King
Sky King
ä½œè€…ï¼Œæ‚¨å¥½ï¼Œè¯·é—®ï¼Œæ‚¨çŸ¥é“æ€ä¹ˆåœ¨lightingæ¨¡å—ä¸‹ï¼ŒåŠ fgmè¿™ç±»çš„å¯¹æŠ—æ‰°åŠ¨å—
2022-04-10
â€‹ å›å¤ â€‹ 2
å–„è‹¥æ°´
å–„è‹¥æ°´
æŠŠè‡ªåŠ¨æ¢¯åº¦ä¼˜åŒ–å…³é—­å°±è¡Œäº†
2022-05-31
â€‹ å›å¤ â€‹ 2
é“¶è‰²çš„å¤ªé˜³
é“¶è‰²çš„å¤ªé˜³
æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œåœ¨datamoduleé‚£ä¸€èŠ‚ä¸­ï¼Œä¸ºä»€ä¹ˆè¯´â€œå› ä¸ºæ˜¯å•çº¿ç¨‹ï¼Œæ‰€ä»¥æœ€å¥½ä¸è¦åœ¨prepare_dataé‡Œä¸ºå±æ€§èµ‹å€¼â€å‘¢ï¼Ÿ
2021-08-16
â€‹ å›å¤ â€‹ 1
æ…ç‹¬å­¦ä¹ 
æ…ç‹¬å­¦ä¹ 
åŒé—®
2022-03-31
â€‹ å›å¤ â€‹ èµ
é«˜é£
é«˜é£
è¿™ä¸ªå’Œkerasç±»ä¼¼çš„å®šä½å—
2022-05-11
â€‹ å›å¤ â€‹ 1
æˆ‘ä»¬éƒ½ç©ä¸è¿‡æ—¶é—´
æˆ‘ä»¬éƒ½ç©ä¸è¿‡æ—¶é—´
å¤§ä½¬ï¼Œä¸€ä¸ªepochæµ‹è¯•å››æ¬¡check validation set 4 times during a training epoch
trainer = Trainer(val_check_interval=0.25)ï¼Œ
è¿™ç§åšæ³•æœ‰å•¥ç”¨ã€‚ã€‚ã€‚
2022-04-11
â€‹ å›å¤ â€‹ 1
æ—æ—æ—
æ—æ—æ—

è®­ç»ƒæ•°æ®é‡å¤§çš„æ—¶å€™ä¸è‡³äºå¾ˆä¹…æ‰è·‘ä¸€æ¬¡éªŒè¯
2022-08-10
â€‹ å›å¤ â€‹ 4
å­£çƒ¨
å­£çƒ¨
æœ‰ä¸ªé”™åˆ«å­—modle[è°ƒçš®]
2022-01-21
â€‹ å›å¤ â€‹ 1
æŸ¯é£
æŸ¯é£
åœ¨ç»ˆç«¯è¿è¡Œï¼Œä»–çš„è¿›åº¦æ¡æ€ä¹ˆéƒ½ä¸åŠ¨çš„
2021-07-01
â€‹ å›å¤ â€‹ 3
ç¬¬äº”å£°é¹¤å—¥
ç¬¬äº”å£°é¹¤å—¥

https:// towardsdatascience.com/ from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09
08-02
â€‹ å›å¤ â€‹ èµ
ä¸‡æœ‰å¼•åŠ›
ä¸‡æœ‰å¼•åŠ›

æƒ³é—®é—®å¤§ä½¬è¯¥æ€ä¹ˆæ ·æ”¹å˜æ¨¡å‹çš„åˆå§‹åŒ–å‘€ï¼Œæ²¡æ‰¾åˆ°å¯¹åº”çš„æˆ–è€…ç›¸å…³çš„åœ°æ–¹
06-21
â€‹ å›å¤ â€‹ èµ
piggyJ
piggyJ

æ„Ÿè°¢å¤§ä½¬ï¼è¯·é—®ä¸€ä¸‹ï¼Œå¦‚æœæˆ‘æ¯ä¸ªepochéƒ½æƒ³ä¿å­˜æ¨¡å‹ï¼Œä¸”ä¸è¢«æ–°ä¿å­˜çš„æ¨¡å‹æ›¿æ¢æ‰è¯¥æ€ä¹ˆæ”¹å‘¢ï¼Ÿ
05-26
â€‹ å›å¤ â€‹ èµ
ç¾è½®ç¾å¥‚å›
ç¾è½®ç¾å¥‚å›

niurenä½©æœå·²æ”¶è—

03-10
â€‹ å›å¤ â€‹ èµ
Daniel
Daniel

å¤ªæ£’äº†ï¼Œdzæ˜¯æ€ä¹ˆå†™å‡ºæ¥è¿™ä¹ˆæ¼‚äº®çš„æ–‡æ¡£çš„
02-01
â€‹ å›å¤ â€‹ èµ
Takanashi
Takanashi
ä½œè€…
å“ˆå“ˆå“ˆè°¢è°¢ è¾¹åšè¾¹å†™ï¼Ÿèµ·ä¸ªæ¡†æ¶ ç„¶åéšç€é¡¹ç›®æ¨è¿› éœ€è¦è¶Šæ¥è¶Šå¤šçš„åŠŸèƒ½ è¿™ä¸ªè¿‡ç¨‹ä¸­å†å»ä¸°å¯Œæ–‡æ¡£å†…å®¹
08-07
â€‹ å›å¤ â€‹ èµ
Tom.tao
Tom.tao

å¤§ä½¬æƒ³é—®ä¸€ä¸‹ï¼Œplå¯ä»¥åœ¨è™šæ‹Ÿæœºä¸Šå®ç°å—
01-31
â€‹ å›å¤ â€‹ èµ
Takanashi
Takanashi
ä½œè€…
è¿™ä¸ªå’Œè¿è¡Œä½ç½®æ— å…³çš„ åªæ˜¯æƒ³ç”¨gpuçš„è¯è¦ä¿è¯ä½ è™šæ‹Ÿæœºèƒ½è®¿é—®åˆ°gpu
08-07
â€‹ å›å¤ â€‹ èµ
å­¦ä¸€ç§’æ˜¯ä¸€ç§’223
å­¦ä¸€ç§’æ˜¯ä¸€ç§’223

æƒ³é—®ä¸€ä¸‹ç”¨çš„æ˜¯ä»¥å‰ç‰ˆæœ¬çš„plçš„è¯ï¼Œè¯´æ˜æ–‡æ¡£è¯¥å»å“ªé‡Œæ‰¾å‘¢ï¼Ÿåœ¨ç›®å‰å®˜ç½‘ä¸­æ²¡æœ‰çœ‹åˆ°ã€‚
2022-10-21
â€‹ å›å¤ â€‹ èµ
Takanashi
Takanashi
ä½œè€…
pip install pytorch_lightning==<ç‰ˆæœ¬å·>
08-07
â€‹ å›å¤ â€‹ èµ
thinkingæ…¢ç¾Šç¾Š
thinkingæ…¢ç¾Šç¾Š
å¤§ä½¬å¤ªç‰›äº†ï¼Œå¾ˆæœ‰ç”¨ï¼Œè°¢è°¢æ‚¨
2021-11-10
â€‹ å›å¤ â€‹ èµ
ç´¢ç½—æ ¼
ç´¢ç½—æ ¼
è¯·é—®æˆ‘ç”¨ä½ æ–‡ç« é‡Œé¢çš„ModelCheckpointä»£ç ä¿å­˜æ¨¡å‹ï¼Œç»“æœä»€ä¹ˆéƒ½æ²¡æœ‰ä¿å­˜ä¸‹æ¥ï¼Œè¿™æ˜¯ä»€ä¹ˆæƒ…å†µå‘¢ï¼Ÿ
2021-11-03
â€‹ å›å¤ â€‹ èµ
Joe12138
Joe12138

é‚£ä¸ªä»£ç æ˜¯åªä¿å­˜æœ€å¥½çš„ä¸‰ä¸ªæ¨¡å‹
2022-07-06
â€‹ å›å¤ â€‹ èµ
çŸ¥ä¹ç”¨æˆ·fDzE81
çŸ¥ä¹ç”¨æˆ·fDzE81
ç‰›[èµåŒ]
2021-10-14
â€‹ å›å¤ â€‹ èµ
ç™½å°é±¼
ç™½å°é±¼
â€‹
æŠ„é€
@è–°é£åˆå…¥å¼¦
2021-09-15
â€‹ å›å¤ â€‹ èµ
é“¶è‰²çš„å¤ªé˜³
é“¶è‰²çš„å¤ªé˜³
èŠ‚çœäº†åäººæ—¶é—´ï¼Œèµ
2021-08-16
â€‹ å›å¤ â€‹ èµ
ç‚¹å‡»æŸ¥çœ‹å…¨éƒ¨è¯„è®º
å‘å¸ƒä¸€æ¡å¸¦å›¾è¯„è®ºå§

æ–‡ç« è¢«ä»¥ä¸‹ä¸“æ æ”¶å½•

    Miracleyoo's
    Miracleyoo's
    ç§‘ç ”é€”ä¸­çš„ä¸€äº›åˆ†äº«ï¼ŒDL+HCIã€‚
    äººå·¥æ™ºèƒ½ç ”ç©¶
    äººå·¥æ™ºèƒ½ç ”ç©¶
    æ¬¢è¿å¤§å®¶å…³æ³¨â€œäººå·¥æ™ºèƒ½ä¸ç®—æ³•å­¦ä¹ â€å¾®ä¿¡å…¬ä¼—å·ï¼
    ML&&DL
    ML&&DL

æ¨èé˜…è¯»

    Pytorch-Lightningç®€æ˜ä½¿ç”¨æ•™ç¨‹
    Pytorch-Lightningç®€æ˜ä½¿ç”¨æ•™ç¨‹
    ç”µç“¶è½¦ å‘è¡¨äºç”µç“¶è½¦çš„å¿ƒ...
    PyTorch Lightningåˆæ­¥æ•™ç¨‹ï¼ˆä¸‹ï¼‰
    PyTorch Lightningåˆæ­¥æ•™ç¨‹ï¼ˆä¸‹ï¼‰
    ç°ç° å‘è¡¨äºç£åˆ›AI
    PyTorch Lightning å·¥å…·å­¦ä¹ 
    PyTorch Lightning å·¥å…·å­¦ä¹ 
    pprp å‘è¡¨äºGiant...
    PyTorch Lightningåˆæ­¥æ•™ç¨‹ï¼ˆä¸Šï¼‰
    PyTorch Lightningåˆæ­¥æ•™ç¨‹ï¼ˆä¸Šï¼‰
    ç°ç° å‘è¡¨äºç£åˆ›AI

