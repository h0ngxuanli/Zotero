
Skip to main content
Cornell University
We gratefully acknowledge support from
the Simons Foundation and member institutions.
arxiv logo > cs > arXiv:2205.13733

Help | Advanced Search
Search
Computer Science > Machine Learning
(cs)
[Submitted on 27 May 2022 ( v1 ), last revised 16 Dec 2022 (this version, v2)]
Title: Towards Faithful and Consistent Explanations for Graph Neural Networks
Authors: Tianxiang Zhao , Dongsheng Luo , Xiang Zhang , Suhang Wang
Download a PDF of the paper titled Towards Faithful and Consistent Explanations for Graph Neural Networks, by Tianxiang Zhao and 3 other authors
Download PDF

    Abstract: Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over recent years. Instance-level GNN explanation aims to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions. Though various algorithms are proposed, most of them formalize this task by searching the minimal subgraph which can preserve original predictions. However, an inductive bias is deep-rooted in this framework: several subgraphs can result in the same or similar outputs as the original graphs. Consequently, they have the danger of providing spurious explanations and fail to provide consistent explanations. Applying them to explain weakly-performed GNNs would further amplify these issues. To address this problem, we theoretically examine the predictions of GNNs from the causality perspective. Two typical reasons of spurious explanations are identified: confounding effect of latent variables like distribution shift, and causal factors distinct from the original input. Observing that both confounding effects and diverse causal rationales are encoded in internal representations, we propose a simple yet effective countermeasure by aligning embeddings. Concretely, concerning potential shifts in the high-dimensional space, we design a distribution-aware alignment algorithm based on anchors. This new objective is easy to compute and can be incorporated into existing techniques with no or little effort. Theoretical analysis shows that it is in effect optimizing a more faithful explanation objective in design, which further justifies the proposed approach. 

Comments: 	Accepted by WSDM2023
Subjects: 	Machine Learning (cs.LG) ; Social and Information Networks (cs.SI)
Cite as: 	arXiv:2205.13733 [cs.LG]
  	(or arXiv:2205.13733v2 [cs.LG] for this version)
  	https://doi.org/10.48550/arXiv.2205.13733
Focus to learn more
arXiv-issued DOI via DataCite
Related DOI : 	https://doi.org/10.1145/3539597.3570421
Focus to learn more
DOI(s) linking to related resources
Submission history
From: Tianxiang Zhao [ view email ]
[v1] Fri, 27 May 2022 02:58:07 UTC (450 KB)
[v2] Fri, 16 Dec 2022 19:17:50 UTC (1,089 KB)
Full-text links:
Download:

    Download a PDF of the paper titled Towards Faithful and Consistent Explanations for Graph Neural Networks, by Tianxiang Zhao and 3 other authors
    PDF
    Other formats 

( license )
Current browse context:
cs.LG
< prev   |   next >
new | recent | 2205
Change to browse by:
cs
cs.SI
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

a export BibTeX citation Loading...
Bookmark
BibSonomy logo Mendeley logo Reddit logo ScienceWISE logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

