Provably Secure Federated Learning against Malicious Clients
Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong
Duke University {xiaoyu.cao, jinyuan.jia, neil.gong}@duke.edu

arXiv:2102.01854v4 [cs.CR] 27 Oct 2021

Abstract
Federated learning enables clients to collaboratively learn a shared global model without sharing their local training data with a cloud server. However, malicious clients can corrupt the global model to predict incorrect labels for testing examples. Existing defenses against malicious clients leverage Byzantine-robust federated learning methods. However, these methods cannot provably guarantee that the predicted label for a testing example is not affected by malicious clients. We bridge this gap via ensemble federated learning. In particular, given any base federated learning algorithm, we use the algorithm to learn multiple global models, each of which is learnt using a randomly selected subset of clients. When predicting the label of a testing example, we take majority vote among the global models. We show that our ensemble federated learning with any base federated learning algorithm is provably secure against malicious clients. Speciï¬cally, the label predicted by our ensemble global model for a testing example is provably not affected by a bounded number of malicious clients. Moreover, we show that our derived bound is tight. We evaluate our method on MNIST and Human Activity Recognition datasets. For instance, our method can achieve a certiï¬ed accuracy of 88% on MNIST when 20 out of 1,000 clients are malicious.
Introduction
Federated learning (KonecË‡ny` et al. 2016; McMahan et al. 2017) is an emerging machine learning paradigm, which enables many clients (e.g., smartphones, IoT devices, and organizations) to collaboratively learn a model without sharing their local training data with a cloud server. Due to its promise for protecting privacy of the clientsâ€™ local training data and the emerging privacy regulations such as General Data Protection Regulation (GDPR), federated learning has been deployed by industry. For instance, Google has deployed federated learning for next-word prediction on Android Gboard. Existing federated learning methods mainly follow a single-global-model paradigm. Speciï¬cally, a cloud server maintains a global model and each client maintains a local model. The global model is trained via multiple iterations of communications between the clients and server. In
Copyright Â© 2021, Association for the Advancement of Artiï¬cial Intelligence (www.aaai.org). All rights reserved.

each iteration, three steps are performed: 1) the server sends the current global model to the clients; 2) the clients update their local models based on the global model and their local training data, and send the model updates to the server; and 3) the server aggregates the model updates and uses them to update the global model. The learnt global model is then used to predict labels of testing examples.
However, such single-global-model paradigm is vulnerable to security attacks. In particular, an attacker can inject fake clients to federated learning or compromise existing clients, where we call the fake/compromised clients malicious clients. Such malicious clients can corrupt the global model via carefully tampering their local training data or model updates sent to the server. As a result, the corrupted global model has a low accuracy for the normal testing examples (Fang et al. 2020; Xie, Koyejo, and Gupta 2019) or certain attacker-chosen testing examples (Bagdasaryan et al. 2020; Bhagoji et al. 2019; Xie et al. 2020). For instance, when learning an image classiï¬er, the malicious clients can re-label the cars with certain strips as birds in their local training data and scale up their model updates sent to the server, such that the learnt global model incorrectly predicts a car with the strips as bird (Bagdasaryan et al. 2020).
Various Byzantine-robust federated learning methods have been proposed to defend against malicious clients (Blanchard et al. 2017; Chen, Su, and Xu 2017; Mhamdi, Guerraoui, and Rouault 2018; Yin et al. 2018, 2019; Chen et al. 2018; Alistarh, Allen-Zhu, and Li 2018). The main idea of these methods is to mitigate the impact of statistical outliers among the clientsâ€™ model updates. They can bound the difference between the global model parameters learnt without malicious clients and the global model parameters learnt when some clients become malicious. However, these methods cannot provably guarantee that the label predicted by the global model for a testing example is not affected by malicious clients. Indeed, studies showed that malicious clients can still substantially degrade the testing accuracy of a global model learnt by a Byzantine-robust method via carefully tampering their model updates sent to the server (Bhagoji et al. 2019; Fang et al. 2020; Xie, Koyejo, and Gupta 2019).
In this work, we propose ensemble federated learning, the ï¬rst federated learning method that is provably secure against malicious clients. Speciï¬cally, given n clients, we

deï¬ne a subsample as a set of k clients sampled from the n

clients uniformly at random without replacement. For each

subsample, we can learn a global model using a base feder-

ated learning algorithm with the k clients in the subsample.

Since there are

n k

subsamples with k clients,

n k

global

models can be trained in total. Suppose we are given a test-

ing example x. We deï¬ne pi as the fraction of the

n k

global

models that predict label i for x, where i = 1, 2, Â· Â· Â· , L.

We call pi label probability. Our ensemble global model predicts the label with the largest label probability for x.

In other words, our ensemble global model takes a major-

ity vote among the global models to predict label for x.

Since each global model is learnt using a subsample with

k clients, a majority of the global models are learnt using

normal clients when most clients are normal. Therefore, the

majority vote among the global models is secure against a

bounded number of malicious clients.

Theory: Our ï¬rst major theoretical result is that our ensemble global model provably predicts the same label for a testing example x when the number of malicious clients is no larger than a threshold, which we call certiï¬ed security level. Our second major theoretical result is that we prove our derived certiï¬ed security level is tight, i.e., when no assumptions are made on the base federated learning algorithm, it is impossible to derive a certiï¬ed security level that is larger than ours. Note that the certiï¬ed security level may be different for different testing examples.

Algorithm: Computing our certiï¬ed security level for x

requires its largest and second largest label probabilities.

When

n k

is small (e.g., the n clients are dozens of orga-

nizations (Kairouz et al. 2019) and k is small), we can com-

pute the largest and second largest label probabilities ex-

actly via training

n k

global models. However, it is challeng-

ing to compute them exactly when

n k

is large. To address

the computational challenge, we develop a Monte Carlo al-

gorithm to estimate them with probabilistic guarantees via

training N instead of

n k

global models.

Evaluation: We empirically evaluate our method on

MNIST (LeCun, Cortes, and Burges 1998) and Human Ac-

tivity Recognition datasets (Anguita et al. 2013). We dis-

tribute the training examples in MNIST to clients to simu-

late federated learning scenarios, while the Human Activity

Recognition dataset represents a real-world federated learn-

ing scenario, where each user is a client. We use the pop-

ular FedAvg developed by Google (McMahan et al. 2017)

as the base federated learning algorithm. Moreover, we use

certiï¬ed accuracy as our evaluation metric, which is a lower

bound of the testing accuracy that a method can provably

achieve no matter how the malicious clients tamper their lo-

cal training data and model updates. For instance, our en-

semble FedAvg with N = 500 and k = 10 can achieve

a certiï¬ed accuracy of 88% on MNIST when evenly dis-

tributing the training examples among 1,000 clients and 20

of them are malicious.

In summary, our key contributions are as follows:

â€¢ Theory: We propose ensemble federated learning, the ï¬rst provably secure federated learning method against malicious clients. We derive a certiï¬ed security level for

Algorithm 1 Single-global-model federated learning

1: Input: C, globalIter, localIter, Î·, Agg.

2: Output: Global model w.

3: w â† random initialization.

4: for Iter global = 1, 2, Â· Â· Â· , globalIter do

5: /* Step I */

6: The server sends w to the clients.

7: /* Step II */

8: for i âˆˆ C do

9:

wi â† w.

10: for Iter local = 1, 2, Â· Â· Â· , localIter do

11:

Sample a Batch from local training data Di.

12:

wi â† wi âˆ’ Î·âˆ‡Loss(Batch; wi).

13: end for

14:

Send gi = wi âˆ’ w to the server.

15: end for

16: /* Step III */

17: g â† Agg(g1, g2, Â· Â· Â· , g|C|). 18: w â† w âˆ’ Î· Â· g.

19: end for

20: return w.

our ensemble federated learning. Moreover, we prove that our derived certiï¬ed security level is tight.
â€¢ Algorithm: We propose a Monte Carlo algorithm to compute our certiï¬ed security level in practice.
â€¢ Evaluation: We evaluate our methods on MNIST and Human Activity Recognition datasets.
All our proofs are shown in Supplemental Material.
Background on Federated Learning
Assuming we have n clients C = {1, 2, Â· Â· Â· , n} and a cloud server in a federated learning setting. The ith client holds some local training dataset Di, where i = 1, 2, Â· Â· Â· , n. Existing federated learning methods (KonecË‡ny` et al. 2016; McMahan et al. 2017; Wang et al. 2020; Li et al. 2020b) mainly focus on learning a single global model for the n clients. Speciï¬cally, the server maintains a global model and each client maintains a local model. Then, federated learning iteratively performs the following three steps, which are shown in Algorithm 1. In Step I, the server sends the current global model to the clients.1 In Step II, each client trains a local model via ï¬ne-tuning the global model to its local training dataset. In particular, each client performs localIter iterations of stochastic gradient descent with a learning rate Î· to train its local model. Then, each client sends its model update (i.e., the difference between the local model and the global model) to the server. In Step III, the server aggregates the clientsâ€™ model updates according to some aggregation rule Agg and uses the aggregated model update to update the global model. The three steps are repeated for globalIter iterations. Existing federated learning algorithms
1The server may select a subset of clients, but we assume the server sends the global model to all clients for convenience.

essentially use different aggregation rules in Step III. For instance, Google developed FedAvg (McMahan et al. 2017), which computes the average of the clientsâ€™ model updates weighted by the sizes of their local training datasets as the aggregated model update to update the global model.
We call such a federated learning algorithm that learns a single global model base federated learning algorithm and denote it as A. Note that given any subset of the n clients C, a base federated learning algorithm can learn a global model for them. Speciï¬cally, the server learns a global model via iteratively performing the three steps between the server and the given subset of clients.

Our Ensemble Federated Learning

Unlike single-global-model federated learning, our ensem-

ble federated learning trains multiple global models, each of

which is trained using the base algorithm A and a subsample

with k clients sampled from the n clients uniformly at ran-

dom without replacement. Among the n clients C, we have

n k

subsamples with k clients. Therefore,

n k

global models

can be trained in total if we train a global model using each

subsample. For a given testing input x, these global models

may predict different labels for it. We deï¬ne pi as the frac-

tion of the

n k

global models that predict label i for x, where

i = 1, 2, Â· Â· Â· , L. We call pi label probability. Note that pi

is

an

integer

multiplication

of

1
(nk)

,

which

we

will

leverage

to derive a tight security guarantee of ensemble federated

learning. Moreover, pi can also be viewed as the probability that a global model trained on a random subsample with

k clients predicts label i for x. Our ensemble global model

predicts the label with the largest label probability for x, i.e.,

we deï¬ne:

h(C, x) = argmax pi,

(1)

i

where h is our ensemble global model and h(C, x) is the

label that our ensemble global model predicts for x when

the ensemble global model is trained on clients C.

Deï¬ning provable security guarantees against malicious clients: Suppose some of the n clients C become malicious. These malicious clients can arbitrarily tamper their local training data and model updates sent to the server in each iteration of federated learning. We denote by C the set of n clients with malicious ones. Moreover, we denote by M (C ) the number of malicious clients in C , e.g., M (C ) = m means that m clients are malicious. Note that we donâ€™t know which clients are malicious. For a testing example x, our goal is to show that our ensemble global model h provably predicts the same label for x when the number of malicious clients is bounded. Formally, we aim to show the following:

h(C , x) = h(C, x), âˆ€C , M (C ) â‰¤ mâˆ—,

(2)

where h(C , x) is the label that the ensemble global model trained on the clients C predicts for x. We call mâˆ— certiï¬ed
security level. When a global model satisï¬es Equation (2)
for a testing example x, we say the global model achieves
a provable security guarantee for x with a certiï¬ed security level mâˆ—. Note that the certiï¬ed security level may be

different for different testing examples. Next, we derive the certiï¬ed security level of our ensemble global model.

Deriving certiï¬ed security level using exact label prob-

abilities: Suppose we are given a testing example x. As-

suming that, when there are no malicious clients, our en-

semble global model predicts label y for x, py is the largest

label probability, and pz is the second largest label probabil-

ity. Moreover, we denote by py and pz respectively the label probabilities for y and z in the ensemble global model when

there are malicious clients. Suppose m clients become mali-

cious.

Then,

1âˆ’

( ) nâˆ’m k (nk)

fraction

of

subsamples

with

k

clients

include at least one malicious client. In the worst-case sce-

nario, for each global model learnt using a subsample in-

cluding at least one malicious client, its predicted label for

x changes from y to z. Therefore, in the worst-case scenario,

the m malicious clients decrease the largest label probability

py

by

1âˆ’

( ) nâˆ’m k (nk)

and

increase

the

second

largest

label

proba-

bility

pz

by

1âˆ’

( ) nâˆ’m k (nk)

,

i.e.,

we

have

py

=

py

âˆ’ (1 âˆ’

( ) nâˆ’m k (nk)

)

and

pz

=

pz

+ (1 âˆ’

( ) nâˆ’m k (nk)

).

Our

ensemble

global

model

still

predicts label y for x, i.e., h(C , x) = h(C, x) = y, once m

satisï¬es the following inequality:

nâˆ’m

py > pz â‡â‡’ py âˆ’ pz > 2 âˆ’ 2

k n

.

(3)

k

In other words, the largest integer m that satisï¬es the inequality (3) is our certiï¬ed security level mâˆ— for the testing example x. The inequality (3) shows that our certiï¬ed security level is related to the gap py âˆ’ pz between the largest and second largest label probabilities in the ensemble global model trained on the clients C without malicious ones. For instance, when a testing example has a larger gap py âˆ’pz, the inequality (3) may be satisï¬ed by a larger m, which means
that our ensemble global model may have a larger certiï¬ed
security level for the testing example.

Deriving certiï¬ed security level using approximate label

probabilities:

When

n k

is small (e.g., several hundred),

we can compute the exact label probabilities py and pz via

training

n k

global models, and compute the certiï¬ed secu-

rity level via inequality (3). However, when

n k

is large, it

is computationally challenging to compute the exact label

probabilities via training

n k

global models. For instance,

when n = 100 and k = 10, there are already 1.73 Ã— 1013

global models, training all of which is computationally in-

tractable in practice. Therefore, we also derive certiï¬ed se-

curity level using a lower bound py of py (i.e., py â‰¤ py)

and an upper bound pz of pz (i.e., pz â‰¥ pz). We use a lower bound py of py and an upper bound pz of pz because our

certiï¬ed security level is related to the gap py âˆ’ pz and we

aim to estimate a lower bound of the gap. The lower bound

py and upper bound pz may be estimated by different meth-

ods. For instance, in the next section, we propose a Monte

Carlo algorithm to estimate a lower bound py and an upper

bound pz via only training N of the

n k

global models.

Next, we derive our certiï¬ed security level based on the probability bounds py and pz. One way is to replace py and pz in inequality (3) as py and pz, respectively. Formally, we have the following inequality:

nâˆ’m

py âˆ’ pz > 2 âˆ’ 2

k n

.

(4)

k

If an m satisï¬es inequality (4), then the m also satisï¬es in-

equality (3), because py âˆ’ pz â‰¤ py âˆ’ pz. Therefore, we can

ï¬nd the largest integer m that satisï¬es the inequality (4) as the certiï¬ed security level mâˆ—. However, we found that the certiï¬ed security level mâˆ— derived based on inequality (4) is

not tight, i.e., our ensemble global model may still predict

label y for x even if the number of malicious clients is larger

than mâˆ— derived based on inequality (4). The key reason is

that

the

label

probabilities

are

integer

multiplications

of

1
(nk)

.

Therefore, we normalize py and pz as integer multiplications

of

1
(nk)

to

derive

a

tight

certiï¬ed

security

level.

Speciï¬cally,

we derive the certiï¬ed security level as the largest integer m

that satisï¬es the following inequality (formally described in

Theorem 1):

py Â·

n k

n

âˆ’

pz Â·

n k

n

nâˆ’m

>2âˆ’2Â·

k n

.

(5)

k

k

k

Figure 1 illustrates the relationships between py, py, and

py Â·(nk ) (nk)

as well as pz, pz, and

pz Â·(nk ) (nk)

. When an m sat-

isï¬es inequality (4), the m also satisï¬es inequality (5), be-

cause py âˆ’ pz â‰¤

py Â·(nk ) (nk)

âˆ’

pz Â·(nk ) (nk)

. Therefore, the certi-

ï¬ed security level derived based on inequality (4) is smaller

than or equals the certiï¬ed security level derived based on

inequality (5). Note that when py = py and pz = pz, both

(4) and (5) reduce to (3) as the label probabilities are inte-

ger

multiplications

of

1
(nk)

.

The

following

theorem

formally

summarizes our certiï¬ed security level.

Theorem 1. Given n clients C, an arbitrary base federated learning algorithm A, a subsample size k, and a testing example x, we deï¬ne an ensemble global model h as Equation (1). y and z are the labels that have the largest and second largest label probabilities for x in the ensemble global model. py is a lower bound of py and pz is an upper bound of pz. Formally, py and pz satisfy the following conditions:

max
i=y

pi

=

pz

â‰¤

pz

â‰¤

py

â‰¤

py .

(6)

Then, h provably predicts y for x when at most mâˆ— clients in C become malicious, i.e., we have:

h(C , x) = h(C, x) = y, âˆ€C , M (C ) â‰¤ mâˆ—,

(7)

where mâˆ— is the largest integer m (0 â‰¤ m â‰¤ n âˆ’ k) that satisï¬es inequality (5).

Our Theorem 1 is applicable to any base federated learning algorithm, any lower bound py of py and any upper

1
ğ‘› ğ‘˜

}

ğ‘ğ‘§

ğ‘ğ‘§ âˆ™

ğ‘› ğ‘˜

ğ‘›

ğ‘ğ‘§

ğ‘˜

ğ‘ğ‘¦

ğ‘ğ‘¦ âˆ™

ğ‘› ğ‘˜

ğ‘y

ğ‘›

ğ‘˜

Figure 1: An example to illustrate the relationships between

py, py, and

py Â·(nk ) (nk)

as well as pz, pz, and

pz Â·(nk ) (nk)

.

bound pz of pz that satisfy (6). When the lower bound py and upper bound pz are estimated more accurately, i.e., py and pz are respectively closer to py and pz, our certiï¬ed security level may be larger. The following theorem shows that our derived certiï¬ed security level is tight, i.e., when no assumptions on the base federated learning algorithm are made, it is impossible to derive a certiï¬ed security level that is larger than ours for the given probability bounds py and pz.
Theorem 2. Suppose py + pz â‰¤ 1. For any C satisfying M (C ) > mâˆ—, i.e., at least mâˆ— + 1 clients are malicious, there exists a base federated learning algorithm Aâˆ— that satisï¬es (6) but h(C , x) = y or there exist ties.

Computing the Certiï¬ed Security Level

Suppose we are given n clients C, a base federated learn-

ing algorithm A, a subsample size k, and a testing dataset

D with d testing examples. For each testing example xt in

D, we aim to compute its label yË†t predicted by our ensemble

global model h and the corresponding certiï¬ed security level

mË† âˆ—t . To compute the certiï¬ed security level based on our Theorem 1, we need a lower bound pyË†t of the largest label

probability pyË†t and an upper bound pzË†t of the second largest

label probability pzË†t . When

n k

is small, we can compute

the exact label probabilities via training

n k

global models.

When

n k

is large, we propose a Monte Carlo algorithm to

estimate the predicted label and the two probability bounds

for all testing examples in D simultaneously with a conï¬-

dence level 1 âˆ’ Î± via training N of the

n k

global models.

Computing predicted label and probability bounds for

one testing example: We ï¬rst discuss how to compute the

predicted label yË†t and probability bounds pyË†t and pzË†t for one testing example xt. We sample N subsamples with k clients

from the n clients uniformly at random without replace-

ment and use them to train N global models g1, g2, Â· Â· Â· , gN .

We use the N global models to predict labels for xt and

count the frequency of each label. We treat the label with the

largest frequency as the predicted label yË†t. Recall that, based

on the deï¬nition of label probability, a global model trained

on a random subsample with k clients predicts label yË†t for

xt with the label probability pyË†t . Therefore, the frequency NyË†t of the label yË†t among the N global models follows a binomial distribution B(N, pyË†t ) with parameters N and pyË†t . Thus, given NyË†t and N , we can use the standard one-
sided Clopper-Pearson method (Clopper and Pearson 1934)

to estimate a lower bound pyË†t of pyË†t with a conï¬dence level

1âˆ’Î±. Speciï¬cally, we have pyË†t = B (Î±; NyË†t , N âˆ’ NyË†t + 1),

Algorithm 2 Computing Predicted Label and Certiï¬ed Security Level

1: Input: C, A, k, N , D, Î±.

2: Output: Predicted label and certiï¬ed security level for

each testing example in D.

g1, g2, Â· Â· Â· , gN â† SAMPLE&TRAIN(C, A, k, N )

3: for xt in D do

4:

counts[i] â†

N l=1

I(gl(xt)

=

i),

i

âˆˆ

{1,

2,

Â·

Â·Â·

, L}

5: /* I is the indicator function */

6: yË†t â† index of the largest entry in counts (ties are

broken uniformly at random)

7:

pyË†t â† B

Î± d

;

NyË†t

,

N

âˆ’

NyË†t

+

1

8: pzË†t â† 1 âˆ’ pyË†t

9: if pyË†t > pzË†t then

10:

mË† âˆ—t â† SEARCHLEVEL(pyË†t , pzË†t , k, |C|)

11: else

12:

yË†t â† ABSTAIN, mË† âˆ—t â† ABSTAIN

13: end if

14: end for

15: return yË†1, yË†2, Â· Â· Â· , yË†d and mË† âˆ—1, mË† âˆ—2, Â· Â· Â· , mË† âˆ—d

where B(q; v, w) is the qth quantile from a beta distribution with shape parameters v and w. Moreover, we can estimate pzË†t = 1 âˆ’ pyË†t â‰¥ 1 âˆ’ pyË†t â‰¥ pzt as an upper bound of pzË†t .

Computing predicted labels and probability bounds for

d testing examples: One method to compute the predicted

labels and probability bounds for the d testing examples is

to apply the above process to each testing example individ-

ually. However, such method is computationally intractable

because it requires training N global models for every test-

ing example. To address the computational challenge, we

propose a method that only needs to train N global mod-

els in total. Our idea is to split Î± among the d testing ex-

amples. Speciï¬cally, we follow the above process to train N

global models and use them to predict labels for the d test-

ing examples. For each testing example xt, we estimate the

lower bound pyË†t = B

Î± d

;

NyË†t

,

N

âˆ’

NyË†t

+

1

with conï¬-

dence level 1 âˆ’ Î±/d instead of 1 âˆ’ Î±. According to the Bon-

ferroni correction, the simultaneous conï¬dence level of esti-

mating the lower bounds for the d testing examples is 1 âˆ’ Î±.

Following the above process, we still estimate pzË†t = 1 âˆ’ pyË†t as an upper bound of pzË†t for each testing example. Complete algorithm: Algorithm 2 shows our algorithm to

compute the predicted labels and certiï¬ed security levels for

the d testing examples in D. The function SAMPLE&TRAIN

randomly samples N subsamples with k clients and trains N

global models using the base federated learning algorithm

A. Given the probability bounds pyË†t and pzË†t for a testing
example xt, the function SEARCHLEVEL ï¬nds the certiï¬ed security level mË† âˆ—t via ï¬nding the largest integer m that satisï¬es (5). For example, SEARCHLEVEL can simply start m from 0 and iteratively increase it by one until ï¬nding mË† âˆ—t .
Probabilistic guarantees: In Algorithm 2, since we es-

timate the lower bound pyË†t using the Clopper-Pearson

method, there is a probability that the estimated lower bound

Dataset Model architecture Number of clients
globalI ter localI ter Learning rate Î· Batch size

MNIST HAR CNN DNN 1,000 30 3,000 5,000
5 0.001
32

Table 1: Federated learning settings and hyperparameters.

is incorrect, i.e., pyË†t > pyË†t . When the lower bound is esti-
mated incorrectly for a testing example xt, the certiï¬ed security level mË† âˆ—t outputted by Algorithm 2 for xt may also be incorrect, i.e., there may exist an C such that M (C ) â‰¤ mË† âˆ—t but h(C , xt) = yË†t. In other words, our Algorithm 2 has probabilistic guarantees for its outputted certiï¬ed security
levels. However, in the following theorem, we prove the
probability that Algorithm 2 returns an incorrect certiï¬ed security level for at least one testing example is at most Î±.

Theorem 3. The probability that Algorithm 2 returns an incorrect certiï¬ed security level for at least one testing example in D is bounded by Î±, which is equivalent to:

Pr(âˆ©xtâˆˆD(h(C , xt) = yË†t, âˆ€C , M (C ) â‰¤ mË† âˆ—t |yË†t = ABSTAIN))

â‰¥ 1 âˆ’ Î±.

(8)

Note that when the probability bounds are estimated de-

terministically, e.g., when

n k

is small and the exact label

probabilities can be computed via training

n k

global mod-

els, the certiï¬ed security level obtained from our Theorem 1

is also deterministic.

Experiments

Experimental Setup

Datasets, model architectures, and base algorithm: We

use MNIST (LeCun, Cortes, and Burges 1998) and Hu-

man Activity Recognition (HAR) datasets (Anguita et al.

2013). MNIST is used to simulate federated learning sce-

narios, while HAR represents a real-world federated learn-

ing scenario. Speciï¬cally, MNIST has 60,000 training exam-

ples and 10,000 testing examples. We consider n = 1, 000

clients and we split them into 10 groups. We assign a training

example with label l to the lth group of clients with proba-

bility q and assign it to each remaining group with a prob-

ability

1âˆ’q 9

.

After

assigning

a

training

example

to

a

group,

we distribute it to a client in the group uniformly at ran-

dom. The parameter q controls local training data distribu-

tion on clients and we call q degree of non-IID. q = 0.1

means that clientsâ€™ local training data are IID, while a larger

q indicates a larger degree of non-IID. By default, we set

q = 0.5. However, we will study the impact of q (degree

of non-IID) on our method. HAR includes human activity

data from 30 users, each of which is a client. The task is to

predict a userâ€™s activity based on the sensor signals (e.g., ac-

celeration) collected from the userâ€™s smartphone. There are

Certified accuracy @ m

1.0

0.8

0.6

0.4

0.2

FedAvg

Ensemble FedAvg

0.0 0 10 20 30 40 50 60 70
Number of malicious clients m

Certified accuracy @ m

1.0

FedAvg

0.8

Ensemble FedAvg

0.6

0.4

0.2

0.0

0

2

4

6

8

10

Number of malicious clients m

(a) MNIST

(b) HAR

Figure 2: FedAvg vs. ensemble FedAvg.

1.0

0.8

0.6

0.4

k = 10

0.2

k = 20

k = 50

0.0 0 10 20 30 40 50 60 70
Number of malicious clients m

Certified accuracy @ m

1.0

0.8

0.6

0.4

k=2

0.2

k=4

k=6

0.0

0

2

4

6

8

10

Number of malicious clients m

(a) MNIST

(b) HAR

Figure 3: Impact of k on our ensemble FedAvg.

Certified accuracy @ m

6 possible activities (e.g., walking, sitting, and standing), indicating a 6-class classiï¬cation problem. There are 10,299 examples in total and each example has 561 features. We use 75% of each userâ€™s examples as training examples and the rest as testing examples.
We consider a convolutional neural network (CNN) architecture (shown in Supplemental Material) for MNIST. For HAR, we consider a deep neural network (DNN) with two fully-connected hidden layers, each of which contains 256 neurons and uses ReLU as the activation function. We use the popular FedAvg (McMahan et al. 2017) as the base federated learning algorithm. Recall that a base federated learning algorithm has hyperparameters (shown in Algorithm 1): globalIter, localIter, learning rate Î·, and batch size. Table 1 summarizes these hyperparameters for FedAvg in our experiments. In particular, we set the globalIter in Table 1 because FedAvg converges with such settings. Evaluation metric: We use certiï¬ed accuracy as our evaluation metric. Speciï¬cally, we deï¬ne the certiï¬ed accuracy at m malicious clients (denoted as CA@m) for a federated learning method as the fraction of testing examples in the testing dataset D whose labels are correctly predicted by the method and whose certiï¬ed security levels are at least m. Formally, we deï¬ne CA@m as follows:

CA@m =

xt âˆˆD

I(yË†t

=

yt)

Â·

I(mË† âˆ—t

â‰¥

m) ,

(9)

|D|

where I is the indicator function, yt is the true label for xt, and yË†t and mË† âˆ—t are respectively the predicted label and certiï¬ed security level for xt. Intuitively, CA@m means that when at most m clients are malicious, the accuracy of the federated learning method for D is at least CA@m no mat-
ter what attacks the malicious clients use (i.e., no matter
how the malicious clients tamper their local training data and model updates). Note that CA@0 reduces to the stan-
dard accuracy when there are no malicious clients.

Certified accuracy @ m

1.0

0.8

0.6

0.4

N = 100

0.2

N = 500

N = 1000

0.0 0 10 20 30 40 50 60 70
Number of malicious clients m

Certified accuracy @ m

1.0

N = 100

0.8

N = 500

N = 1000
0.6

0.4

0.2

0.0

0

2

4

6

8

10

Number of malicious clients m

(a) MNIST

(b) HAR

Figure 4: Impact of N on our ensemble FedAvg.

1.0

0.8

0.6

0.4

Î± = 0.1

0.2

Î± = 0.01

Î± = 0.001

0.0 0 10 20 30 40 50 60 70
Number of malicious clients m

Certified accuracy @ m

1.0

Î± = 0.1

0.8

Î± = 0.01

Î± = 0.001
0.6

0.4

0.2

0.0

0

2

4

6

8

10

Number of malicious clients m

(a) MNIST

(b) HAR

Figure 5: Impact of Î± on our ensemble FedAvg.

Certified accuracy @ m

When we can compute the exact label probabilities via

training

n k

global models, the CA@m of our ensemble

global model h computed using the certiï¬ed security levels

derived from Theorem 1 is deterministic. When

n k

is large,

we estimate predicted labels and certiï¬ed security levels us-

ing Algorithm 2, and thus our CA@m has a conï¬dence level

1 âˆ’ Î± according to Theorem 3.

Parameter settings: Our method has three parameters: N ,

k, and Î±. Unless otherwise mentioned, we adopt the follow-

ing default settings for them: N = 500, Î± = 0.001, k = 10

for MNIST, and k = 2 for HAR. Under such default set-

ting for HAR, we have

n k

=

30 2

= 435 < N = 500 and

we can compute the exact label probabilities via training 435

global models. Therefore, we have deterministic certiï¬ed ac-

curacy for HAR under the default setting. We will explore

the impact of each parameter while using the default settings

for the other two parameters. For HAR, we set k = 4 when

exploring the impact of N (i.e., Figure 4(b)) and Î± (i.e., Fig-

ure 5(b)) since the default setting k = 2 gives deterministic

certiï¬ed accuracy, making N and Î± not relevant.

Experimental Results
Single-global-model FedAvg vs. ensemble FedAvg: Figure 2 compares single-global-model FedAvg and ensemble FedAvg with respect to certiï¬ed accuracy on the two datasets. When there are no malicious clients (i.e., m = 0), single-global-model FedAvg is more accurate than ensemble FedAvg. This is because ensemble FedAvg uses a subsample of clients to train each global model. However, singleglobal-model FedAvg has 0 certiï¬ed accuracy when just one client is malicious. This is because a single malicious client can arbitrarily manipulate the global model learnt by FedAvg (Blanchard et al. 2017). However, the certiï¬ed accuracy of ensemble FedAvg reduces to 0 when up to 61

1.0

Certified accuracy @ m

0.8

0.6

0.4

q = 0.1

q = 0.5
0.2
q = 0.9

0.0 0 10 20 30 40 50 60 70
Number of malicious clients m

Figure 6: Impact of the degree of non-IID q on MNIST.

and 9 clients (6.1% and 30%) are malicious on MNIST and HAR, respectively. Note that it is unknown whether existing Byzantine-robust federated learning methods have non-zero certiï¬ed accuracy when m > 0, and thus we cannot compare ensemble FedAvg with them.
Impact of k, N , and Î±: Figure 3, 4, and 5 show the impact of k, N , and Î±, respectively. k achieves a trade-off between accuracy under no malicious clients and security under malicious clients. Speciï¬cally, when k is larger, the ensemble global model is more accurate at m = 0, but the certiï¬ed accuracy drops more quickly to 0 as m increases. This is because when k is larger, it is more likely for the sampled k clients to include malicious ones. The certiï¬ed accuracy increases as N or Î± increases. This is because training more global models or a larger Î± allows Algorithm 2 to estimate tighter probability bounds and larger certiï¬ed security levels. When N increases from 100 to 500, the certiï¬ed accuracy increases signiï¬cantly. However, when N further grows to 1,000, the increase of certiï¬ed accuracy is marginal. Our results show that we donâ€™t need to train too many global models in practice, as the certiï¬ed accuracy saturates when N is larger than some threshold.
Impact of degree of non-IID q: Figure 6 shows the certiï¬ed accuracy of our ensemble FedAvg on MNIST when the clientsâ€™ local training data have different degrees of nonIID. We observe that the certiï¬ed accuracy drops when q increases from 0.5 to 0.9, which represents a high degree of non-IID. However, the certiï¬ed accuracy is still high when m is small for q = 0.9, e.g., the certiï¬ed accuracy is still 83% when m = 10. This is because although each global model trained using a subsample of clients is less accurate when the local training data are highly non-IID, the ensemble of multiple global models is still accurate.
Related Work
In federated learning, the ï¬rst category of studies (Smith et al. 2017; Li et al. 2020b; Wang et al. 2020; Liu et al. 2020; Peng et al. 2020) aim to design federated learning methods that can learn more accurate global models and/or analyze their convergence properties. For instance, FedMA (Wang et al. 2020) constructs the global model via matching and averaging the hidden elements in a neural network with similar feature extraction signatures. The second category of studies (KonecË‡ny` et al. 2016; McMahan et al. 2017; Wen et al. 2017; Alistarh et al. 2017; Lee et al. 2017; Sahu et al. 2018; Bernstein et al. 2018; Vogels, Karimireddy, and Jaggi 2019;

Yurochkin et al. 2019; Mohri, Sivek, and Suresh 2019; Wang et al. 2020; Li, Wen, and He 2020; Li et al. 2020c; Hamer, Mohri, and Suresh 2020; Rothchild et al. 2020; Malinovsky et al. 2020) aim to improve the communication efï¬ciency between the clients and server via sparsiï¬cation, quantization, and/or encoding of the model updates sent from the clients to the server. The third category of studies (Bonawitz et al. 2017; Geyer, Klein, and Nabi 2017; Hitaj, Ateniese, and Perez-Cruz 2017; Melis et al. 2019; Zhu, Liu, and Han 2019; Mohri, Sivek, and Suresh 2019; Wang, Tong, and Shi 2020; Li et al. 2020a) aim to explore the privacy/fairness issues of federated learning and their defenses.These studies often assume a single global model is shared among the clients. Smith et al. (Smith et al. 2017) proposed to learn a customized model for each client via multi-task learning.
Our work is on security of federated learning, which is orthogonal to the studies above. Multiple studies (Fang et al. 2020; Bagdasaryan et al. 2020; Xie, Koyejo, and Gupta 2019; Bhagoji et al. 2019) showed that the global modelâ€™s accuracy can be signiï¬cantly downgraded by malicious clients. Existing defenses against malicious clients leverage Byzantine-robust aggregation rules such as Krum (Blanchard et al. 2017), trimmed mean (Yin et al. 2018), coordinate-wise median (Yin et al. 2018), and Bulyan (Mhamdi, Guerraoui, and Rouault 2018). However, they cannot provably guarantee that the global modelâ€™s predicted label for a testing example is not affected by malicious clients. As a result, they may be broken by strong attacks that carefully craft the model updates sent from the malicious clients to the server, e.g., (Fang et al. 2020). We propose ensemble federated learning whose predicted label for a testing example is provably not affected by a bounded number of malicious clients.
We note that ensemble methods were also proposed as provably secure defenses (e.g., (Jia, Cao, and Gong 2020)) against data poisoning attacks. However, they are insufï¬cient to defend against malicious clients that can manipulate both the local training data and the model updates. In particular, a provably secure defense against data poisoning attacks guarantees that the label predicted for a testing example is unaffected by a bounded number of poisoned training examples. However, a single malicious client can poison an arbitrary number of its local training examples, breaking the assumption of provably secure defenses against data poisoning attacks.
Conclusion
In this work, we propose ensemble federated learning and derive its tight provable security guarantee against malicious clients. Moreover, we propose an algorithm to compute the certiï¬ed security levels. Our empirical results on two datasets show that our ensemble federated learning can effectively defend against malicious clients with provable security guarantees. Interesting future work includes estimating the probability bounds deterministically and considering the internal structure of a base federated learning algorithm to further improve our provable security guarantees.

Acknowledgement
We thank the anonymous reviewers for insightful reviews. This work was supported by NSF grant No.1937786.
References
Alistarh, D.; Allen-Zhu, Z.; and Li, J. 2018. Byzantine stochastic gradient descent. In NeurIPS.
Alistarh, D.; Grubic, D.; Li, J.; Tomioka, R.; and Vojnovic, M. 2017. QSGD: Communication-efï¬cient SGD via gradient quantization and encoding. In NeurIPS.
Anguita, D.; Ghio, A.; Oneto, L.; Parra, X.; and Reyes-Ortiz, J. L. 2013. A public domain dataset for human activity recognition using smartphones. In ESANN.
Bagdasaryan, E.; Veit, A.; Hua, Y.; Estrin, D.; and Shmatikov, V. 2020. How to backdoor federated learning. In AISTATS.
Bernstein, J.; Wang, Y.-X.; Azizzadenesheli, K.; and Anandkumar, A. 2018. signSGD: Compressed Optimisation for Non-Convex Problems. In ICML.
Bhagoji, A.; Chakraborty, S.; Mittal, P.; and Calo, S. 2019. Analyzing Federated Learning through an Adversarial Lens. In ICML.
Blanchard, P.; Mhamdi, E. M. E.; Guerraoui, R.; and Stainer, J. 2017. Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent. In NeurIPS.
Bonawitz, K.; Ivanov, V.; Kreuter, B.; Marcedone, A.; McMahan, H. B.; Patel, S.; Ramage, D.; Segal, A.; and Seth, K. 2017. Practical secure aggregation for privacy-preserving machine learning. In CCS.
Chen, L.; Wang, H.; Charles, Z.; and Papailiopoulos, D. 2018. DRACO: Byzantine-resilient Distributed Training via Redundant Gradients. In ICML.
Chen, Y.; Su, L.; and Xu, J. 2017. Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent. In POMACS.
Clopper, C. J.; and Pearson, E. S. 1934. The use of conï¬dence or ï¬ducial limits illustrated in the case of the binomial. Biometrika .
Fang, M.; Cao, X.; Jia, J.; and Gong, N. Z. 2020. Local model poisoning attacks to Byzantine-robust federated learning. In USENIX Security.
Geyer, R. C.; Klein, T.; and Nabi, M. 2017. Differentially private federated learning: A client level perspective. arXiv preprint arXiv:1712.07557 .
Hamer, J.; Mohri, M.; and Suresh, A. T. 2020. FedBoost: Communication-Efï¬cient Algorithms for Federated Learning. In ICML.
Hitaj, B.; Ateniese, G.; and Perez-Cruz, F. 2017. Deep models under the GAN: information leakage from collaborative deep learning. In CCS.

Jia, J.; Cao, X.; and Gong, N. Z. 2020. Intrinsic certiï¬ed robustness of bagging against data poisoning attacks. arXiv preprint arXiv:2008.04495 .
Kairouz, P.; McMahan, H. B.; Avent, B.; Bellet, A.; Bennis, M.; Bhagoji, A. N.; Bonawitz, K.; Charles, Z.; Cormode, G.; Cummings, R.; et al. 2019. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977 .
KonecË‡ny`, J.; McMahan, H. B.; Yu, F. X.; RichtaÂ´rik, P.; Suresh, A. T.; and Bacon, D. 2016. Federated learning: Strategies for improving communication efï¬ciency. In NeurIPS Workshop on Private Multi-Party Machine Learning.
LeCun, Y.; Cortes, C.; and Burges, C. 1998. MNIST handwritten digit database. Available: http://yann. lecun. com/exdb/mnist .
Lee, K.; Lam, M.; Pedarsani, R.; Papailiopoulos, D.; and Ramchandran, K. 2017. Speeding up distributed machine learning using codes. IEEE Transactions on Information Theory .
Li, Q.; Wen, Z.; and He, B. 2020. Practical Federated Gradient Boosting Decision Trees. In AAAI.
Li, T.; Sanjabi, M.; Beirami, A.; and Smith, V. 2020a. Fair Resource Allocation in Federated Learning. In ICLR.
Li, X.; Huang, K.; Yang, W.; Wang, S.; and Zhang, Z. 2020b. On the convergence of fedavg on non-iid data. In ICLR.
Li, Z.; Kovalev, D.; Qian, X.; and RichtaÂ´rik, P. 2020c. Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization. In ICML.
Liu, F.; Wu, X.; Ge, S.; Fan, W.; and Zou, Y. 2020. Federated Learning for Vision-and-Language Grounding Problems. In AAAI.
Malinovsky, G.; Kovalev, D.; Gasanov, E.; Condat, L.; and Richtarik, P. 2020. From Local SGD to Local Fixed Point Methods for Federated Learning. In ICML.
McMahan, H. B.; Moore, E.; Ramage, D.; Hampson, S.; et al. 2017. Communication-efï¬cient learning of deep networks from decentralized data. In AISTATS.
Melis, L.; Song, C.; De Cristofaro, E.; and Shmatikov, V. 2019. Exploiting unintended feature leakage in collaborative learning. In IEEE S&P.
Mhamdi, E. M. E.; Guerraoui, R.; and Rouault, S. 2018. The Hidden Vulnerability of Distributed Learning in Byzantium. In ICML.
Mohri, M.; Sivek, G.; and Suresh, A. T. 2019. Agnostic Federated Learning. In ICML.
Peng, X.; Huang, Z.; Zhu, Y.; and Saenko, K. 2020. Federated Adversarial Domain Adaptation. In ICLR.
Rothchild, D.; Panda, A.; Ullah, E.; Ivkin, N.; Stoica, I.; Braverman, V.; Gonzalez, J.; and Arora, R. 2020. FetchSGD: Communication-Efï¬cient Federated Learning with Sketching. In ICML.
Sahu, A. K.; Li, T.; Sanjabi, M.; Zaheer, M.; Talwalkar, A.; and Smith, V. 2018. On the convergence of federated

optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127 .
Smith, V.; Chiang, C.-K.; Sanjabi, M.; and Talwalkar, A. S. 2017. Federated multi-task learning. In NeurIPS.
Vogels, T.; Karimireddy, S. P.; and Jaggi, M. 2019. PowerSGD: Practical low-rank gradient compression for distributed optimization. In NeurIPS.
Wang, H.; Yurochkin, M.; Sun, Y.; Papailiopoulos, D.; and Khazaeni, Y. 2020. Federated Learning with Matched Averaging. In ICLR.
Wang, Y.; Tong, Y.; and Shi, D. 2020. Federated Latent Dirichlet Allocation: A Local Differential Privacy Based Framework. In AAAI.
Wen, W.; Xu, C.; Yan, F.; Wu, C.; Wang, Y.; Chen, Y.; and Li, H. 2017. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In NeurIPS.
Xie, C.; Huang, K.; Chen, P.-Y.; and Li, B. 2020. DBA: Distributed Backdoor Attacks against Federated Learning. In ICLR.
Xie, C.; Koyejo, S.; and Gupta, I. 2019. Fall of empires: Breaking byzantine-tolerant SGD by inner product manipulation. In UAI.
Yin, D.; Chen, Y.; Kannan, R.; and Bartlett, P. 2019. Defending Against Saddle Point Attack in Byzantine-Robust Distributed Learning. In ICML.
Yin, D.; Chen, Y.; Ramchandran, K.; and Bartlett, P. 2018. Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates. In ICML.
Yurochkin, M.; Agarwal, M.; Ghosh, S.; Greenewald, K.; Hoang, N.; and Khazaeni, Y. 2019. Bayesian Nonparametric Federated Learning of Neural Networks. In ICML.
Zhu, L.; Liu, Z.; and Han, S. 2019. Deep leakage from gradients. In NeurIPS.

Proof of Theorem 1

Layer Input Convolution + ReLU Max Pooling Convolution + ReLU Max Pooling Fully Connected + ReLU Softmax

Size 28 Ã— 28 Ã— 1 5 Ã— 5 Ã— 20
2Ã—2 5 Ã— 5 Ã— 50
2Ã—2 512 10

Table 2: The CNN architecture for MNIST.

ğ‘œ

Figure 7: Illustration of OC , OC , and Oo.
We ï¬rst deï¬ne a subsample of k clients from C as S(C, k). Then, we deï¬ne the space of all possible subsamples from C as OC = {S(C, k)} and the space of all possible subsamples from C as OC = {S(C , k)}. Let Oo = {S(C âˆ© C , k)} = OC âˆ© OC denote the space of all possible subsamples from the set of normal clients C âˆ© C , and O = {S(C âˆª C , k)} = OC âˆª OC denote the space of all possible subsamples from either C or C . Figure 7 illustrates OC, OC, and Oo. We use a random variable X to denote a subsample S(C, k) and Y to denote a subsample S(C , k) in O. We know that X and Y have the following probability distributions:

Pr(X = s) =

1
(nk)

,

if s âˆˆ OC

(10)

0, otherwise,

Pr(Y = s) =

1
(nk)

,

if s âˆˆ OC

(11)

0, otherwise.

Recall that given a set of clients s, the base federated learning algorithm A learns a global model. For simplicity, we denote by A(s, x) the predicted label of a testing example x given by this global model. We have the following equations:

py = Pr(A(X, x) = y)

(12)

= Pr(A(X, x) = y|X âˆˆ Oo) Â· Pr(X âˆˆ Oo)

+ Pr(A(X, x) = y|X âˆˆ (OC âˆ’ Oo)) Â· Pr(X âˆˆ (OC âˆ’ Oo)),

(13)

py = Pr(A(Y, x) = y)

(14)

= Pr(A(Y, x) = y|Y âˆˆ Oo) Â· Pr(Y âˆˆ Oo)

+ Pr(A(Y, x) = y|Y âˆˆ (OC âˆ’ Oo)) Â· Pr(Y âˆˆ (OC âˆ’ Oo)).

(15)

Note that we have:

Pr(A(X, x) = y|X âˆˆ Oo) = Pr(A(Y, x) = y|Y âˆˆ Oo),

(16)

nâˆ’m

Pr(X âˆˆ Oo) = Pr(Y âˆˆ Oo) =

k n

,

(17)

k

where m is the number of malicious clients. Therefore, we know:

Pr(A(X, x) = y|X âˆˆ Oo) Â· Pr(X âˆˆ Oo) = Pr(A(Y, x) = y|Y âˆˆ Oo) Â· Pr(Y âˆˆ Oo).

(18)

By subtracting (13) from (15), we obtain:

py âˆ’ py = Pr(A(Y, x) = y|Y âˆˆ (OC âˆ’ Oo)) Â· Pr(Y âˆˆ (OC âˆ’ Oo))

âˆ’ Pr(A(X, x) = y|X âˆˆ (OC âˆ’ Oo)) Â· Pr(X âˆˆ (OC âˆ’ Oo)).

(19)

Similarly, we have the following equation for any i = y:

pi âˆ’ pi = Pr(A(Y, x) = i|Y âˆˆ (OC âˆ’ Oo)) Â· Pr(Y âˆˆ (OC âˆ’ Oo))

âˆ’ Pr(A(X, x) = i|X âˆˆ (OC âˆ’ Oo)) Â· Pr(X âˆˆ (OC âˆ’ Oo)).

(20)

Therefore, we can show:

py âˆ’ pi = py âˆ’ pi + (py âˆ’ py) âˆ’ (pi âˆ’ pi)

(21)

= py âˆ’ pi

+ [Pr(A(Y, x) = y|Y âˆˆ (OC âˆ’ Oo)) âˆ’ Pr(A(Y, x) = i|Y âˆˆ (OC âˆ’ Oo))] Â· Pr(Y âˆˆ (OC âˆ’ Oo))

âˆ’ [Pr(A(X, x) = y|X âˆˆ (OC âˆ’ Oo)) âˆ’ Pr(A(X, x) = i|X âˆˆ (OC âˆ’ Oo))] Â· Pr(X âˆˆ (OC âˆ’ Oo)).

(22)

Note that we have:

Pr(A(Y, x) = y|Y âˆˆ (OC âˆ’ Oo)) âˆ’ Pr(A(Y, x) = i|Y âˆˆ (OC âˆ’ Oo)) â‰¥ âˆ’1,

(23)

Pr(A(X, x) = y|X âˆˆ (OC âˆ’ Oo)) âˆ’ Pr(A(X, x) = i|X âˆˆ (OC âˆ’ Oo)) â‰¤ 1,

(24)

nâˆ’m

Pr(Y âˆˆ (OC âˆ’ Oo)) = Pr(X âˆˆ (OC âˆ’ Oo)) = 1 âˆ’

k n

.

(25)

k

Therefore,

based

on

(22)

and

that

py

and

pi

are

integer

multiplications

of

1
(nk)

,

we

have

the

following:

nâˆ’m

nâˆ’m

py âˆ’ pi â‰¥ py âˆ’ pi + (âˆ’1) Â· 1 âˆ’

k n

âˆ’ 1âˆ’

k n

(26)

k

k

nâˆ’m

= py âˆ’ pi âˆ’ 2 âˆ’ 2 Â·

k n

(27)

k

=

py Â·

n k

n

k

âˆ’

pi Â·

n k

n

k

nâˆ’m

âˆ’2 1âˆ’

k n

k

(28)

py Â·

n k

â‰¥

n

k

âˆ’

pz Â·

n k

n

k

nâˆ’mâˆ—

âˆ’2 1âˆ’

k n

k

(29)

> 0,

(30)

which indicates h(C , x) = y.

Proof of Theorem 2
We prove Theorem 2 by constructing a base federated learning algorithm Aâˆ— such that the conditions in (6) are satisï¬ed but h(C , x) = y or there exist ties.
We follow the deï¬nitions of O, OC , OC , Oo, X, and Y in the previous section. Next, we consider four cases (Figure 8 illustrates them).

ğ´ ğ´
ğµ ğµğµ
(a) Case 1
ğ‘œ ğ´ ğµ ğµğ‘œ ğ´ ğµğµ ğµ

ğ‘œ

ğ´

ğµğ‘œ

ğ´
ğµğµ

(b) Case 2
ğ´
ğ´ğ´ ğ‘œ ğµğ‘œ
ğµğµ

(c) Case 3

(d) Case 4

Figure 8: Illustration of OC , OC , Oo, OA, and OB in the four cases.

Case 1: m â‰¥ n âˆ’ k. In this case, we know Oo = âˆ…. Let OA âŠ† OC and OB âŠ† OC such that |OA| = OA âˆ© OB = âˆ…. Since py + pz â‰¤ 1, we have:

py Â·

n k

, |OB| =

pz Â·

n k

, and

n

n

|OA| + |OB| = py Â· k + pz Â· k

(31)

n

n

â‰¤ py Â· k + (1 âˆ’ py) Â· k

(32)

n

n

n

= py Â· k + k âˆ’ py Â· k

(33)

n

= k = |OC |.

(34)

Therefore, we can always ï¬nd such a pair of disjoint sets (OA, OB). Figure 8(a) illustrates OA, OB, OC , and OC . We can construct Aâˆ— as follows:

ï£±y, ï£²

if s âˆˆ OA

Aâˆ—(s, x) = z,

if s âˆˆ OB âˆª OC

(35)

ï£³i, i = y and i = z, otherwise.

We can show that such Aâˆ— satisï¬es the following probability properties:

py

=

Pr(Aâˆ—(X, x)

=

y)

=

|OA| |OC |

=

py Â·

n k

n k

â‰¥ py,

(36)

pz

=

Pr(Aâˆ—(X, x)

=

z)

=

|OB | |OC |

=

pz Â·

n k

n

k

â‰¤ pz.

(37)

Therefore, Aâˆ— satisï¬es the probability conditions in (6). However, we have:

pz = Pr(Aâˆ—(Y, x) = z) = 1,

(38)

which indicates h(C , x) = z = y.

Case 2:

mâˆ— < m < n âˆ’ k, 0 â‰¤ py

â‰¤1âˆ’

( ) nâˆ’m k (nk)

,

and

0

â‰¤

pz

â‰¤

( ) nâˆ’m k (nk)

.

Let OA âŠ† OC âˆ’ Oo such that |OA| =

py Â·

n k

. Let OB âŠ† Oo such that |OB| =

pz Â·

n k

. Figure 8(b) illustrates

OA, OB, OC , OC , and Oo. We can construct a federated learning algorithm Aâˆ— as follows:

ï£±y,

if s âˆˆ OA

ï£²

Aâˆ—(s, x) = z,

if s âˆˆ OB âˆª (OC âˆ’ Oo)

(39)

ï£³i, i = y and i = z, otherwise.

We can show that such Aâˆ— satisï¬es the following probability conditions:

py

=

Pr(Aâˆ—(X, x)

=

y)

=

|OA| |OC |

=

py Â·

n k

n k

â‰¥ py,

(40)

pz

=

Pr(Aâˆ—(X, x)

=

z)

=

|OB | |OC |

=

pz Â·

n k

n

k

â‰¤ pz,

(41)

which indicates Aâˆ— satisï¬es (6). However, we have:

py âˆ’ pz = Pr(Aâˆ—(Y, x) = y) âˆ’ Pr(Aâˆ—(Y, x) = z)

= 0 âˆ’ |OB| + |OC âˆ’ Oo| |OC |

=âˆ’

pz Â·

n k

n

nâˆ’m

âˆ’1+

k n

k

k

< 0,

which implies h(C , x) = y.

Case 3:

mâˆ— < m < n âˆ’ k, 0 â‰¤ py

â‰¤1âˆ’

( ) nâˆ’m k (nk)

,

and

( ) nâˆ’m k (nk)

â‰¤ pz

â‰¤ 1 âˆ’ py.

Let OA âŠ† OC âˆ’ Oo and OB âŠ† OC âˆ’ Oo such that |OA| =

py Â·

n k

, |OB| =

pz Â·

n k

Note that |OC âˆ’ Oo| =

n k

âˆ’

nâˆ’m k

, and

we have:

(42) (43)
(44) (45)

âˆ’

nâˆ’m k

, and OA

âˆ© OB

=

âˆ….

n

n

nâˆ’m

|OA| + |OB| = py Â· k + pz Â· k âˆ’ k

(46)

n

n

nâˆ’m

â‰¤ py Â· k + (1 âˆ’ py) Â· k âˆ’ k

(47)

n

n

n

nâˆ’m

= py Â· k + k âˆ’ py Â· k âˆ’ k

(48)

n

nâˆ’m

=âˆ’

.

(49)

k

k

Therefore, we can always ï¬nd a pair of such disjoint sets (OA, OB). Figure 8(c) illustrates OA, OB, OC , OC , and Oo. We can construct an algorithm Aâˆ— as follows:

ï£±y,

if s âˆˆ OA

ï£²

Aâˆ—(s, x) = z,

if s âˆˆ OB âˆª OC

(50)

ï£³i, i = y and i = z, otherwise.

We can show that such Aâˆ— satisï¬es the following probability conditions:

py

=

Pr(Aâˆ—(X, x)

=

y)

=

|OA| |OC |

=

py Â·

n k

n k

â‰¥ py,

(51)

pz

=

Pr(Aâˆ—(X, x)

=

z)

=

|OB| + |Oo| |OC |

=

pz Â·

n k

n

k

â‰¤ pz,

(52)

which are consistent with the probability conditions in (6). However, we can show the following:

pz = Pr(Aâˆ—(Y, x) = z) = 1,

(53)

which gives h(C , x) = z = y.

Case 4:

mâˆ— < m < n âˆ’ k, 1 âˆ’

( ) nâˆ’m k (nk)

< py

â‰¤ 1, and 0 â‰¤ pz

â‰¤ 1 âˆ’ py

<

( ) nâˆ’m k (nk)

.

Let OA âŠ† Oo and OB âŠ† Co such that |OA| =

py Â·

n k

+

nâˆ’m k

âˆ’

n k

,

|OB |

=

|Oo| =

nâˆ’m k

, and we have:

pz Â·

n k

, and OA âˆ© OB = âˆ…. Note that

n

nâˆ’m

n

n

|OA| + |OB| = py Â· k + k âˆ’ k + pz Â· k

(54)

n

nâˆ’m

n

n

â‰¤ py Â· k + k âˆ’ k + (1 âˆ’ py) Â· k

(55)

n

nâˆ’m

n

n

n

= py Â· k +

k

âˆ’+ k

k âˆ’ py Â· k

(56)

nâˆ’m

=

.

(57)

k

Therefore, we can always ï¬nd such a pair of disjoint sets (OA, OB). Figure 8(d) illustrates OA, OB, OC , OC , and Oo. Next, we can construct an algorithm Aâˆ— as follows:

ï£±y,

if s âˆˆ OA âˆª (OC âˆ’ Oo)

ï£²

Aâˆ—(s, x) = z,

if s âˆˆ OB âˆª (OC âˆ’ Oo)

(58)

ï£³i, i = y and i = z, otherwise.

We can show that Aâˆ— has the following properties:

py

=

Pr(Aâˆ—(X, x)

=

y)

=

|OA|

+ |OC |OC |

âˆ’

Oo|

=

py Â·

n k

n k

â‰¥ py,

(59)

pz

=

Pr(Aâˆ—(X, x)

=

z)

=

|OB | |OC |

=

pz Â·

n k

n

k

â‰¤ pz,

(60)

which implies Aâˆ— satisï¬es the probability conditions in (6). However, we also have:

py âˆ’ pz = Pr(Aâˆ—(Y, x) = y) âˆ’ Pr(Aâˆ—(Y, x) = z)

(61)

= |OA| âˆ’ |OB| + |OC âˆ’ Oo|

(62)

|OC |

|OC |

py Â·

n k

=

+

nâˆ’m k

âˆ’

n k

n

âˆ’

pz Â·

n k

k

âˆ’

nâˆ’m k

+

n k

n

k

(63)

py Â·

n k

=

n

âˆ’

pz Â·

n k

n

nâˆ’m

âˆ’ 2âˆ’2Â·

k n

.

(64)

k

k

k

Since m > mâˆ—, we have:

py Â·

n k

n

âˆ’

pz Â·

n k

n

nâˆ’m

â‰¤ 2âˆ’2Â·

k n

.

(65)

k

k

k

Therefore, we have py âˆ’ pz â‰¤ 0, which indicates h(C , x) = y or there exist ties.
To summarize, we have proven that in any possible cases, Theorem 2 holds, indicating that our derived certiï¬ed security level is tight.

Proof of Theorem 3

Based on the Clopper-Pearson method, for each testing example xt, we have:

Î±

Pr(pyË†t

â‰¤

Pr(A(S(C, k), xt)

=

yË†t)

âˆ§

pzË†t

â‰¥

Pr(A(S(C, k), xt)

=

i), âˆ€i

=

yË†t)

â‰¥

1

âˆ’

. d

(66)

Therefore, for a testing example xt, if our Algorithm 2 does not abstain for xt, the probability that it returns an incorrect

certiï¬ed

security

level

is

at

most

Î± d

.

Formally,

we

have

the

following:

Pr((âˆƒC

, M (C

)

â‰¤

mË† âˆ—t , h(C

, xt)

=

yË†t)|yË†t

=

ABSTAIN)

â‰¤

Î± .
d

(67)

Therefore, we have the following:

Pr(âˆ©xtâˆˆD((âˆ€C , M (C ) â‰¤ mË† âˆ—t , h(C , xt) = yË†t)|yË†t = ABSTAIN))

(68)

= 1 âˆ’ Pr(âˆªxtâˆˆD((âˆƒC , M (C ) â‰¤ mË† âˆ—t , h(C , xt) = yË†t)|yË†t = ABSTAIN))

(69)

â‰¥1âˆ’

Pr((âˆƒC , M (C ) â‰¤ mË† âˆ—t , h(C , xt) = yË†t)|yË†t = ABSTAIN)

(70)

xt âˆˆD

Î±

â‰¥1âˆ’dÂ·

(71)

d

= 1 âˆ’ Î±.

(72)

We have (70) from (69) based on the Booleâ€™s inequality.

