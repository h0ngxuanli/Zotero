Provably Secure Federated Learning against Malicious Clients
Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong
Duke University {xiaoyu.cao, jinyuan.jia, neil.gong}@duke.edu

arXiv:2102.01854v4 [cs.CR] 27 Oct 2021

Abstract
Federated learning enables clients to collaboratively learn a shared global model without sharing their local training data with a cloud server. However, malicious clients can corrupt the global model to predict incorrect labels for testing examples. Existing defenses against malicious clients leverage Byzantine-robust federated learning methods. However, these methods cannot provably guarantee that the predicted label for a testing example is not affected by malicious clients. We bridge this gap via ensemble federated learning. In particular, given any base federated learning algorithm, we use the algorithm to learn multiple global models, each of which is learnt using a randomly selected subset of clients. When predicting the label of a testing example, we take majority vote among the global models. We show that our ensemble federated learning with any base federated learning algorithm is provably secure against malicious clients. Speciﬁcally, the label predicted by our ensemble global model for a testing example is provably not affected by a bounded number of malicious clients. Moreover, we show that our derived bound is tight. We evaluate our method on MNIST and Human Activity Recognition datasets. For instance, our method can achieve a certiﬁed accuracy of 88% on MNIST when 20 out of 1,000 clients are malicious.
Introduction
Federated learning (Konecˇny` et al. 2016; McMahan et al. 2017) is an emerging machine learning paradigm, which enables many clients (e.g., smartphones, IoT devices, and organizations) to collaboratively learn a model without sharing their local training data with a cloud server. Due to its promise for protecting privacy of the clients’ local training data and the emerging privacy regulations such as General Data Protection Regulation (GDPR), federated learning has been deployed by industry. For instance, Google has deployed federated learning for next-word prediction on Android Gboard. Existing federated learning methods mainly follow a single-global-model paradigm. Speciﬁcally, a cloud server maintains a global model and each client maintains a local model. The global model is trained via multiple iterations of communications between the clients and server. In
Copyright © 2021, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved.

each iteration, three steps are performed: 1) the server sends the current global model to the clients; 2) the clients update their local models based on the global model and their local training data, and send the model updates to the server; and 3) the server aggregates the model updates and uses them to update the global model. The learnt global model is then used to predict labels of testing examples.
However, such single-global-model paradigm is vulnerable to security attacks. In particular, an attacker can inject fake clients to federated learning or compromise existing clients, where we call the fake/compromised clients malicious clients. Such malicious clients can corrupt the global model via carefully tampering their local training data or model updates sent to the server. As a result, the corrupted global model has a low accuracy for the normal testing examples (Fang et al. 2020; Xie, Koyejo, and Gupta 2019) or certain attacker-chosen testing examples (Bagdasaryan et al. 2020; Bhagoji et al. 2019; Xie et al. 2020). For instance, when learning an image classiﬁer, the malicious clients can re-label the cars with certain strips as birds in their local training data and scale up their model updates sent to the server, such that the learnt global model incorrectly predicts a car with the strips as bird (Bagdasaryan et al. 2020).
Various Byzantine-robust federated learning methods have been proposed to defend against malicious clients (Blanchard et al. 2017; Chen, Su, and Xu 2017; Mhamdi, Guerraoui, and Rouault 2018; Yin et al. 2018, 2019; Chen et al. 2018; Alistarh, Allen-Zhu, and Li 2018). The main idea of these methods is to mitigate the impact of statistical outliers among the clients’ model updates. They can bound the difference between the global model parameters learnt without malicious clients and the global model parameters learnt when some clients become malicious. However, these methods cannot provably guarantee that the label predicted by the global model for a testing example is not affected by malicious clients. Indeed, studies showed that malicious clients can still substantially degrade the testing accuracy of a global model learnt by a Byzantine-robust method via carefully tampering their model updates sent to the server (Bhagoji et al. 2019; Fang et al. 2020; Xie, Koyejo, and Gupta 2019).
In this work, we propose ensemble federated learning, the ﬁrst federated learning method that is provably secure against malicious clients. Speciﬁcally, given n clients, we

deﬁne a subsample as a set of k clients sampled from the n

clients uniformly at random without replacement. For each

subsample, we can learn a global model using a base feder-

ated learning algorithm with the k clients in the subsample.

Since there are

n k

subsamples with k clients,

n k

global

models can be trained in total. Suppose we are given a test-

ing example x. We deﬁne pi as the fraction of the

n k

global

models that predict label i for x, where i = 1, 2, · · · , L.

We call pi label probability. Our ensemble global model predicts the label with the largest label probability for x.

In other words, our ensemble global model takes a major-

ity vote among the global models to predict label for x.

Since each global model is learnt using a subsample with

k clients, a majority of the global models are learnt using

normal clients when most clients are normal. Therefore, the

majority vote among the global models is secure against a

bounded number of malicious clients.

Theory: Our ﬁrst major theoretical result is that our ensemble global model provably predicts the same label for a testing example x when the number of malicious clients is no larger than a threshold, which we call certiﬁed security level. Our second major theoretical result is that we prove our derived certiﬁed security level is tight, i.e., when no assumptions are made on the base federated learning algorithm, it is impossible to derive a certiﬁed security level that is larger than ours. Note that the certiﬁed security level may be different for different testing examples.

Algorithm: Computing our certiﬁed security level for x

requires its largest and second largest label probabilities.

When

n k

is small (e.g., the n clients are dozens of orga-

nizations (Kairouz et al. 2019) and k is small), we can com-

pute the largest and second largest label probabilities ex-

actly via training

n k

global models. However, it is challeng-

ing to compute them exactly when

n k

is large. To address

the computational challenge, we develop a Monte Carlo al-

gorithm to estimate them with probabilistic guarantees via

training N instead of

n k

global models.

Evaluation: We empirically evaluate our method on

MNIST (LeCun, Cortes, and Burges 1998) and Human Ac-

tivity Recognition datasets (Anguita et al. 2013). We dis-

tribute the training examples in MNIST to clients to simu-

late federated learning scenarios, while the Human Activity

Recognition dataset represents a real-world federated learn-

ing scenario, where each user is a client. We use the pop-

ular FedAvg developed by Google (McMahan et al. 2017)

as the base federated learning algorithm. Moreover, we use

certiﬁed accuracy as our evaluation metric, which is a lower

bound of the testing accuracy that a method can provably

achieve no matter how the malicious clients tamper their lo-

cal training data and model updates. For instance, our en-

semble FedAvg with N = 500 and k = 10 can achieve

a certiﬁed accuracy of 88% on MNIST when evenly dis-

tributing the training examples among 1,000 clients and 20

of them are malicious.

In summary, our key contributions are as follows:

• Theory: We propose ensemble federated learning, the ﬁrst provably secure federated learning method against malicious clients. We derive a certiﬁed security level for

Algorithm 1 Single-global-model federated learning

1: Input: C, globalIter, localIter, η, Agg.

2: Output: Global model w.

3: w ← random initialization.

4: for Iter global = 1, 2, · · · , globalIter do

5: /* Step I */

6: The server sends w to the clients.

7: /* Step II */

8: for i ∈ C do

9:

wi ← w.

10: for Iter local = 1, 2, · · · , localIter do

11:

Sample a Batch from local training data Di.

12:

wi ← wi − η∇Loss(Batch; wi).

13: end for

14:

Send gi = wi − w to the server.

15: end for

16: /* Step III */

17: g ← Agg(g1, g2, · · · , g|C|). 18: w ← w − η · g.

19: end for

20: return w.

our ensemble federated learning. Moreover, we prove that our derived certiﬁed security level is tight.
• Algorithm: We propose a Monte Carlo algorithm to compute our certiﬁed security level in practice.
• Evaluation: We evaluate our methods on MNIST and Human Activity Recognition datasets.
All our proofs are shown in Supplemental Material.
Background on Federated Learning
Assuming we have n clients C = {1, 2, · · · , n} and a cloud server in a federated learning setting. The ith client holds some local training dataset Di, where i = 1, 2, · · · , n. Existing federated learning methods (Konecˇny` et al. 2016; McMahan et al. 2017; Wang et al. 2020; Li et al. 2020b) mainly focus on learning a single global model for the n clients. Speciﬁcally, the server maintains a global model and each client maintains a local model. Then, federated learning iteratively performs the following three steps, which are shown in Algorithm 1. In Step I, the server sends the current global model to the clients.1 In Step II, each client trains a local model via ﬁne-tuning the global model to its local training dataset. In particular, each client performs localIter iterations of stochastic gradient descent with a learning rate η to train its local model. Then, each client sends its model update (i.e., the difference between the local model and the global model) to the server. In Step III, the server aggregates the clients’ model updates according to some aggregation rule Agg and uses the aggregated model update to update the global model. The three steps are repeated for globalIter iterations. Existing federated learning algorithms
1The server may select a subset of clients, but we assume the server sends the global model to all clients for convenience.

essentially use different aggregation rules in Step III. For instance, Google developed FedAvg (McMahan et al. 2017), which computes the average of the clients’ model updates weighted by the sizes of their local training datasets as the aggregated model update to update the global model.
We call such a federated learning algorithm that learns a single global model base federated learning algorithm and denote it as A. Note that given any subset of the n clients C, a base federated learning algorithm can learn a global model for them. Speciﬁcally, the server learns a global model via iteratively performing the three steps between the server and the given subset of clients.

Our Ensemble Federated Learning

Unlike single-global-model federated learning, our ensem-

ble federated learning trains multiple global models, each of

which is trained using the base algorithm A and a subsample

with k clients sampled from the n clients uniformly at ran-

dom without replacement. Among the n clients C, we have

n k

subsamples with k clients. Therefore,

n k

global models

can be trained in total if we train a global model using each

subsample. For a given testing input x, these global models

may predict different labels for it. We deﬁne pi as the frac-

tion of the

n k

global models that predict label i for x, where

i = 1, 2, · · · , L. We call pi label probability. Note that pi

is

an

integer

multiplication

of

1
(nk)

,

which

we

will

leverage

to derive a tight security guarantee of ensemble federated

learning. Moreover, pi can also be viewed as the probability that a global model trained on a random subsample with

k clients predicts label i for x. Our ensemble global model

predicts the label with the largest label probability for x, i.e.,

we deﬁne:

h(C, x) = argmax pi,

(1)

i

where h is our ensemble global model and h(C, x) is the

label that our ensemble global model predicts for x when

the ensemble global model is trained on clients C.

Deﬁning provable security guarantees against malicious clients: Suppose some of the n clients C become malicious. These malicious clients can arbitrarily tamper their local training data and model updates sent to the server in each iteration of federated learning. We denote by C the set of n clients with malicious ones. Moreover, we denote by M (C ) the number of malicious clients in C , e.g., M (C ) = m means that m clients are malicious. Note that we don’t know which clients are malicious. For a testing example x, our goal is to show that our ensemble global model h provably predicts the same label for x when the number of malicious clients is bounded. Formally, we aim to show the following:

h(C , x) = h(C, x), ∀C , M (C ) ≤ m∗,

(2)

where h(C , x) is the label that the ensemble global model trained on the clients C predicts for x. We call m∗ certiﬁed
security level. When a global model satisﬁes Equation (2)
for a testing example x, we say the global model achieves
a provable security guarantee for x with a certiﬁed security level m∗. Note that the certiﬁed security level may be

different for different testing examples. Next, we derive the certiﬁed security level of our ensemble global model.

Deriving certiﬁed security level using exact label prob-

abilities: Suppose we are given a testing example x. As-

suming that, when there are no malicious clients, our en-

semble global model predicts label y for x, py is the largest

label probability, and pz is the second largest label probabil-

ity. Moreover, we denote by py and pz respectively the label probabilities for y and z in the ensemble global model when

there are malicious clients. Suppose m clients become mali-

cious.

Then,

1−

( ) n−m k (nk)

fraction

of

subsamples

with

k

clients

include at least one malicious client. In the worst-case sce-

nario, for each global model learnt using a subsample in-

cluding at least one malicious client, its predicted label for

x changes from y to z. Therefore, in the worst-case scenario,

the m malicious clients decrease the largest label probability

py

by

1−

( ) n−m k (nk)

and

increase

the

second

largest

label

proba-

bility

pz

by

1−

( ) n−m k (nk)

,

i.e.,

we

have

py

=

py

− (1 −

( ) n−m k (nk)

)

and

pz

=

pz

+ (1 −

( ) n−m k (nk)

).

Our

ensemble

global

model

still

predicts label y for x, i.e., h(C , x) = h(C, x) = y, once m

satisﬁes the following inequality:

n−m

py > pz ⇐⇒ py − pz > 2 − 2

k n

.

(3)

k

In other words, the largest integer m that satisﬁes the inequality (3) is our certiﬁed security level m∗ for the testing example x. The inequality (3) shows that our certiﬁed security level is related to the gap py − pz between the largest and second largest label probabilities in the ensemble global model trained on the clients C without malicious ones. For instance, when a testing example has a larger gap py −pz, the inequality (3) may be satisﬁed by a larger m, which means
that our ensemble global model may have a larger certiﬁed
security level for the testing example.

Deriving certiﬁed security level using approximate label

probabilities:

When

n k

is small (e.g., several hundred),

we can compute the exact label probabilities py and pz via

training

n k

global models, and compute the certiﬁed secu-

rity level via inequality (3). However, when

n k

is large, it

is computationally challenging to compute the exact label

probabilities via training

n k

global models. For instance,

when n = 100 and k = 10, there are already 1.73 × 1013

global models, training all of which is computationally in-

tractable in practice. Therefore, we also derive certiﬁed se-

curity level using a lower bound py of py (i.e., py ≤ py)

and an upper bound pz of pz (i.e., pz ≥ pz). We use a lower bound py of py and an upper bound pz of pz because our

certiﬁed security level is related to the gap py − pz and we

aim to estimate a lower bound of the gap. The lower bound

py and upper bound pz may be estimated by different meth-

ods. For instance, in the next section, we propose a Monte

Carlo algorithm to estimate a lower bound py and an upper

bound pz via only training N of the

n k

global models.

Next, we derive our certiﬁed security level based on the probability bounds py and pz. One way is to replace py and pz in inequality (3) as py and pz, respectively. Formally, we have the following inequality:

n−m

py − pz > 2 − 2

k n

.

(4)

k

If an m satisﬁes inequality (4), then the m also satisﬁes in-

equality (3), because py − pz ≤ py − pz. Therefore, we can

ﬁnd the largest integer m that satisﬁes the inequality (4) as the certiﬁed security level m∗. However, we found that the certiﬁed security level m∗ derived based on inequality (4) is

not tight, i.e., our ensemble global model may still predict

label y for x even if the number of malicious clients is larger

than m∗ derived based on inequality (4). The key reason is

that

the

label

probabilities

are

integer

multiplications

of

1
(nk)

.

Therefore, we normalize py and pz as integer multiplications

of

1
(nk)

to

derive

a

tight

certiﬁed

security

level.

Speciﬁcally,

we derive the certiﬁed security level as the largest integer m

that satisﬁes the following inequality (formally described in

Theorem 1):

py ·

n k

n

−

pz ·

n k

n

n−m

>2−2·

k n

.

(5)

k

k

k

Figure 1 illustrates the relationships between py, py, and

py ·(nk ) (nk)

as well as pz, pz, and

pz ·(nk ) (nk)

. When an m sat-

isﬁes inequality (4), the m also satisﬁes inequality (5), be-

cause py − pz ≤

py ·(nk ) (nk)

−

pz ·(nk ) (nk)

. Therefore, the certi-

ﬁed security level derived based on inequality (4) is smaller

than or equals the certiﬁed security level derived based on

inequality (5). Note that when py = py and pz = pz, both

(4) and (5) reduce to (3) as the label probabilities are inte-

ger

multiplications

of

1
(nk)

.

The

following

theorem

formally

summarizes our certiﬁed security level.

Theorem 1. Given n clients C, an arbitrary base federated learning algorithm A, a subsample size k, and a testing example x, we deﬁne an ensemble global model h as Equation (1). y and z are the labels that have the largest and second largest label probabilities for x in the ensemble global model. py is a lower bound of py and pz is an upper bound of pz. Formally, py and pz satisfy the following conditions:

max
i=y

pi

=

pz

≤

pz

≤

py

≤

py .

(6)

Then, h provably predicts y for x when at most m∗ clients in C become malicious, i.e., we have:

h(C , x) = h(C, x) = y, ∀C , M (C ) ≤ m∗,

(7)

where m∗ is the largest integer m (0 ≤ m ≤ n − k) that satisﬁes inequality (5).

Our Theorem 1 is applicable to any base federated learning algorithm, any lower bound py of py and any upper

1
𝑛 𝑘

}

𝑝𝑧

𝑝𝑧 ∙

𝑛 𝑘

𝑛

𝑝𝑧

𝑘

𝑝𝑦

𝑝𝑦 ∙

𝑛 𝑘

𝑝y

𝑛

𝑘

Figure 1: An example to illustrate the relationships between

py, py, and

py ·(nk ) (nk)

as well as pz, pz, and

pz ·(nk ) (nk)

.

bound pz of pz that satisfy (6). When the lower bound py and upper bound pz are estimated more accurately, i.e., py and pz are respectively closer to py and pz, our certiﬁed security level may be larger. The following theorem shows that our derived certiﬁed security level is tight, i.e., when no assumptions on the base federated learning algorithm are made, it is impossible to derive a certiﬁed security level that is larger than ours for the given probability bounds py and pz.
Theorem 2. Suppose py + pz ≤ 1. For any C satisfying M (C ) > m∗, i.e., at least m∗ + 1 clients are malicious, there exists a base federated learning algorithm A∗ that satisﬁes (6) but h(C , x) = y or there exist ties.

Computing the Certiﬁed Security Level

Suppose we are given n clients C, a base federated learn-

ing algorithm A, a subsample size k, and a testing dataset

D with d testing examples. For each testing example xt in

D, we aim to compute its label yˆt predicted by our ensemble

global model h and the corresponding certiﬁed security level

mˆ ∗t . To compute the certiﬁed security level based on our Theorem 1, we need a lower bound pyˆt of the largest label

probability pyˆt and an upper bound pzˆt of the second largest

label probability pzˆt . When

n k

is small, we can compute

the exact label probabilities via training

n k

global models.

When

n k

is large, we propose a Monte Carlo algorithm to

estimate the predicted label and the two probability bounds

for all testing examples in D simultaneously with a conﬁ-

dence level 1 − α via training N of the

n k

global models.

Computing predicted label and probability bounds for

one testing example: We ﬁrst discuss how to compute the

predicted label yˆt and probability bounds pyˆt and pzˆt for one testing example xt. We sample N subsamples with k clients

from the n clients uniformly at random without replace-

ment and use them to train N global models g1, g2, · · · , gN .

We use the N global models to predict labels for xt and

count the frequency of each label. We treat the label with the

largest frequency as the predicted label yˆt. Recall that, based

on the deﬁnition of label probability, a global model trained

on a random subsample with k clients predicts label yˆt for

xt with the label probability pyˆt . Therefore, the frequency Nyˆt of the label yˆt among the N global models follows a binomial distribution B(N, pyˆt ) with parameters N and pyˆt . Thus, given Nyˆt and N , we can use the standard one-
sided Clopper-Pearson method (Clopper and Pearson 1934)

to estimate a lower bound pyˆt of pyˆt with a conﬁdence level

1−α. Speciﬁcally, we have pyˆt = B (α; Nyˆt , N − Nyˆt + 1),

Algorithm 2 Computing Predicted Label and Certiﬁed Security Level

1: Input: C, A, k, N , D, α.

2: Output: Predicted label and certiﬁed security level for

each testing example in D.

g1, g2, · · · , gN ← SAMPLE&TRAIN(C, A, k, N )

3: for xt in D do

4:

counts[i] ←

N l=1

I(gl(xt)

=

i),

i

∈

{1,

2,

·

··

, L}

5: /* I is the indicator function */

6: yˆt ← index of the largest entry in counts (ties are

broken uniformly at random)

7:

pyˆt ← B

α d

;

Nyˆt

,

N

−

Nyˆt

+

1

8: pzˆt ← 1 − pyˆt

9: if pyˆt > pzˆt then

10:

mˆ ∗t ← SEARCHLEVEL(pyˆt , pzˆt , k, |C|)

11: else

12:

yˆt ← ABSTAIN, mˆ ∗t ← ABSTAIN

13: end if

14: end for

15: return yˆ1, yˆ2, · · · , yˆd and mˆ ∗1, mˆ ∗2, · · · , mˆ ∗d

where B(q; v, w) is the qth quantile from a beta distribution with shape parameters v and w. Moreover, we can estimate pzˆt = 1 − pyˆt ≥ 1 − pyˆt ≥ pzt as an upper bound of pzˆt .

Computing predicted labels and probability bounds for

d testing examples: One method to compute the predicted

labels and probability bounds for the d testing examples is

to apply the above process to each testing example individ-

ually. However, such method is computationally intractable

because it requires training N global models for every test-

ing example. To address the computational challenge, we

propose a method that only needs to train N global mod-

els in total. Our idea is to split α among the d testing ex-

amples. Speciﬁcally, we follow the above process to train N

global models and use them to predict labels for the d test-

ing examples. For each testing example xt, we estimate the

lower bound pyˆt = B

α d

;

Nyˆt

,

N

−

Nyˆt

+

1

with conﬁ-

dence level 1 − α/d instead of 1 − α. According to the Bon-

ferroni correction, the simultaneous conﬁdence level of esti-

mating the lower bounds for the d testing examples is 1 − α.

Following the above process, we still estimate pzˆt = 1 − pyˆt as an upper bound of pzˆt for each testing example. Complete algorithm: Algorithm 2 shows our algorithm to

compute the predicted labels and certiﬁed security levels for

the d testing examples in D. The function SAMPLE&TRAIN

randomly samples N subsamples with k clients and trains N

global models using the base federated learning algorithm

A. Given the probability bounds pyˆt and pzˆt for a testing
example xt, the function SEARCHLEVEL ﬁnds the certiﬁed security level mˆ ∗t via ﬁnding the largest integer m that satisﬁes (5). For example, SEARCHLEVEL can simply start m from 0 and iteratively increase it by one until ﬁnding mˆ ∗t .
Probabilistic guarantees: In Algorithm 2, since we es-

timate the lower bound pyˆt using the Clopper-Pearson

method, there is a probability that the estimated lower bound

Dataset Model architecture Number of clients
globalI ter localI ter Learning rate η Batch size

MNIST HAR CNN DNN 1,000 30 3,000 5,000
5 0.001
32

Table 1: Federated learning settings and hyperparameters.

is incorrect, i.e., pyˆt > pyˆt . When the lower bound is esti-
mated incorrectly for a testing example xt, the certiﬁed security level mˆ ∗t outputted by Algorithm 2 for xt may also be incorrect, i.e., there may exist an C such that M (C ) ≤ mˆ ∗t but h(C , xt) = yˆt. In other words, our Algorithm 2 has probabilistic guarantees for its outputted certiﬁed security
levels. However, in the following theorem, we prove the
probability that Algorithm 2 returns an incorrect certiﬁed security level for at least one testing example is at most α.

Theorem 3. The probability that Algorithm 2 returns an incorrect certiﬁed security level for at least one testing example in D is bounded by α, which is equivalent to:

Pr(∩xt∈D(h(C , xt) = yˆt, ∀C , M (C ) ≤ mˆ ∗t |yˆt = ABSTAIN))

≥ 1 − α.

(8)

Note that when the probability bounds are estimated de-

terministically, e.g., when

n k

is small and the exact label

probabilities can be computed via training

n k

global mod-

els, the certiﬁed security level obtained from our Theorem 1

is also deterministic.

Experiments

Experimental Setup

Datasets, model architectures, and base algorithm: We

use MNIST (LeCun, Cortes, and Burges 1998) and Hu-

man Activity Recognition (HAR) datasets (Anguita et al.

2013). MNIST is used to simulate federated learning sce-

narios, while HAR represents a real-world federated learn-

ing scenario. Speciﬁcally, MNIST has 60,000 training exam-

ples and 10,000 testing examples. We consider n = 1, 000

clients and we split them into 10 groups. We assign a training

example with label l to the lth group of clients with proba-

bility q and assign it to each remaining group with a prob-

ability

1−q 9

.

After

assigning

a

training

example

to

a

group,

we distribute it to a client in the group uniformly at ran-

dom. The parameter q controls local training data distribu-

tion on clients and we call q degree of non-IID. q = 0.1

means that clients’ local training data are IID, while a larger

q indicates a larger degree of non-IID. By default, we set

q = 0.5. However, we will study the impact of q (degree

of non-IID) on our method. HAR includes human activity

data from 30 users, each of which is a client. The task is to

predict a user’s activity based on the sensor signals (e.g., ac-

celeration) collected from the user’s smartphone. There are

Certified accuracy @ m

1.0

0.8

0.6

0.4

0.2

FedAvg

Ensemble FedAvg

0.0 0 10 20 30 40 50 60 70
Number of malicious clients m

Certified accuracy @ m

1.0

FedAvg

0.8

Ensemble FedAvg

0.6

0.4

0.2

0.0

0

2

4

6

8

10

Number of malicious clients m

(a) MNIST

(b) HAR

Figure 2: FedAvg vs. ensemble FedAvg.

1.0

0.8

0.6

0.4

k = 10

0.2

k = 20

k = 50

0.0 0 10 20 30 40 50 60 70
Number of malicious clients m

Certified accuracy @ m

1.0

0.8

0.6

0.4

k=2

0.2

k=4

k=6

0.0

0

2

4

6

8

10

Number of malicious clients m

(a) MNIST

(b) HAR

Figure 3: Impact of k on our ensemble FedAvg.

Certified accuracy @ m

6 possible activities (e.g., walking, sitting, and standing), indicating a 6-class classiﬁcation problem. There are 10,299 examples in total and each example has 561 features. We use 75% of each user’s examples as training examples and the rest as testing examples.
We consider a convolutional neural network (CNN) architecture (shown in Supplemental Material) for MNIST. For HAR, we consider a deep neural network (DNN) with two fully-connected hidden layers, each of which contains 256 neurons and uses ReLU as the activation function. We use the popular FedAvg (McMahan et al. 2017) as the base federated learning algorithm. Recall that a base federated learning algorithm has hyperparameters (shown in Algorithm 1): globalIter, localIter, learning rate η, and batch size. Table 1 summarizes these hyperparameters for FedAvg in our experiments. In particular, we set the globalIter in Table 1 because FedAvg converges with such settings. Evaluation metric: We use certiﬁed accuracy as our evaluation metric. Speciﬁcally, we deﬁne the certiﬁed accuracy at m malicious clients (denoted as CA@m) for a federated learning method as the fraction of testing examples in the testing dataset D whose labels are correctly predicted by the method and whose certiﬁed security levels are at least m. Formally, we deﬁne CA@m as follows:

CA@m =

xt ∈D

I(yˆt

=

yt)

·

I(mˆ ∗t

≥

m) ,

(9)

|D|

where I is the indicator function, yt is the true label for xt, and yˆt and mˆ ∗t are respectively the predicted label and certiﬁed security level for xt. Intuitively, CA@m means that when at most m clients are malicious, the accuracy of the federated learning method for D is at least CA@m no mat-
ter what attacks the malicious clients use (i.e., no matter
how the malicious clients tamper their local training data and model updates). Note that CA@0 reduces to the stan-
dard accuracy when there are no malicious clients.

Certified accuracy @ m

1.0

0.8

0.6

0.4

N = 100

0.2

N = 500

N = 1000

0.0 0 10 20 30 40 50 60 70
Number of malicious clients m

Certified accuracy @ m

1.0

N = 100

0.8

N = 500

N = 1000
0.6

0.4

0.2

0.0

0

2

4

6

8

10

Number of malicious clients m

(a) MNIST

(b) HAR

Figure 4: Impact of N on our ensemble FedAvg.

1.0

0.8

0.6

0.4

α = 0.1

0.2

α = 0.01

α = 0.001

0.0 0 10 20 30 40 50 60 70
Number of malicious clients m

Certified accuracy @ m

1.0

α = 0.1

0.8

α = 0.01

α = 0.001
0.6

0.4

0.2

0.0

0

2

4

6

8

10

Number of malicious clients m

(a) MNIST

(b) HAR

Figure 5: Impact of α on our ensemble FedAvg.

Certified accuracy @ m

When we can compute the exact label probabilities via

training

n k

global models, the CA@m of our ensemble

global model h computed using the certiﬁed security levels

derived from Theorem 1 is deterministic. When

n k

is large,

we estimate predicted labels and certiﬁed security levels us-

ing Algorithm 2, and thus our CA@m has a conﬁdence level

1 − α according to Theorem 3.

Parameter settings: Our method has three parameters: N ,

k, and α. Unless otherwise mentioned, we adopt the follow-

ing default settings for them: N = 500, α = 0.001, k = 10

for MNIST, and k = 2 for HAR. Under such default set-

ting for HAR, we have

n k

=

30 2

= 435 < N = 500 and

we can compute the exact label probabilities via training 435

global models. Therefore, we have deterministic certiﬁed ac-

curacy for HAR under the default setting. We will explore

the impact of each parameter while using the default settings

for the other two parameters. For HAR, we set k = 4 when

exploring the impact of N (i.e., Figure 4(b)) and α (i.e., Fig-

ure 5(b)) since the default setting k = 2 gives deterministic

certiﬁed accuracy, making N and α not relevant.

Experimental Results
Single-global-model FedAvg vs. ensemble FedAvg: Figure 2 compares single-global-model FedAvg and ensemble FedAvg with respect to certiﬁed accuracy on the two datasets. When there are no malicious clients (i.e., m = 0), single-global-model FedAvg is more accurate than ensemble FedAvg. This is because ensemble FedAvg uses a subsample of clients to train each global model. However, singleglobal-model FedAvg has 0 certiﬁed accuracy when just one client is malicious. This is because a single malicious client can arbitrarily manipulate the global model learnt by FedAvg (Blanchard et al. 2017). However, the certiﬁed accuracy of ensemble FedAvg reduces to 0 when up to 61

1.0

Certified accuracy @ m

0.8

0.6

0.4

q = 0.1

q = 0.5
0.2
q = 0.9

0.0 0 10 20 30 40 50 60 70
Number of malicious clients m

Figure 6: Impact of the degree of non-IID q on MNIST.

and 9 clients (6.1% and 30%) are malicious on MNIST and HAR, respectively. Note that it is unknown whether existing Byzantine-robust federated learning methods have non-zero certiﬁed accuracy when m > 0, and thus we cannot compare ensemble FedAvg with them.
Impact of k, N , and α: Figure 3, 4, and 5 show the impact of k, N , and α, respectively. k achieves a trade-off between accuracy under no malicious clients and security under malicious clients. Speciﬁcally, when k is larger, the ensemble global model is more accurate at m = 0, but the certiﬁed accuracy drops more quickly to 0 as m increases. This is because when k is larger, it is more likely for the sampled k clients to include malicious ones. The certiﬁed accuracy increases as N or α increases. This is because training more global models or a larger α allows Algorithm 2 to estimate tighter probability bounds and larger certiﬁed security levels. When N increases from 100 to 500, the certiﬁed accuracy increases signiﬁcantly. However, when N further grows to 1,000, the increase of certiﬁed accuracy is marginal. Our results show that we don’t need to train too many global models in practice, as the certiﬁed accuracy saturates when N is larger than some threshold.
Impact of degree of non-IID q: Figure 6 shows the certiﬁed accuracy of our ensemble FedAvg on MNIST when the clients’ local training data have different degrees of nonIID. We observe that the certiﬁed accuracy drops when q increases from 0.5 to 0.9, which represents a high degree of non-IID. However, the certiﬁed accuracy is still high when m is small for q = 0.9, e.g., the certiﬁed accuracy is still 83% when m = 10. This is because although each global model trained using a subsample of clients is less accurate when the local training data are highly non-IID, the ensemble of multiple global models is still accurate.
Related Work
In federated learning, the ﬁrst category of studies (Smith et al. 2017; Li et al. 2020b; Wang et al. 2020; Liu et al. 2020; Peng et al. 2020) aim to design federated learning methods that can learn more accurate global models and/or analyze their convergence properties. For instance, FedMA (Wang et al. 2020) constructs the global model via matching and averaging the hidden elements in a neural network with similar feature extraction signatures. The second category of studies (Konecˇny` et al. 2016; McMahan et al. 2017; Wen et al. 2017; Alistarh et al. 2017; Lee et al. 2017; Sahu et al. 2018; Bernstein et al. 2018; Vogels, Karimireddy, and Jaggi 2019;

Yurochkin et al. 2019; Mohri, Sivek, and Suresh 2019; Wang et al. 2020; Li, Wen, and He 2020; Li et al. 2020c; Hamer, Mohri, and Suresh 2020; Rothchild et al. 2020; Malinovsky et al. 2020) aim to improve the communication efﬁciency between the clients and server via sparsiﬁcation, quantization, and/or encoding of the model updates sent from the clients to the server. The third category of studies (Bonawitz et al. 2017; Geyer, Klein, and Nabi 2017; Hitaj, Ateniese, and Perez-Cruz 2017; Melis et al. 2019; Zhu, Liu, and Han 2019; Mohri, Sivek, and Suresh 2019; Wang, Tong, and Shi 2020; Li et al. 2020a) aim to explore the privacy/fairness issues of federated learning and their defenses.These studies often assume a single global model is shared among the clients. Smith et al. (Smith et al. 2017) proposed to learn a customized model for each client via multi-task learning.
Our work is on security of federated learning, which is orthogonal to the studies above. Multiple studies (Fang et al. 2020; Bagdasaryan et al. 2020; Xie, Koyejo, and Gupta 2019; Bhagoji et al. 2019) showed that the global model’s accuracy can be signiﬁcantly downgraded by malicious clients. Existing defenses against malicious clients leverage Byzantine-robust aggregation rules such as Krum (Blanchard et al. 2017), trimmed mean (Yin et al. 2018), coordinate-wise median (Yin et al. 2018), and Bulyan (Mhamdi, Guerraoui, and Rouault 2018). However, they cannot provably guarantee that the global model’s predicted label for a testing example is not affected by malicious clients. As a result, they may be broken by strong attacks that carefully craft the model updates sent from the malicious clients to the server, e.g., (Fang et al. 2020). We propose ensemble federated learning whose predicted label for a testing example is provably not affected by a bounded number of malicious clients.
We note that ensemble methods were also proposed as provably secure defenses (e.g., (Jia, Cao, and Gong 2020)) against data poisoning attacks. However, they are insufﬁcient to defend against malicious clients that can manipulate both the local training data and the model updates. In particular, a provably secure defense against data poisoning attacks guarantees that the label predicted for a testing example is unaffected by a bounded number of poisoned training examples. However, a single malicious client can poison an arbitrary number of its local training examples, breaking the assumption of provably secure defenses against data poisoning attacks.
Conclusion
In this work, we propose ensemble federated learning and derive its tight provable security guarantee against malicious clients. Moreover, we propose an algorithm to compute the certiﬁed security levels. Our empirical results on two datasets show that our ensemble federated learning can effectively defend against malicious clients with provable security guarantees. Interesting future work includes estimating the probability bounds deterministically and considering the internal structure of a base federated learning algorithm to further improve our provable security guarantees.

Acknowledgement
We thank the anonymous reviewers for insightful reviews. This work was supported by NSF grant No.1937786.
References
Alistarh, D.; Allen-Zhu, Z.; and Li, J. 2018. Byzantine stochastic gradient descent. In NeurIPS.
Alistarh, D.; Grubic, D.; Li, J.; Tomioka, R.; and Vojnovic, M. 2017. QSGD: Communication-efﬁcient SGD via gradient quantization and encoding. In NeurIPS.
Anguita, D.; Ghio, A.; Oneto, L.; Parra, X.; and Reyes-Ortiz, J. L. 2013. A public domain dataset for human activity recognition using smartphones. In ESANN.
Bagdasaryan, E.; Veit, A.; Hua, Y.; Estrin, D.; and Shmatikov, V. 2020. How to backdoor federated learning. In AISTATS.
Bernstein, J.; Wang, Y.-X.; Azizzadenesheli, K.; and Anandkumar, A. 2018. signSGD: Compressed Optimisation for Non-Convex Problems. In ICML.
Bhagoji, A.; Chakraborty, S.; Mittal, P.; and Calo, S. 2019. Analyzing Federated Learning through an Adversarial Lens. In ICML.
Blanchard, P.; Mhamdi, E. M. E.; Guerraoui, R.; and Stainer, J. 2017. Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent. In NeurIPS.
Bonawitz, K.; Ivanov, V.; Kreuter, B.; Marcedone, A.; McMahan, H. B.; Patel, S.; Ramage, D.; Segal, A.; and Seth, K. 2017. Practical secure aggregation for privacy-preserving machine learning. In CCS.
Chen, L.; Wang, H.; Charles, Z.; and Papailiopoulos, D. 2018. DRACO: Byzantine-resilient Distributed Training via Redundant Gradients. In ICML.
Chen, Y.; Su, L.; and Xu, J. 2017. Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent. In POMACS.
Clopper, C. J.; and Pearson, E. S. 1934. The use of conﬁdence or ﬁducial limits illustrated in the case of the binomial. Biometrika .
Fang, M.; Cao, X.; Jia, J.; and Gong, N. Z. 2020. Local model poisoning attacks to Byzantine-robust federated learning. In USENIX Security.
Geyer, R. C.; Klein, T.; and Nabi, M. 2017. Differentially private federated learning: A client level perspective. arXiv preprint arXiv:1712.07557 .
Hamer, J.; Mohri, M.; and Suresh, A. T. 2020. FedBoost: Communication-Efﬁcient Algorithms for Federated Learning. In ICML.
Hitaj, B.; Ateniese, G.; and Perez-Cruz, F. 2017. Deep models under the GAN: information leakage from collaborative deep learning. In CCS.

Jia, J.; Cao, X.; and Gong, N. Z. 2020. Intrinsic certiﬁed robustness of bagging against data poisoning attacks. arXiv preprint arXiv:2008.04495 .
Kairouz, P.; McMahan, H. B.; Avent, B.; Bellet, A.; Bennis, M.; Bhagoji, A. N.; Bonawitz, K.; Charles, Z.; Cormode, G.; Cummings, R.; et al. 2019. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977 .
Konecˇny`, J.; McMahan, H. B.; Yu, F. X.; Richta´rik, P.; Suresh, A. T.; and Bacon, D. 2016. Federated learning: Strategies for improving communication efﬁciency. In NeurIPS Workshop on Private Multi-Party Machine Learning.
LeCun, Y.; Cortes, C.; and Burges, C. 1998. MNIST handwritten digit database. Available: http://yann. lecun. com/exdb/mnist .
Lee, K.; Lam, M.; Pedarsani, R.; Papailiopoulos, D.; and Ramchandran, K. 2017. Speeding up distributed machine learning using codes. IEEE Transactions on Information Theory .
Li, Q.; Wen, Z.; and He, B. 2020. Practical Federated Gradient Boosting Decision Trees. In AAAI.
Li, T.; Sanjabi, M.; Beirami, A.; and Smith, V. 2020a. Fair Resource Allocation in Federated Learning. In ICLR.
Li, X.; Huang, K.; Yang, W.; Wang, S.; and Zhang, Z. 2020b. On the convergence of fedavg on non-iid data. In ICLR.
Li, Z.; Kovalev, D.; Qian, X.; and Richta´rik, P. 2020c. Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization. In ICML.
Liu, F.; Wu, X.; Ge, S.; Fan, W.; and Zou, Y. 2020. Federated Learning for Vision-and-Language Grounding Problems. In AAAI.
Malinovsky, G.; Kovalev, D.; Gasanov, E.; Condat, L.; and Richtarik, P. 2020. From Local SGD to Local Fixed Point Methods for Federated Learning. In ICML.
McMahan, H. B.; Moore, E.; Ramage, D.; Hampson, S.; et al. 2017. Communication-efﬁcient learning of deep networks from decentralized data. In AISTATS.
Melis, L.; Song, C.; De Cristofaro, E.; and Shmatikov, V. 2019. Exploiting unintended feature leakage in collaborative learning. In IEEE S&P.
Mhamdi, E. M. E.; Guerraoui, R.; and Rouault, S. 2018. The Hidden Vulnerability of Distributed Learning in Byzantium. In ICML.
Mohri, M.; Sivek, G.; and Suresh, A. T. 2019. Agnostic Federated Learning. In ICML.
Peng, X.; Huang, Z.; Zhu, Y.; and Saenko, K. 2020. Federated Adversarial Domain Adaptation. In ICLR.
Rothchild, D.; Panda, A.; Ullah, E.; Ivkin, N.; Stoica, I.; Braverman, V.; Gonzalez, J.; and Arora, R. 2020. FetchSGD: Communication-Efﬁcient Federated Learning with Sketching. In ICML.
Sahu, A. K.; Li, T.; Sanjabi, M.; Zaheer, M.; Talwalkar, A.; and Smith, V. 2018. On the convergence of federated

optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127 .
Smith, V.; Chiang, C.-K.; Sanjabi, M.; and Talwalkar, A. S. 2017. Federated multi-task learning. In NeurIPS.
Vogels, T.; Karimireddy, S. P.; and Jaggi, M. 2019. PowerSGD: Practical low-rank gradient compression for distributed optimization. In NeurIPS.
Wang, H.; Yurochkin, M.; Sun, Y.; Papailiopoulos, D.; and Khazaeni, Y. 2020. Federated Learning with Matched Averaging. In ICLR.
Wang, Y.; Tong, Y.; and Shi, D. 2020. Federated Latent Dirichlet Allocation: A Local Differential Privacy Based Framework. In AAAI.
Wen, W.; Xu, C.; Yan, F.; Wu, C.; Wang, Y.; Chen, Y.; and Li, H. 2017. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In NeurIPS.
Xie, C.; Huang, K.; Chen, P.-Y.; and Li, B. 2020. DBA: Distributed Backdoor Attacks against Federated Learning. In ICLR.
Xie, C.; Koyejo, S.; and Gupta, I. 2019. Fall of empires: Breaking byzantine-tolerant SGD by inner product manipulation. In UAI.
Yin, D.; Chen, Y.; Kannan, R.; and Bartlett, P. 2019. Defending Against Saddle Point Attack in Byzantine-Robust Distributed Learning. In ICML.
Yin, D.; Chen, Y.; Ramchandran, K.; and Bartlett, P. 2018. Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates. In ICML.
Yurochkin, M.; Agarwal, M.; Ghosh, S.; Greenewald, K.; Hoang, N.; and Khazaeni, Y. 2019. Bayesian Nonparametric Federated Learning of Neural Networks. In ICML.
Zhu, L.; Liu, Z.; and Han, S. 2019. Deep leakage from gradients. In NeurIPS.

Proof of Theorem 1

Layer Input Convolution + ReLU Max Pooling Convolution + ReLU Max Pooling Fully Connected + ReLU Softmax

Size 28 × 28 × 1 5 × 5 × 20
2×2 5 × 5 × 50
2×2 512 10

Table 2: The CNN architecture for MNIST.

𝑜

Figure 7: Illustration of OC , OC , and Oo.
We ﬁrst deﬁne a subsample of k clients from C as S(C, k). Then, we deﬁne the space of all possible subsamples from C as OC = {S(C, k)} and the space of all possible subsamples from C as OC = {S(C , k)}. Let Oo = {S(C ∩ C , k)} = OC ∩ OC denote the space of all possible subsamples from the set of normal clients C ∩ C , and O = {S(C ∪ C , k)} = OC ∪ OC denote the space of all possible subsamples from either C or C . Figure 7 illustrates OC, OC, and Oo. We use a random variable X to denote a subsample S(C, k) and Y to denote a subsample S(C , k) in O. We know that X and Y have the following probability distributions:

Pr(X = s) =

1
(nk)

,

if s ∈ OC

(10)

0, otherwise,

Pr(Y = s) =

1
(nk)

,

if s ∈ OC

(11)

0, otherwise.

Recall that given a set of clients s, the base federated learning algorithm A learns a global model. For simplicity, we denote by A(s, x) the predicted label of a testing example x given by this global model. We have the following equations:

py = Pr(A(X, x) = y)

(12)

= Pr(A(X, x) = y|X ∈ Oo) · Pr(X ∈ Oo)

+ Pr(A(X, x) = y|X ∈ (OC − Oo)) · Pr(X ∈ (OC − Oo)),

(13)

py = Pr(A(Y, x) = y)

(14)

= Pr(A(Y, x) = y|Y ∈ Oo) · Pr(Y ∈ Oo)

+ Pr(A(Y, x) = y|Y ∈ (OC − Oo)) · Pr(Y ∈ (OC − Oo)).

(15)

Note that we have:

Pr(A(X, x) = y|X ∈ Oo) = Pr(A(Y, x) = y|Y ∈ Oo),

(16)

n−m

Pr(X ∈ Oo) = Pr(Y ∈ Oo) =

k n

,

(17)

k

where m is the number of malicious clients. Therefore, we know:

Pr(A(X, x) = y|X ∈ Oo) · Pr(X ∈ Oo) = Pr(A(Y, x) = y|Y ∈ Oo) · Pr(Y ∈ Oo).

(18)

By subtracting (13) from (15), we obtain:

py − py = Pr(A(Y, x) = y|Y ∈ (OC − Oo)) · Pr(Y ∈ (OC − Oo))

− Pr(A(X, x) = y|X ∈ (OC − Oo)) · Pr(X ∈ (OC − Oo)).

(19)

Similarly, we have the following equation for any i = y:

pi − pi = Pr(A(Y, x) = i|Y ∈ (OC − Oo)) · Pr(Y ∈ (OC − Oo))

− Pr(A(X, x) = i|X ∈ (OC − Oo)) · Pr(X ∈ (OC − Oo)).

(20)

Therefore, we can show:

py − pi = py − pi + (py − py) − (pi − pi)

(21)

= py − pi

+ [Pr(A(Y, x) = y|Y ∈ (OC − Oo)) − Pr(A(Y, x) = i|Y ∈ (OC − Oo))] · Pr(Y ∈ (OC − Oo))

− [Pr(A(X, x) = y|X ∈ (OC − Oo)) − Pr(A(X, x) = i|X ∈ (OC − Oo))] · Pr(X ∈ (OC − Oo)).

(22)

Note that we have:

Pr(A(Y, x) = y|Y ∈ (OC − Oo)) − Pr(A(Y, x) = i|Y ∈ (OC − Oo)) ≥ −1,

(23)

Pr(A(X, x) = y|X ∈ (OC − Oo)) − Pr(A(X, x) = i|X ∈ (OC − Oo)) ≤ 1,

(24)

n−m

Pr(Y ∈ (OC − Oo)) = Pr(X ∈ (OC − Oo)) = 1 −

k n

.

(25)

k

Therefore,

based

on

(22)

and

that

py

and

pi

are

integer

multiplications

of

1
(nk)

,

we

have

the

following:

n−m

n−m

py − pi ≥ py − pi + (−1) · 1 −

k n

− 1−

k n

(26)

k

k

n−m

= py − pi − 2 − 2 ·

k n

(27)

k

=

py ·

n k

n

k

−

pi ·

n k

n

k

n−m

−2 1−

k n

k

(28)

py ·

n k

≥

n

k

−

pz ·

n k

n

k

n−m∗

−2 1−

k n

k

(29)

> 0,

(30)

which indicates h(C , x) = y.

Proof of Theorem 2
We prove Theorem 2 by constructing a base federated learning algorithm A∗ such that the conditions in (6) are satisﬁed but h(C , x) = y or there exist ties.
We follow the deﬁnitions of O, OC , OC , Oo, X, and Y in the previous section. Next, we consider four cases (Figure 8 illustrates them).

𝐴 𝐴
𝐵 𝐵𝐵
(a) Case 1
𝑜 𝐴 𝐵 𝐵𝑜 𝐴 𝐵𝐵 𝐵

𝑜

𝐴

𝐵𝑜

𝐴
𝐵𝐵

(b) Case 2
𝐴
𝐴𝐴 𝑜 𝐵𝑜
𝐵𝐵

(c) Case 3

(d) Case 4

Figure 8: Illustration of OC , OC , Oo, OA, and OB in the four cases.

Case 1: m ≥ n − k. In this case, we know Oo = ∅. Let OA ⊆ OC and OB ⊆ OC such that |OA| = OA ∩ OB = ∅. Since py + pz ≤ 1, we have:

py ·

n k

, |OB| =

pz ·

n k

, and

n

n

|OA| + |OB| = py · k + pz · k

(31)

n

n

≤ py · k + (1 − py) · k

(32)

n

n

n

= py · k + k − py · k

(33)

n

= k = |OC |.

(34)

Therefore, we can always ﬁnd such a pair of disjoint sets (OA, OB). Figure 8(a) illustrates OA, OB, OC , and OC . We can construct A∗ as follows:

y, 

if s ∈ OA

A∗(s, x) = z,

if s ∈ OB ∪ OC

(35)

i, i = y and i = z, otherwise.

We can show that such A∗ satisﬁes the following probability properties:

py

=

Pr(A∗(X, x)

=

y)

=

|OA| |OC |

=

py ·

n k

n k

≥ py,

(36)

pz

=

Pr(A∗(X, x)

=

z)

=

|OB | |OC |

=

pz ·

n k

n

k

≤ pz.

(37)

Therefore, A∗ satisﬁes the probability conditions in (6). However, we have:

pz = Pr(A∗(Y, x) = z) = 1,

(38)

which indicates h(C , x) = z = y.

Case 2:

m∗ < m < n − k, 0 ≤ py

≤1−

( ) n−m k (nk)

,

and

0

≤

pz

≤

( ) n−m k (nk)

.

Let OA ⊆ OC − Oo such that |OA| =

py ·

n k

. Let OB ⊆ Oo such that |OB| =

pz ·

n k

. Figure 8(b) illustrates

OA, OB, OC , OC , and Oo. We can construct a federated learning algorithm A∗ as follows:

y,

if s ∈ OA



A∗(s, x) = z,

if s ∈ OB ∪ (OC − Oo)

(39)

i, i = y and i = z, otherwise.

We can show that such A∗ satisﬁes the following probability conditions:

py

=

Pr(A∗(X, x)

=

y)

=

|OA| |OC |

=

py ·

n k

n k

≥ py,

(40)

pz

=

Pr(A∗(X, x)

=

z)

=

|OB | |OC |

=

pz ·

n k

n

k

≤ pz,

(41)

which indicates A∗ satisﬁes (6). However, we have:

py − pz = Pr(A∗(Y, x) = y) − Pr(A∗(Y, x) = z)

= 0 − |OB| + |OC − Oo| |OC |

=−

pz ·

n k

n

n−m

−1+

k n

k

k

< 0,

which implies h(C , x) = y.

Case 3:

m∗ < m < n − k, 0 ≤ py

≤1−

( ) n−m k (nk)

,

and

( ) n−m k (nk)

≤ pz

≤ 1 − py.

Let OA ⊆ OC − Oo and OB ⊆ OC − Oo such that |OA| =

py ·

n k

, |OB| =

pz ·

n k

Note that |OC − Oo| =

n k

−

n−m k

, and

we have:

(42) (43)
(44) (45)

−

n−m k

, and OA

∩ OB

=

∅.

n

n

n−m

|OA| + |OB| = py · k + pz · k − k

(46)

n

n

n−m

≤ py · k + (1 − py) · k − k

(47)

n

n

n

n−m

= py · k + k − py · k − k

(48)

n

n−m

=−

.

(49)

k

k

Therefore, we can always ﬁnd a pair of such disjoint sets (OA, OB). Figure 8(c) illustrates OA, OB, OC , OC , and Oo. We can construct an algorithm A∗ as follows:

y,

if s ∈ OA



A∗(s, x) = z,

if s ∈ OB ∪ OC

(50)

i, i = y and i = z, otherwise.

We can show that such A∗ satisﬁes the following probability conditions:

py

=

Pr(A∗(X, x)

=

y)

=

|OA| |OC |

=

py ·

n k

n k

≥ py,

(51)

pz

=

Pr(A∗(X, x)

=

z)

=

|OB| + |Oo| |OC |

=

pz ·

n k

n

k

≤ pz,

(52)

which are consistent with the probability conditions in (6). However, we can show the following:

pz = Pr(A∗(Y, x) = z) = 1,

(53)

which gives h(C , x) = z = y.

Case 4:

m∗ < m < n − k, 1 −

( ) n−m k (nk)

< py

≤ 1, and 0 ≤ pz

≤ 1 − py

<

( ) n−m k (nk)

.

Let OA ⊆ Oo and OB ⊆ Co such that |OA| =

py ·

n k

+

n−m k

−

n k

,

|OB |

=

|Oo| =

n−m k

, and we have:

pz ·

n k

, and OA ∩ OB = ∅. Note that

n

n−m

n

n

|OA| + |OB| = py · k + k − k + pz · k

(54)

n

n−m

n

n

≤ py · k + k − k + (1 − py) · k

(55)

n

n−m

n

n

n

= py · k +

k

−+ k

k − py · k

(56)

n−m

=

.

(57)

k

Therefore, we can always ﬁnd such a pair of disjoint sets (OA, OB). Figure 8(d) illustrates OA, OB, OC , OC , and Oo. Next, we can construct an algorithm A∗ as follows:

y,

if s ∈ OA ∪ (OC − Oo)



A∗(s, x) = z,

if s ∈ OB ∪ (OC − Oo)

(58)

i, i = y and i = z, otherwise.

We can show that A∗ has the following properties:

py

=

Pr(A∗(X, x)

=

y)

=

|OA|

+ |OC |OC |

−

Oo|

=

py ·

n k

n k

≥ py,

(59)

pz

=

Pr(A∗(X, x)

=

z)

=

|OB | |OC |

=

pz ·

n k

n

k

≤ pz,

(60)

which implies A∗ satisﬁes the probability conditions in (6). However, we also have:

py − pz = Pr(A∗(Y, x) = y) − Pr(A∗(Y, x) = z)

(61)

= |OA| − |OB| + |OC − Oo|

(62)

|OC |

|OC |

py ·

n k

=

+

n−m k

−

n k

n

−

pz ·

n k

k

−

n−m k

+

n k

n

k

(63)

py ·

n k

=

n

−

pz ·

n k

n

n−m

− 2−2·

k n

.

(64)

k

k

k

Since m > m∗, we have:

py ·

n k

n

−

pz ·

n k

n

n−m

≤ 2−2·

k n

.

(65)

k

k

k

Therefore, we have py − pz ≤ 0, which indicates h(C , x) = y or there exist ties.
To summarize, we have proven that in any possible cases, Theorem 2 holds, indicating that our derived certiﬁed security level is tight.

Proof of Theorem 3

Based on the Clopper-Pearson method, for each testing example xt, we have:

α

Pr(pyˆt

≤

Pr(A(S(C, k), xt)

=

yˆt)

∧

pzˆt

≥

Pr(A(S(C, k), xt)

=

i), ∀i

=

yˆt)

≥

1

−

. d

(66)

Therefore, for a testing example xt, if our Algorithm 2 does not abstain for xt, the probability that it returns an incorrect

certiﬁed

security

level

is

at

most

α d

.

Formally,

we

have

the

following:

Pr((∃C

, M (C

)

≤

mˆ ∗t , h(C

, xt)

=

yˆt)|yˆt

=

ABSTAIN)

≤

α .
d

(67)

Therefore, we have the following:

Pr(∩xt∈D((∀C , M (C ) ≤ mˆ ∗t , h(C , xt) = yˆt)|yˆt = ABSTAIN))

(68)

= 1 − Pr(∪xt∈D((∃C , M (C ) ≤ mˆ ∗t , h(C , xt) = yˆt)|yˆt = ABSTAIN))

(69)

≥1−

Pr((∃C , M (C ) ≤ mˆ ∗t , h(C , xt) = yˆt)|yˆt = ABSTAIN)

(70)

xt ∈D

α

≥1−d·

(71)

d

= 1 − α.

(72)

We have (70) from (69) based on the Boole’s inequality.

