2346

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 56, NO. 6, JUNE 2008

Bayesian Compressive Sensing
Shihao Ji, Member, IEEE, Ya Xue, and Lawrence Carin, Fellow, IEEE

Abstract—The data of interest are assumed to be represented as

B -dimensional real vectors, and these vectors are compressible in
some linear basis , implying that the signal can be reconstructed

B accurately using only a small number

of basis-function

coefﬁcients associated with . Compressive sensing is a frame-

work whereby one does not measure one of the aforementioned

-dimensional signals directly, but rather a set of related mea-

surements, with the new measurements a linear combination of

the original underlying -dimensional signal. The number of

required compressive-sensing measurements is typically much

smaller than , offering the potential to simplify the sensing

system. Let denote the unknown underlying -dimensional

signal, and a vector of compressive-sensing measurements, then

one may approximate accurately by utilizing knowledge of

B the (under-determined) linear relationship between and , in
addition to knowledge of the fact that is compressible in . In

this paper we employ a Bayesian formalism for estimating the

underlying signal based on compressive-sensing measurements

. The proposed framework has the following properties: i) in

addition to estimating the underlying signal , “error bars”

are also estimated, these giving a measure of conﬁdence in the

inverted signal; ii) using knowledge of the error bars, a principled

means is provided for determining when a sufﬁcient number of

compressive-sensing measurements have been performed; iii)

this setting lends itself naturally to a framework whereby the

compressive sensing measurements are optimized adaptively and

hence not determined randomly; and iv) the framework accounts

for additive noise in the compressive-sensing measurements and

provides an estimate of the noise variance. In this paper we present

the underlying theory, an associated algorithm, example results,

and provide comparisons to other compressive-sensing inversion

algorithms in the literature.

Index Terms—Adaptive compressive sensing, Bayesian model selection, compressive sensing (CS), experimental design, relevance vector machine (RVM), sparse Bayesian learning.

I. INTRODUCTION
OVER the last two decades there have been signiﬁcant advances in the development of orthonormal bases for compact representation of a wide class of discrete signals. An important example of this is the wavelet transform [1], [2], with
Manuscript received January 29, 2007; revised November 9, 2007. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Hakan Johansson. This research was supported by the Ofﬁce of Naval Research (ONR) and the Defense Advanced Research Project Agency (DARPA) under the Mathematical Time Reversal program.
S. Ji was with the Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708-0291 USA. He is now with Integrian, Inc., Durham, NC 27703 USA (e-mail: shji@ee.duke.edu).
Y. Xue was with the Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708-0291 USA. She is now with GE Global Research, Niskayuna, NY 12309 USA (e-mail:yx10@ee.duke.edu).
L. Carin is with the Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708-0291 USA (e-mail: lcarin@ee.duke.edu).
Color versions of one or more of the ﬁgures in this paper are available online at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TSP.2007.914345

which general signals are represented in terms of atomic ele-

ments localized in time and frequency, assuming that the data

index represents time (it may similarly represent space). The

localized properties of these orthonormal time-frequency atoms

yields highly compact representations of many natural signals

[1], [2]. Let the

matrix represent a wavelet basis, with

basis functions deﬁned by associated columns; a general signal

may be represented as

, where

repre-

sents the wavelet and scaling function coefﬁcients [1], [2]. For

most natural signals , most components of the vector have

negligible amplitude. Therefore, if represents the weights

with the smallest

coefﬁcients set to zero, and

,

then the relative error

is often negligibly small

for

. This property has led to the development of

state-of-the-art compression algorithms based on wavelet-based

transform coding [3], [4].

In conventional applications one ﬁrst measures the -dimen-

sional signal , is then compressed (often using a wavelet-

based transform coding scheme), and the compressed set of

basis-function coefﬁcients are stored in binary [3], [4]. This

invites the following question: If the underlying signal is ulti-

mately compressible, is it possible to perform a compact (“com-

pressive”) set of measurements directly, thereby offering the po-

tential to simplify the sensing system (reduce the number of

required measurements)? This question has recently been an-

swered in the afﬁrmative [5], [6], introducing the ﬁeld of com-

pressive sensing (CS).

In its earliest form the relationship between the underlying

signal and the CS measurements has been constituted

through random projections [6], [7]. Speciﬁcally, assume that

the signal is compressible in some basis (not necessarily

a wavelet basis), the th CS measurement ( th component

of ) is constituted by projecting onto a “random” basis

that is constituted with “random” linear combination of the

basis functions in , i.e.,

, where

is a column vector with each element an independent and

identically distributed (i.i.d.) draw of a random variable, with

arbitrary alphabet (e.g., real or binary) [6], [7].

Based on the above discussion, the CS measurements may be

represented as

, where

is

a

matrix, assuming random CS measurements are

made. Since typically

this amounts to having fewer

measurements than degrees of freedom for the signal . There-

fore, inversion for the -weights represented by (and hence

) is ill-posed. However, if one exploits the fact that is sparse

with respect to a known orthonormal basis , then one may ap-

proximate accurately [5], [6]. A typical means of solving such

an ill-posed problem, for which it is known that is sparse, is

via an -regularized formulation [6]

(1)

1053-587X/$25.00 © 2008 IEEE

JI et al.: BAYESIAN COMPRESSIVE SENSING

2347

where the scalar controls the relative importance applied to the Euclidian error and the sparseness term [the ﬁrst and second expressions, respectively, inside the brackets in (1)]. This basic framework has been the starting point for several recent CS inversion algorithms, including linear programming [8] and greedy algorithms [9], [10], for a point estimate of the weights .
In this paper we consider the inversion of compressive measurements from a Bayesian perspective. Speciﬁcally, from this standpoint we have a prior belief that should be sparse in the basis , data are observed from compressive measurements, and the objective is to provide a posterior belief (density function) for the values of the weights . Besides the improved accuracy over the point estimate (to be discussed in Section III-B), the Bayesian formalism, more importantly, provides a new framework that allows us to address a variety of issues that previously have not been addressed. Speciﬁcally, rather than providing a point (single) estimate for the weights
, a full posterior density function is provided, which yields “error bars” on the estimated ; these error bars may be used to give a sense of conﬁdence in the approximation to , and they may also be used to guide the optimal design of additional CS measurements, implemented with the goal of reducing the uncertainty in ; in addition, the Bayesian framework provides an estimate for the posterior density function of additive noise encountered when implementing the compressive measurements.
The remainder of the paper is organized as follows. In Section II, we consider the CS inversion problem from a Bayesian perspective, and make connections with what has been done previously for this problem. The analysis is then generalized in Section III, yielding a framework that lends itself to efﬁcient computation of an approximation to a posterior density function for . In Section IV, we examine how this framework allows adaptive CS, whereby the aforementioned projections are selected to optimize a (myopic) information measure. Example results on canonical data are presented in Section V, with comparisons to other algorithms currently in the literature. Conclusions and future work are discussed in Section VI.

II. COMPRESSIVE-SENSING INVERSION FROM BAYESIAN VIEWPOINT

A. Compressive Sensing as Linear Regression

It was assumed at the start that is compressible in the basis

. Therefore, let represent an -dimensional vector that is

identical to the vector for the elements in with largest

magnitude; the remaining

elements in are set to zero.

Similarly, we introduce a vector that is identical to for the

smallest

elements in , with all remaining elements of

set to zero. We therefore have

, and

(2)

with

. Since it was assumed at the start that is con-

stituted through random samples, the components of may be

approximated as a zero-mean Gaussian noise as a consequence

of Central Limit Theorem [11] for large

. We also note

that the CS measurements may be noisy, with the measurement noise, denoted by , represented by a zero-mean Gaussian distribution, and therefore
(3)
where the components of are approximated as a zero-mean Gaussian noise1 with unknown variance . We therefore have the Gaussian likelihood model
(4)
This above analysis has converted the CS problem of inverting for the sparse weights into a linear-regression problem with a constraint (prior) that is sparse. Assuming knowledge of , the quantities to be estimated based on the CS measurements are the sparse weights and the noise variance . In a Bayesian analysis we seek a full posterior density function for and .
B. Sparseness Prior and MAP Approximation
In a Bayesian formulation our understanding of the fact that is sparse is formalized by placing a sparseness-promoting
prior on . A widely used sparseness prior is the Laplace density function [12], [13]
(5)
where in (5) and henceforth we drop the subscript on , recognizing that we are always interested in a sparse solution for the weights. Given the CS measurements , and assuming the likelihood function in (4), it is straightforward to demonstrate that the solution in (1) corresponds to a maximum a posteriori (MAP) estimate for using the prior in (5) [13], [14].
III. ESTIMATE OF SPARSE WEIGHTS VIA RELEVANCE VECTOR MACHINE
A. Hierarchical Sparseness Prior
The above discussion connected conventional CS inversion for the weights to a MAP approximation to a Bayesian linearregression analysis, with a Laplace sparseness prior on . This then raises the question of whether the Bayesian analysis may be carried further, to realize an estimate of the full posterior on and . This is not readily accomplished using the Laplace prior directly, since the Laplace prior is not conjugate2 to the Gaussian likelihood and hence the associated Bayesian inference may not be performed in closed form [12], [15].
This issue has been addressed previously in sparse Bayesian learning, particularly, with the relevance vector machine (RVM) [16]. Rather than imposing a Laplace prior on , in the RVM a hierarchical prior has been invoked, which has similar properties
1In practice, not all of the assumptions made in deriving (3) will necessarily be valid, but henceforth we simply use (3) as a starting point, motivated for the reasons discussed above, and desirable from the standpoint of analysis.
2In Bayesian probability theory, a class of prior probability distributions p() is said to be conjugate to a class of likelihood functions p(xj) if the resulting posterior distributions p(jx) are in the same family as p().

2348

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 56, NO. 6, JUNE 2008

objective of achieving highly efﬁcient computations while still preserving accurate results.
As one may note, the Bayesian linear model considered in RVM is essentially one of the simpliﬁed models for Bayesian model selection [19]–[21]. Although more accurate models may be desired, the main motivation of adopting the RVM is due to its highly efﬁcient computation as discussed below.

Fig. 1. Graphical model of the Bayesian CS formulation.
as the Laplace prior but allows convenient conjugate-exponential analysis. To see this, one ﬁrst deﬁnes a zero-mean Gaussian prior on each element of

B. Bayesian CS Inversion via RVM Assuming the hyperparameters and are known, given
the CS measurements and the projection matrix , the posterior for can be expressed analytically as a multivariate Gaussian distribution with mean and covariance
(9) (10)

(6)
with the precision (inverse-variance) of a Gaussian density function. Further, a Gamma prior is considered over
(7)

where

. The associated “learning”

problem, in the context of the RVM, thus becomes the search

for the hyperparameters and . In the RVM, these hyper-

parameters are estimated from the data by performing a type-II

ML (or evidence maximization) procedure [16]. Speciﬁcally, by

marginalizing over the weights , the marginal likelihood for

and , or equivalently, its logarithm

can be expressed

analytically as

By marginalizing over the hyperparameters , the overall prior on is then evaluated as

(8)

The density function

is the conjugate prior

for , when plays the role of observed data and

is a likelihood function. Consequently, the

integral

can be evaluated ana-

lytically, and it corresponds to the Student- distribution [16].

With appropriate choice of and , the Student- distribution

is strongly peaked about

, and therefore the prior in (8)

favors most being zero (i.e., it is a sparseness prior). Simi-

larly, a Gamma prior

is introduced on the inverse of

the noise variance

.

To see the advantage of this hierarchical prior, consider the

graphical structure of the model as reﬂected in Fig. 1, for gen-

eration of the observed data . Following consecutive blocks in

Fig. 1 (following the direction of the arrows), let represent

the parameter associated with block , and represents the

next parameter in the sequence. For all steps in Fig. 1, the den-

sity function for is the conjugate prior for the likelihood de-

ﬁned in terms of the density function for , assuming that all

parameters except are held constant (i.e., all parameters other

than temporarily play the role of ﬁxed data). This structural

form is very convenient for implementing iterative algorithms

for evaluation of the posterior density function for and .

For example, one may conveniently implement a Markov chain

Monte Carlo (MCMC) [17] or, more efﬁciently and approxi-

mately, a variational Bayesian (VB) analysis [18]. While the VB

analysis is efﬁcient relative to MCMC, in the RVM a type-II

maximum-likelihood (ML) procedure is considered, with the

(11)

with

. A type-II ML approximation em-

ploys the point estimates for and to maximize (11), which

can be implemented via the EM algorithm (or other techniques)

[16], to yield

(12)

where is the th posterior mean weight from (9) and we have

deﬁned the quantities

, with the th diagonal

element of the posterior weight covariance from (10). For the

noise variance

, differentiation leads to the re-esti-

mate

(13)

Note that

and

are a function of and , while

and are a function of and . This suggests an iterative al-

gorithm, which iterates between (9) and (10) and (12) and (13),

until a convergence criterion has been satisﬁed. In this process,

it is observed that many of the tend to inﬁnity (or are nu-

merically indistinguishable from inﬁnity given the machine pre-

cision) for those that have insigniﬁcant amplitudes for rep-

resentation of

; only a relatively small set of , for

which the corresponding remains relatively small, contribute

for representation of , and the level of sparseness (size of )

is determined automatically (see [22] for an interesting expla-

nation from a variational approximation perspective). It is also

important to note that, as a result of the type-II ML estimate

JI et al.: BAYESIAN COMPRESSIVE SENSING

2349

(11), the point estimates (rather than the posterior densities) of

and are sought. Therefore, there is no need to set , , ,

and on the Gamma hyperpriors. This is equivalent to setting

, , , and to zero, and thus uniform hyperpriors (over a log-

arithmic scale) on and have been invoked [16].

While it is useful to have a measure of uncertainty in the

weights , the quantity of most interest is the signal

.

Since is drawn from a multivariate Gaussian distribution with

mean and covariance deﬁned in (9) and (10), the posterior den-

sity function on is also a multivariate Gaussian distribution

with mean and covariance

(14) (15)

the explanations for the improvement in sparsity demonstrated in the experiments (see Section V).
In addition, recent theoretical analysis of the RVM [28], [29] indicates that the RVM provides a tighter approximation to the
-norm sparsity measure than the -norm, and prove that even in the worst-case scenario, the RVM still outperforms the most widely used sparse representation algorithms, including BP [8] and OMP [9]. Although these studies are based on the iterative (EM) implementation of the RVM, they indeed shed light on the fast implementation considered here, since both implementations are based on the same cost function (11). Our empirical study in Section V is also consistent with these theoretical results. Nonetheless, rigorous analysis of this fast algorithm remains worthy of further inquiry.

The diagonal elements of the covariance matrix in (15) provide

“error bars”3 on the accuracy of the inversion of , as repre-

sented in terms of its mean in (14).

While the iterative algorithm described above has been

demonstrated to yield a highly accurate sparse linear-regression

representation [16], we note the following practical limitation.

When evaluating (10) one must invert matrices of size

:

an

operation,4 thereby making this approach relatively

slow for data of large dimension (at least for the ﬁrst

few iterations). This motivates development of a fast RVM

algorithm with the objective of achieving highly efﬁcient com-

putations that are comparable to existing CS algorithms (e.g.,

OMP [9] and StOMP [10]).

Fortunately, this fast RVM algorithm has been developed in

[26] and [27] by analyzing the properties of the marginal like-

lihood function in (11). This enables a principled and efﬁcient

sequential addition and deletion of candidate basis function

(columns of ) to monotonically maximize the marginal likeli-

hood. We omit the detailed discussion of this fast algorithm and

refer the reader to [26] and [27] for more details. We here only

brieﬂy summarize some of its key properties. Compared to the

iterative algorithm presented above, the fast algorithm operates

in a constructive manner, i.e., sequentially adds (or deletes)

candidate basis function to the model until all “relevant” basis

functions (for which the associated weights are nonzero) have

been included. Thus, the complexity of the algorithm is more

related to than . Further, by exploiting the matrix inverse

identity, the inverse operation in (10) has been implemented

by an iterative update formula with reduced complexity. De-

tailed analysis of this algorithm shows that it has complexity

, which is more efﬁcient than the original RVM, espe-

cially when the underlying signal is truly sparse (

).

In contrast to other CS algorithms (e.g., OMP [9] and StOMP

[10], in which basis functions once added are never removed),

the fast RVM algorithm has the operation of deleting a basis

function from the model (i.e., setting the corresponding

). This deletion operation allows the fast algorithm to main-

tain a more concise signal representation and is likely one of

3While previous works [23], [24] in CS do obtain ` error bounds for function
estimates, the “error bars” may be more useful from a practical standpoint as discussed in the next section.
O( ) 4A simple modiﬁcation to (10) is available from [25] by exploiting the matrix
inverse identity, which leads to an K operation per iteration. Nonetheless,
the iterative (EM) implementation still does not scale well.

IV. ADAPTIVE COMPRESSIVE SENSING

A. Selecting Projections to Reduce Signal Uncertainty

In the original CS construction [6], [7], the projections rep-

resented by the matrix were constituted via i.i.d. realizations

of an underlying random variable. In addition, previous CS al-

gorithms [8]–[10] focused on estimating (and hence ) have

employed a point estimate like that in (1); such approaches do

not provide a measure of uncertainty in , and therefore adap-

tive design of was previously not feasible. The Bayesian CS

(BCS) algorithm (in this case the fast RVM algorithm) discussed

in Section III-B allows efﬁcient computation of and associ-

ated error bars, as deﬁned by (14) and (15), and therefore one

may consider the possibility of adaptively selecting projection

, with the goal of reducing uncertainty. Such a framework

has been previously studied in the machine learning commu-

nity under the name of experimental design or active learning

[30]–[32]. Further, the error bars also give a way to determine

how many measurements are enough for faithful CS reconstruc-

tion, i.e., when the change in the uncertainty is not signiﬁcant,

it may be assumed that one is simply reconstructing the noise

in (3), and therefore the adaptive sensing may be stopped.

As discussed above, the estimated posterior on the signal

is a multivariate Gaussian distribution, with mean

and covariance

. The differential entropy [33]

for therefore satisﬁes

(16)

where

is independent of the projection matrix . Recall

that

, and therefore the dependence

of the differential entropy on the observed CS measurements

is deﬁned by the point estimates of and (from the type-II

ML estimates discussed in Section III).5

5In practice, many of the  have the value of inﬁnity (or exceed the machine
8 precision), indicating the corresponding basis functions in are excluded for
A 8 sparse representation. Therefore, when evaluating (16), both and only em-
ploy elements corresponding to the basis functions selected by BCS, and they are thus reduced in general to small matrices.

2350

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 56, NO. 6, JUNE 2008

We may now ask which new projection

would be op-

timal for minimizing the differential entropy in (16). Toward this

end, we augment by adding a

th row represented by

. If we let

represent the new differential entropy

as a consequence of adding this new projection vector, via the

matrix determinant identity, we have

(17)

where and are based on estimates found using the previous

measurements. To minimize , the next projection

should hence be designed to maximize

. Since

(18)

this is equivalent to maximizing the variance of the expected

measurement . In other words, the next projection

should be selected to constitute the measurement

for

which the data is most uncertain, and hence access to the

associated measurement would be most informative.

The adaptive framework provides an attractive setting for

selection of the next projection

, with the goal of op-

timizing—in a one-look-ahead (myopic) sense—the rate at

which the uncertainty in diminishes [30]–[32]. There are

multiple ways this may be utilized in practice. If it is possible

to design new projection

adaptively “on the ﬂy,” then

one might perform an eigendecomposition of the matrix and

select for representation of

the eigenvector with largest

eigenvalue. Alternatively, if from a hardware standpoint such

ﬂexibility in design of

is not feasible, then one might

a priori design a library of possible next projections, with

selected from with the goal of maximizing (18). In

the example results in Section V, we select the next projection

as the eigenvector of that has the largest eigenvalue, but

design of an a priori library may be more useful in practice,

and this remains an important direction for future research.

We also note the following practical issue for implementa-

tion of adaptive CS. Assume that an initial set of CS measure-

ments are performed with a ﬁxed set of projections, for which

data are measured. Based upon and knowledge of the initial

projections, there is a deterministic mapping to the next opti-

mized projection, with which the next CS measurement is per-

formed. Consequently, although the optimized projections are

performed on the sensor, when performing signal reconstruc-

tion subsequently, the optimized projections that are performed

at the sensor may be inferred ofﬂine, and therefore there is no

need to send this information to the decoder. Consequently, the

performance of optimized projections introduces no new over-

head for storage of the compressive measurements (i.e., we do

not have to store the adaptively determined projections).

An additional issue needs to be clariﬁed if the eigenvector

of is used for the next projection . Due to the sparse

Bayesian solution, only employs elements corresponding to

the associated nonzero components of found based on BCS

(i.e., is reduced in general to a small matrix). Thus, when

constructing the next projection based on the eigenvector, some

entries of

will be empty. If we impute all the empty en-

tries with zero, we are under the risk of being wrong. The ini-

tial estimate of can be inaccurate; if we impute all the empty

entries with zero, the estimate of may be always biased and

has no chance to be corrected, since the corresponding contri-

butions from underlying true are always ignored. To mitigate

this problem, we impute the empty entries with random samples

drawn i.i.d. from a Gaussian distribution

. After the im-

putation, we rescale the -norm of the imputed values to 0.14.

By doing so, we utilize the optimized projection and meanwhile

allow some contributions from the empty entries. Overall, the

ﬁnal projection

has the magnitude

.

B. Approximate Adaptive CS

The “error bars” on the estimated signal play a critical role

in implementing the above adaptive CS scheme, with these a di-

rect product from the Bayesian analysis. Since there are estab-

lished CS algorithms based on a point estimate of , one may

ask whether these algorithms may be modiﬁed, utilizing insights

from the Bayesian analysis. The advantage of such an approach

is that, if possible, one would access some of the advantages

of the Bayesian analysis, in an approximate sense, while being

able to retain the advantages of established fast CS algorithms.

In this section, we consider one possible approximate scheme

for adaptive CS, and show that the adaptive CS and its approxi-

mate scheme may be only amenable to the Bayesian analysis.

The uncertainty in and the adaptive algorithm in (18) rely

on computation of the covariance matrix

.

Since is assumed known (but which basis functions have

been selected by BCS are unknown), this indicates that what

is needed are estimates for and , the latter required for the

diagonal matrix . Concerning the diagonal matrix , it may

be viewed from a signal processing standpoint as a regulariza-

tion of the matrix

to assure that the matrix inversion is

well-posed. While the Bayesian analysis in Section III indicates

that the loading represented by should be nonuniform, we

may simply make diagonalized uniformly, with value corre-

sponding to a small fraction of the average value of the diagonal

elements of

, i.e.,

(19)

where is a small positive value (e.g.,

), and is

the number of basis functions selected by BCS based on the

current CS measurements. Since we are only interested in the

eigenvectors of , in (19) can be ignored for the computation

of eigen-decomposition. Therefore, for an approximate adaptive

CS, what is needed is only the basis functions selected by BCS,

with which constitute the projection matrix in (19).

In the derivation of (19), we assume that the diagonal ele-

ments of are relatively unform, such that can be approx-

imated by a uniform diagonal matrix. While this assumption is

typically valid for BCS, there is no guarantee for other CS algo-

rithms, since other CS algorithms may select basis functions that

are distinct from those selected by BCS. In Section V, when pre-

senting example results, we make comparisons between the rig-

orous implementation of adaptive CS presented in Section IV-A

and the approximate scheme discussed here, as applied to BCS

JI et al.: BAYESIAN COMPRESSIVE SENSING

2351

Fig. 2. Reconstruction of uniform Spikes for N

k 0 k k k (a) Original signal; (b) Reconstruction with BP,

k 0 k k k = t

1.66 s; (c) Reconstruction with BCS, f

= t

0.46 s.

=
f

512, M = 20,
f =f
f =f

=K =

= 100. 00::10514862,,

and OMP.6 As demonstrated, both the rigorous implementation and the approximate scheme succeed in BCS, while the approximate scheme fails in OMP. Intuitively, this is because the basis functions selected by OMP are different from those selected by BCS. Compared to BCS, some OMP-selected basis functions should be removed. Therefore, from the Bayesian analysis standpoint, the corresponding should be inﬁnity, and thus the matrix cannot be approximated by a uniform diagonal matrix. These comparisons suggest that the adaptive CS developed in Section IV-A may be only amenable to the Bayesian analysis, while it may not be feasible for other CS algorithms, indicating the adaptive CS may be one of the unique advantages of BCS over other CS algorithms.

V. EXAMPLE RESULTS
We test the performances of BCS and adaptive CS on several example problems considered widely in the CS literature, with comparisons (when appropriate) made to BP [8], OMP [9] and StOMP [10]. While BP is a relatively computationally expensive algorithm that involves linear programming, OMP is a fast greedy strategy that iteratively selects basis functions most aligned with the current residual, and StOMP is an extension of OMP and may be one of the state-of-the-art fast CS algorithms. In the experiments, all the computations were performed on a 3.4 GHz Pentium machine. The Matlab code is available online at http://www.ece.duke.edu/~shji/BCS.html.

A. 1D Signals

In the ﬁrst example, we consider a length

signal

that contains

spikes created by choosing 20 locations

at random and then putting at these points [Fig. 2(a)]. The

projection matrix is constructed by ﬁrst creating a

matrix with i.i.d. draws of a Gaussian distribution

, and

6OMP outputs both the weights and the indexes of the selected basis func-
8 tions. With these selected basis functions (which form ), we can compute an 6^ approximate covariance matrix (19), from which we then compute the eigen-
vector.

then the rows of are normalized to unit magnitude. To sim-

ulate measurement noise, zero-mean Gaussian noise with stan-

dard deviation

is added to each of the measure-

ments that deﬁne the data . In the experiment

, and the

reconstructions are implemented by BP and BCS. For the BP

implementation, we used the -magic package available online

at http://www.acm.caltech.edu/l1magic/, and the BP parameters

were set as those suggested by -magic.

Fig. 2(b) and (c) demonstrates the reconstruction results with

BP and BCS, respectively. Due to noisy measurements, BP

cannot recover the underlying sparse signal exactly, nor can

BCS. However, the BCS reconstruction is much cleaner than

BP, as

spikes are correctly recovered with (about 10

times) smaller reconstruction error relative to BP. In addition,

BCS yields “error bars” for the estimated signal, indicating the

conﬁdence for the current estimation. Regarding the computa-

tion time, BCS also outperforms BP.

As discussed in Section IV, the Bayesian analysis also allows

designing projection matrix for adaptive CS. In the second

experiment, we use the same dataset as in Fig. 2 and study the

performance of BCS for projection design. The initial 40 mea-

surements are conducted by using the random projections as in

Fig. 2, except that the rows of are normalized to 1.01 for the

reasons discussed in Section IV-A. The remaining 80 measure-

ments are sequentially conducted by optimized projections, with

this compared to using random projections. In the experiment,

after each projection vector is determined, the associated

reconstruction error is also computed. For the optimized pro-

jection, is constructed by using the eigenvector of that

has the largest eigenvalue. When examining the approximate

scheme discussed in (19), we set

for diagonal loading.

Because of the randomness in the experiment (i.e., the genera-

tion of the original spike signal, the initial 40 random projections

and the empty-entries imputation for , etc.), we execute the

experiment 100 times with the average performance and vari-

ance reported in Fig. 3(a) and (b), respectively.

It is demonstrated in Fig. 3(a) and (b) that the reconstruction

error of the optimized projection is much smaller than that of the

random projection, indicating the superior performance of this

optimization. Further, the approximate scheme in Section IV-B

yields results very comparable to the rigorous implementation

in Section IV-A, suggesting that this approximate scheme may

be well-suited for BCS.

However, to make a meaningful conclusion, we still have two

questions to address. First, the spike signal that we have consid-

ered above is exactly the case for which the nonzero entries of

have the same magnitude, and thus seems well-suited to the

uniform loading assumption. Second, besides BCS, one may ask

whether other CS algorithms may be modiﬁed to implement this

approximate scheme, with the same success as BCS.

To answer the ﬁrst question, we execute the same experiment

as above but on a nonuniform spike signal as shown in Fig. 4(a).

To make the comparison meaningful, the signal-to-noise-ratio

(SNR) of the both types of spike signals are ﬁxed the same. The

results on the nonuniform spike signal are shown in Figs. 4 and

3(c) and (d), from which similar conclusions as for the uniform

case can be made, indicating that the uniform loading assump-

tion is generally applicable for BCS.

2352

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 56, NO. 6, JUNE 2008

Fig. 3. Comparison of adaptive CS and conventional CS on uniform spikes and nonuniform spikes by using BCS; the results are averaged over 100 runs. (a), (c) Reconstruction error of BCS with random projections, optimized projections (Section IV-A) and approximate projections (Section IV-B); (b), (d) the variances of the reconstruction error of BCS with random projections and optimized projections (Section IV-A); the variance for the approximate projections (Section IV-B) is very similar to that of optimized projections, and thus is omitted to improve visibility. (a) and (b) are the results on uniform spikes (as in Fig. 2); (c) and (d) are the results on nonuniform spikes (as in Fig. 4). Note that the error bars (one standard deviations) in (c) and (d) only show how tight the errors are around their mean values, and do not indicate the errors can be negative.

It is worthwhile to point out some notable observations from Fig. 3 regarding the performance of adaptive CS as compared to conventional CS. Speciﬁcally, Theorem 2 of [6] suggests that adaptive design of projections is of minimal help over (nonadaptive) random projections. Derived from information-based complexity, Theorem 2 of [6] shows that

(20)

where

denotes the minimax reconstruction error of

the method (adaptive or nonadaptive) adopted, and

describes the compressibility of the underlying signal. In the

most common case

, this inequality (20) elucidates that

by using optimized projection at most 50% reduction in error

can be attained as compared to random projection. Not surpris-

ingly, our results in Fig. 3 is consistent with this conclusion.

We note that 50% reduction in error may be not remarkable in

theory (or from a mathematical standpoint), but from a practical engineering standpoint 50% error reduction is often signiﬁcant.
As a secondary point, we also observe the following notable differences between the performances of BCS as applied to the uniform spikes and the non-uniform spikes. Comparing Fig. 3(a)–(c), for a given number of CS measurements, the reconstruction error on the nonuniform spikes is (much) smaller than that on the uniform spikes. Evidently, this observation is consistent with some of the theoretical analysis from [29], i.e., uniform weights offer the worst-case scenario for sparse signal reconstruction, and “the more diverse the weights magnitudes, the better the chances we have of learning the optimal solution.”
To address the second question above, we test the approximate adaptive CS scheme as applied to OMP, with the results on the uniform spikes and the non-uniform spikes shown in Fig. 5. It is demonstrated in Fig. 5 that in both cases the approximate scheme with OMP are unsuccessful. Intuitively, this

JI et al.: BAYESIAN COMPRESSIVE SENSING

2353

Fig. 4. Reconstruction of nonuniform Spikes for N = 512, M = 20, K =

k 0 k k k 100. (a) Original signal; (b) reconstruction with BP, f

f =f

=

k 0 k k k 0:1375, t = 1.89 s; (c) reconstruction with BCS, f

= f = f

0:0178, t = 0.27 s.

is because the basis functions selected by OMP are different from those selected by BCS. Compared to BCS, some OMP-selected basis functions should be removed. Therefore, from the Bayesian analysis standpoint, the corresponding should be inﬁnity, and thus the matrix cannot be approximated by a uniform diagonal matrix.
In summary, the empirical study presented above suggests that i) the adaptive CS developed in Section IV-A outperforms conventional CS, and this improvement is more remarkable for signals with uniform weights and ii) the adaptive CS may be only amenable to the Bayesian analysis, while it may not be feasible for other CS algorithms (e.g., OMP), indicating a unique advantage of BCS over other CS algorithms. For these reasons, in the experiments that follow, when the adaptive CS is applied, we only consider the rigorous implementation presented in Section IV-A, with this a direct beneﬁt from BCS.

B. 2D Images

In the following set of experiments, the performance of BCS

is compared to BPDN (the noise-aware version of BP) and

StOMP (equipped with CFDR and CFAR thresholding) on two

example problems included in the Sparselab package that is

available online at http://sparselab.stanford.edu/. Following the

experiment setting in the package, all the projection matrix

here are drawn from a uniform spherical distribution [7]. For

completeness, we also test the performances of adaptive CS on

these two example images as compared to conventional CS.

1) Random-Bars: Fig. 6 shows the reconstruction results

for Random-Bars that has been used in [7]. We used the Haar

wavelet expansion, which is naturally suited to images of this

type, with a coarsest scale

, and a ﬁnest scale

.

Fig. 6(a) shows the result of linear reconstruction (i.e., the in-

verse wavelet transform) with

4096 samples, which rep-

resents the best performance that could be achieved by all the

CS implementations used, whereas Fig. 6(b)–(d) has results for

the hybrid CS scheme (i.e., the CS measurements are made only

on the ﬁne-scale coefﬁcients; no compression on the coarsest-

scale coefﬁcients) [7] with

hybrid compressed sam-

ples. It is demonstrated that BCS and StOMP with CFAR yield

the near optimal reconstruction error (0.2271); among all the

CS algorithms considered StOMP is the fastest one. However,

as we have noted, the performance of StOMP strongly relies

on the thresholding parameters selected. For the Random-Bars

problem considered, the performance of StOMP with CFDR is

very sensitive to its parameter setting, with one typical example

result shown in Fig. 6(b).

2) Mondrian: Fig. 7 displays a photograph of a painting by

Piet Mondrian, the Dutch neo-plasticist. Despite being a simple

geometric example, this image still presents a challenge, as its

wavelet expansion is not as sparse as the examples considered

above. We used a multiscale CS scheme [7] for image recon-

struction, with a coarsest scale

, and a ﬁnest scale

on the “symmlet8” wavelet. Fig. 7(a) shows the result of linear

reconstruction with

4096 samples, which represents the

best performance that could be achieved by all the CS imple-

mentations used, whereas Fig. 7(b)–(d) has results for the mul-

tiscale CS scheme with 2713 multiscale compressed sam-

ples. In the example results in Fig. 7(b) and (c), we used the same

parameter-setting for StOMP as those used in the SparseLab

package. It is demonstrated that all the CS implementations

yielded a faithful reconstruction to the original image, while

BCS produced the second smallest reconstruction error (0.1498)

using the second smallest computation time (15 s).

To understand why BCS is more efﬁcient than StOMP on this

problem, we checked the number of nonzero weights recovered

by BCS and StOMP, with the results reported in Table I. Ev-

idently, BCS found the sparsest solution (with 751 nonzeros)

relative to the two StOMP implementations, but yielded the

second smallest reconstruction error (0.1498). This indicates

that although each iteration of StOMP allows multiple nonzero

weights to be added into the “active set” [10], this process may

be a too generous usage of weights without reducing the recon-

struction error. The sparser solution of BCS is the likely expla-

nation of its relative higher speed compared to StOMP in this

example.

Finally, the performances of adaptive CS as compared to con-

ventional CS are provided in Fig. 8(a) and (b), for Random-Bars

and Mondrian, respectively. The adaptive CS consistently out-

performs conventional CS in all the cases considered.

VI. CONCLUSION
Compressive sensing has been considered from a Bayesian perspective. It has been demonstrated that by utilizing the previously studied relevance vector machine (RVM) from the sparse Bayesian learning literature, problems in CS can be solved more effectively. In practice, we have found that the results from this Bayesian analysis are often sparser than existing CS solutions [8], [10]. On the examples considered from the literature, BCS typically has computation time comparable to the state-of-the-art algorithms such as StOMP [10]; in some cases, BCS is even faster as a consequence of the improved sparsity. We have also considered adaptive CS by optimizing the projection matrix . Empirical studies demonstrate a signiﬁcantly accelerated rate of convergence compared to the original CS construction. Finally, we have also demonstrated that the adaptive

2354

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 56, NO. 6, JUNE 2008

Fig. 5. Comparison of the approximate adaptive CS (Approx.) and conventional CS (Random) by using OMP on (a) uniform spikes (as in Fig. 2) and (b) nonuniform spikes (as in Fig. 4). The results are averaged over 100 runs.

Fig. 6. Reconstruction of Random-Bars with hybrid CS. (a) Linear reconstruc-

k 0 k k k tion from K = 4096 samples, f f = f = 0:2271; (b) reconstruction

k 0 k k k with CFDR, f

f = f = 0:5619, t

= 3.15 s; (c) reconstruc-

k 0 k k k tion with CFAR, f

f = f = 0:2271, t

= 4.38 s; (d) recon-

k 0 k k k struction with BCS, f

f = f = 0:2271, t = 8.55 s. BP (` )

took 114 s with the reconstruction error 0.2279, which is not shown here.

Fig. 7. Reconstruction of Mondrian with multiscale CS. (a) Linear reconstruc-

k 0 k k k tion from K = 4096 samples, f f = f = 0:1333; (b) reconstruction

k 0 k k k with CFDR, f

f = f = 0:1826, t

= 10 s; (c) reconstruc-

k 0 k k k tion with CFAR, f

f = f = 0:1508, t

= 28 s; (d) recon-

k 0 k k k struction with BCS, f

f = f = 0:1498, t = 15 s. BP (` )

took 162 s with the reconstruction error 0.1416, which is not shown here.

TABLE I SUMMARY OF THE PERFORMANCES OF BP, STOMP AND BCS ON MONDRIAN

CS may be only amenable to the Bayesian analysis, while it may not be feasible for other CS algorithms, indicating a unique advantage of BCS over other CS algorithms.
There is a clear connection between CS and regression shrinkage and selection via the Lasso [14], [34] as both focus on solving the same objective function (1). Research on Lasso has produced algorithms that might have some relevance to BCS. Besides this, other possible areas of future research may include i) even faster sparse Bayesian learning algorithms, as dealing with images is a high-dimensional problem, ii) simultaneous inversion of multiple data sets, borrowing ideas

from multitask learning [35], and iii) a theoretical analysis of adaptive CS, as this could be an important complement to the existing analysis for the conventional CS formulation (e.g., [23] and [24]).

JI et al.: BAYESIAN COMPRESSIVE SENSING

2355

Fig. 8. Comparison of adaptive CS (optimized) and conventional CS (random) on (a) Random-Bars and (b) Mondrian. The results are averaged over 100 runs.

ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers for their constructive suggestions. The authors also thank E. Candès, J. Romberg, and D. Donoho et al. for sharing the
-Magic and SparseLab online. Their generous distribution of the code made the experimental comparisons in this paper very convenient.
REFERENCES
[1] S. Mallat, A Wavelet Tour of Signal Processing, 2nd ed. New York: Academic Press, 1998.
[2] I. Daubechies, Ten Lectures on Wavelets. Philadelphia, PA: SIAM, 1992.
[3] A. Said and W. A. Pearlman, “A new fast and efﬁcient image codec based on set partitioning in hierarchical trees,” IEEE Trans. Circuits Syst. Video Technol., vol. 6, no. 3, pp. 243–250, Jun. 1996.
[4] W. A. Pearlman, A. Islam, N. Nagaraj, and A. Said, “Efﬁcient, low-complexity image coding with a set-partitioning embedded block coder,” IEEE Trans. Circuits Syst. Video Technol., vol. 14, no. 11, pp. 1219–1235, Nov. 2004.
[5] E. Candès, J. Romberg, and T. Tao, “Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information,” IEEE Trans. Inf. Theory, vol. 52, no. 2, pp. 489–509, Feb. 2006.
[6] D. L. Donoho, “Compressed sensing,” IEEE Trans. Inf. Theory, vol. 52, no. 4, pp. 1289–1306, Apr. 2006.
[7] Y. Tsaig and D. L. Donoho, “Extensions of compressed sensing,” Signal Process., vol. 86, no. 3, pp. 549–571, Mar. 2006.
[8] S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic decomposition by basis pursuit,” SIAM J. Sci. Comput., vol. 20, no. 1, pp. 33–61, 1999.
[9] J. A. Tropp and A. C. Gilbert, “Signal recovery from partial information via orthogonal matching pursuit,” IEEE Trans. Inf. Theory, vol. 53, no. 12, pp. 4655–4666, Dec. 2007.
[10] D. L. Donoho, Y. Tsaig, I. Drori, and J.-C. Starck, “Sparse solution of underdetermined linear equations by stagewise orthogonal matching pursuit,” Mar. 2006 [Online]. Available: http://stat.stanford.edu/~idrori/StOMP.pdf, Preprint.
[11] A. Papoulis and S. U. Pillai, Probability, Random Variables and Stochastic Processes, 4th ed. New York: McGraw-Hill, 2002.
[12] J. M. Bernardo and A. F. M. Smith, Bayesian Theory. New York: Wiley, 1994.
[13] M. Figueiredo, “Adaptive sparseness using Jeffreys prior,” in Advances in Neural Information Processing Systems (NIPS 14), 2002 [Online]. Available: http://books.nips.cc/papers/ﬁles/nips14/AA07.pdf

[14] R. Tibshirani, “Regression shrinkage and selection via the lasso,” J. Roy. Stat. Soc. B., vol. 58, no. 1, pp. 267–288, 1996.
[15] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin, Bayesian Data Analysis, 2nd ed. Boca Raton, FL: CRC Press, 2003.
[16] M. E. Tipping, “Sparse Bayesian learning and the relevance vector machine,” J. Mach. Learn. Res., vol. 1, pp. 211–244, 2001.
[17] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter, Markov Chain Monte Carlo in Practice. London, U.K.: Chapman & Hall, 1996.
[18] C. M. Bishop and M. E. Tipping, “Variational relevance vector machines,” in Proc. 16th Conf. Uncertainty in Artiﬁcial Intelligence, 2000, pp. 46–53.
[19] E. I. George and R. E. McCulloch, “Approaches for Bayesian variable selection,” Statistica Sinica, vol. 7, pp. 339–373, 1997.
[20] E. I. George and D. P. Foster, “Calibration and empirical Bayes variable selection,” Biometrika, vol. 87, no. 4, pp. 731–747, 2000.
[21] C. P. Robert, The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation, 2nd ed. New York: Springer, 2001.
[22] D. Wipf, J. Palmer, and B. Rao, “Perspectives on sparse Bayesian learning,” in Advances in Neural Information Processing Systems (NIPS 16), 2004 [Online]. Available: http://books.nips.cc/papers/ﬁles/nips16/NIPS2003_AA32.pdf
[23] E. J. Candès and T. Tao, “The Dantzig selector: Statistical estimation when p is much larger than n,” Ann. Stat., to be published.
[24] J. Haupt and R. Nowak, “Signal reconstruction from noisy random projections,” IEEE Trans. Inf. Theory, vol. 52, no. 9, pp. 4036–4048, Sep. 2006.
[25] D. P. Wipf and B. D. Rao, “Sparse Bayesian learning for basis selection,” IEEE Trans. Signal Process., vol. 52, no. 8, pp. 2153–2164, Aug. 2004.
[26] A. C. Faul and M. E. Tipping, “Analysis of sparse Bayesian learning,” in Advances in Neural Information Processing Systems (NIPS 14), T. G. Dietterich, S. Becker, and Z. Ghahramani, Eds., 2002, pp. 383–389 [Online]. Available: http://citeseer.ist.psu.edu/faul01analysis.html
[27] M. E. Tipping and A. C. Faul, “Fast marginal likelihood maximization for sparse Bayesian models,” in Proc. 9th Int. Workshop Artiﬁcial Intelligence and Statistics, C. M. Bishop and B. J. Frey, Eds., 2003 [Online]. Available: . http://citeseer.ist.psu.edu/611465.html
[28] D. P. Wipf and B. D. Rao, “` -norm minimization for basis selection,” in Advances in Neural Information Processing Systems (NIPS 17), 2005 [Online]. Available: http://books.nips.cc/papers/ﬁles/nips17/ NIPS2004_0819.pdf
[29] D. P. Wipf and B. D. Rao, “Comparing the effects of different weight distributions on ﬁnding sparse representations,” in Advances in Neural Information Processing Systems (NIPS 18), 2006 [Online]. Available: http://books.nips.cc/papers/ﬁles/nips18/NIPS2005_0158.pdf
[30] V. V. Fedorov, Theory of Optimal Experiments. New York: Academic, 1972.

2356

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 56, NO. 6, JUNE 2008

[31] D. MacKay, “Information-based objective functions for active data selection,” Neural Comput., vol. 4, no. 4, pp. 590–604, 1992.
[32] S. Ji, B. Krishnapuram, and L. Carin, “Variational Bayes for continuous hidden Markov models and its application to active learning,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, no. 4, pp. 522–532, Apr. 2006.
[33] T. M. Cover and J. A. Thomas, Elements of Information Theory. New York, NY: Wiley, 1991.
[34] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani, “Least angle regression,” Annals Stat., vol. 32, no. 2, pp. 407–499, 2004.
[35] R. Caruana, “Multitask learning,” Mach. Learn., vol. 28, no. 1, pp. 41–75, 1997.

Ya Xue received the B.S. degree in electrical engineering from Tsinghua University, Beijing, China, in 2000, the M.S. degree in electrical engineering from Arizona State University, Tempe, in 2002, and the Ph.D. degree from Duke University, Durham, NC, in electrical and computer engineering in 2006.
She currently works for GE Global Research, Niskayuna, New York. Her interests are machine learning and nonparametric statistical techniques.

Shihao Ji (M’06) received the B.S. and M.S. degrees from Xidian University, Xi’an, China, in 1998 and 2001, respectively, and the Ph.D. degree from Duke University, Durham, NC, in 2006, all in electrical engineering.
He is now with Integrian, Inc., Durham, NC. His research interests include sequential data processing with hidden Markov models, Bayesian inference, planning under uncertainty, and statistical signal processing.

Lawrence Carin (SM’96–F’01) was born in Washington, DC on March 25, 1963. He received the B.S., M.S., and Ph.D. degrees in electrical engineering at the University of Maryland, College Park, in 1985, 1986, and 1989, respectively.
He joined the Electrical Engineering Department at Polytechnic University, Brooklyn, NY, as an Assistant Professor in 1989 and became an Associate Professor there in 1994. In September 1995, he joined the Electrical and Computer Engineering Department at Duke University, Durham, NC, where he is now the William H. Younger Distinguished Professor. His current research interests include signal processing and machine learning for sensing applications.
Dr. Carin has been the principal investigator on several large research programs, including two Multidisciplinary University Research Initiative (MURI) programs. He is the co-founder of the small business Signal Innovations Group (SIG), which was purchased in 2006 by Integrian, Inc. He was an Associate Editor of the IEEE TRANSACTIONS ON ANTENNAS AND PROPAGATION from 1996 to 2001. He is a member of the Tau Beta Pi and Eta Kappa Nu honor societies.

