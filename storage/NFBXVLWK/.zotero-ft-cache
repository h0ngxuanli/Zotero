Certified Robustness of Graph Neural Networks against Adversarial Structural Perturbation

arXiv:2008.10715v3 [cs.CR] 16 Jul 2021

Binghui Wang, Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong
ECE Department, Duke University, Durham, NC, USA, 27708 {binghui.wang,jinyuan.jia,xiaoyu.cao,neil.gong}@duke.edu

ABSTRACT
Graph neural networks (GNNs) have recently gained much attention for node and graph classification tasks on graph-structured data. However, multiple recent works showed that an attacker can easily make GNNs predict incorrectly via perturbing the graph structure, i.e., adding or deleting edges in the graph. We aim to defend against such attacks via developing certifiably robust GNNs. Specifically, we prove the certified robustness guarantee of any GNN for both node and graph classifications against structural perturbation. Moreover, we show that our certified robustness guarantee is tight. Our results are based on a recently proposed technique called randomized smoothing, which we extend to graph data. We also empirically evaluate our method for both node and graph classifications on multiple GNNs and multiple benchmark datasets. For instance, on the Cora dataset, Graph Convolutional Network with our randomized smoothing can achieve a certified accuracy of 0.49 when the attacker can arbitrarily add/delete at most 15 edges in the graph.
CCS CONCEPTS
• Security and privacy; • Computing methodologies → Machine learning;
KEYWORDS
Adversarial machine learning, graph neural network, certified robustness, structural perturbation
ACM Reference Format: Binghui Wang, Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong. 2021. Certified Robustness of Graph Neural Networks against Adversarial Structural Perturbation. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’21), August 14–18, 2021, Virtual Event, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3447548.3467295
1 INTRODUCTION
Graphs are a powerful tool to represent data from diverse domains such as social networks, biology, finance, security, etc.. Node classification and graph classification are two basic tasks on graphs. Specifically, given a graph, node classification aims to classify nodes in
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD ’21, August 14–18, 2021, Virtual Event, Singapore © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-8332-5/21/08. . . $15.00 https://doi.org/10.1145/3447548.3467295

the graph in a semi-supervised fashion, while graph classification aims to assign a label to the entire graph instead of individual nodes. Node and graph classifications have many applications, including but not limited to, profiling users in social networks [19, 23, 35, 66], classifying proteins [20, 49], and fraud detection [18, 37, 46, 51–55]. Various methods such as label propagation [68], belief propagation [38], iterative classification [43], and graph neural networks (GNNs) [17, 20, 26, 49, 62] have been proposed for node/graph classifications. Among them, GNNs have attracted much attention recently because of their expressiveness and superior performance.
However, several recent works [1, 10, 50, 69, 70] have demonstrated that an attacker can easily fool GNNs to make incorrect predictions via perturbing 1) the node features and/or 2) the structure of the graph (i.e., adding and/or deleting edges in the graph). Therefore, it is of great importance to develop certifiably robust GNNs against such attacks. A few recent works [2, 3, 24, 71, 72] study certified robustness of GNNs against node feature or structural perturbation. However, these methods only focus on certifying robustness for a specific GNN [2, 24, 71, 72] or/and did not formally prove tight robustness guarantees [3] in general.
We aim to bridge this gap in this work. Specifically, we aim to provide tight certified robustness guarantees of any GNN classifier against structural perturbations for both node and graph classification. Towards this goal, we leverage a recently developed technique called randomized smoothing [5, 9, 21, 28, 32, 33], which can turn any base classifier (e.g., a GNN classifier in our problem) to a robust one via adding random noise to a testing example (i.e., a graph in our problem). A classifier is certifiably robust if it provably predicts the same label when the attacker adds/deletes at most 𝐾 edges in the graph, where we call 𝐾 certified perturbation size.
Graph structures are binary data, i.e., a pair of nodes can be either connected or unconnected. Therefore, we develop randomized smoothing for binary data and leverage it to obtain certified robustness of GNNs against structural perturbation. First, we theoretically derive a certified perturbation size for any GNN classifier with randomized smoothing via addressing several challenges. For instance, we divide the graph structure space into regions in a novel way such that we can apply the Neyman-Pearson Lemma [36] to certify robustness. We also prove that our derived certified perturbation size is tight if no assumptions on the GNN classifier are made.
Second, we design a method to compute our certified perturbation size in practice. It is challenging to compute our certified perturbation size as it involves estimating probability bounds simultaneously and solving an optimization problem. To address the challenge, we first adopt the simultaneous confidence interval estimation method [21] to estimate the probability bounds with probabilistic guarantees. Then, we design an algorithm to solve the

optimization problem to obtain our certified perturbation size with the estimated probability bounds.
We also empirically evaluate our method. Specifically, for node classification, we consider Graph Convolutional Network (GCN) [26] and Graph Attention Network (GAT) [49] on several benchmark datasets including Cora, Citeseer, and Pubmed [43]. For graph classification, we consider Graph Isomorphism Network (GIN) [61] on benchmark datasets including MUTAG, PROTEINS, and IMDB [63]. For instance, on the Cora dataset, GCN with our randomized smoothing can achieve certified accuracies of 0.55, 0.50, and 0.49 when the attacker arbitrarily adds/deletes at most 5, 10, and 15 edges, respectively. On the MUTAG dataset, GIN with our randomized smoothing can achieve certified accuracies of 0.45, 0.45, and 0.40 when the attacker arbitrarily adds/deletes at most 5, 10, and 15 edges, respectively.
Our major contributions can be summarized as follows: • We prove the certified robustness guarantee of any GNN against structural perturbation. Moreover, we show that our certified robustness guarantee is tight. • Our certified perturbation size is the solution to an optimization problem and we design an algorithm to solve the problem. • We empirically evaluate our method for both node and graph classifications on multiple benchmark datasets.
2 BACKGROUND 2.1 Node Classification vs. Graph Classification
We consider GNNs for both node classification [20, 26, 49] and graph classification [20, 61]. Suppose we are given an undirected graph 𝐺 with node features. • Node classification. A node classifier predicts labels for nodes
in the graph in a semi-supervised fashion. Specifically, a subset of nodes in 𝐺 are already labeled, which are called training nodes and denoted as 𝑉𝑇 . A node classifier 𝑓𝑛 takes the graph 𝐺 and the training nodes 𝑉𝑇 as an input and predicts the label for remaining nodes, i.e., 𝑓𝑛 (𝐺, 𝑉𝑇 , 𝑢) is the predicted label for a node 𝑢. • Graph classification. A graph classifier aims to predict a label for the entire graph instead of individual nodes, i.e., 𝑓𝑔 (𝐺) is the predicted label for the graph 𝐺. Such a graph classifier can be trained using a set of graphs with ground truth labels.
2.2 Adversarial Structural Perturbation
An attacker aims to fool a node classifier or graph classifier to make predictions as the attacker desires via perturbing the graph structure, i.e., deleting some edges and/or adding some edges in the graph [1, 10, 50, 69, 70]. Since our work focuses on structural perturbation, we treat the node feature matrix and the training nodes as constants. Moreover, we simply write a node classifier 𝑓𝑛 (𝐺, 𝑉𝑇 , 𝑢) or a graph classifier 𝑓𝑔 (𝐺) as 𝑓 (s), where the binary vector s represents the graph structure. Note that a node classifier should also take a node 𝑢 as input and predict its label. However, we omit the explicit dependency on a node 𝑢 for simplicity. We call s structure vector. For instance, s can be the concatenation of the upper triangular part of the adjacency matrix of the graph (excluding the diagonals) when the attacker can modify the connection status of any pair of nodes, i.e., s includes the connection status for each

pair of nodes in the graph. When the attacker can only modify the connection status between 𝑢 and each remaining node, s can also be the 𝑢th row of the adjacency matrix of the graph (excluding the self-loop (𝑢, 𝑢)th entry). We assume the structure vector s has 𝑛 entries. As we will see, such simplification makes it easier to present
our certified robustness against structural perturbation. We denote by vector 𝛿 ∈ {0, 1}𝑛 the attacker’s perturbation to
the graph structure. Specifically, 𝛿𝑖 = 1 if and only if the attacker changes the 𝑖th entry in the structure vector s, i.e., the attacker changes the connection status of the corresponding pair of nodes. Moreover, s ⊕ 𝛿 is the perturbed structure vector, which represents the perturbed graph structure, where ⊕ is the XOR operator between two binary variables. We use ||𝛿 ||0 to measure the magnitude of the adversarial perturbation as it has semantic meanings. Specifically, ||𝛿 ||0 is the number of node pairs whose connection statuses are modified by the attacker.

3 CERTIFIED ROBUSTNESS
3.1 Randomized Smoothing with Binary Noise
We first define a noise distribution in the discrete structure vector space {0, 1}𝑛. Then, we define a smoothed classifier based on the noise distribution and a node/graph classifier (called base classifier). Specifically, we consider the noise vector 𝜖 has the following probability distribution in the discrete space {0, 1}𝑛:

Pr(𝜖𝑖 = 0) = 𝛽, Pr(𝜖𝑖 = 1) = 1 − 𝛽,

(1)

where 𝑖 = 1, 2, · · · , 𝑛. When we add a random noise vector 𝜖 to the structure vector s, the 𝑖th entry of s is preserved with probability 𝛽 and changed with probability 1 − 𝛽. In other words, our random
noise means that, for each pair of nodes in the graph, we keep its
connection status with probability 𝛽 and change its connection status with probability 1 − 𝛽. Based on the noise distribution and a base node/graph classifier 𝑓 (s), we define a smoothed classifier 𝑔(s) as follows:

𝑔(s) = argmax Pr(𝑓 (s ⊕ 𝜖) = 𝑐),

(2)

𝑐∈C

where C is the set of labels, ⊕ is the XOR operator between two binary variables, Pr(𝑓 (s ⊕ 𝜖) = 𝑐) is the probability that the base classifier 𝑓 predicts label 𝑐 when we add random noise 𝜖 to the structure vector s, and 𝑔(s) is the label predicted for s by the smoothed classifier. Moreover, we note that 𝑔(s ⊕ 𝛿) is the label predicted for the perturbed structure vector s ⊕ 𝛿. Existing randomized smoothing methods [5, 9, 21, 28, 32, 33] add random continuous noise (e.g.,
Gaussian noise, Laplacian noise) to a testing example (i.e., the structure vector s in our case). However, such continuous noise is not meaningful for the binary structure vector.
Our goal is to show that a label is provably the predicted label by the smoothed classifier 𝑔 for the perturbed structure vector s ⊕ 𝛿 when the ℓ0-norm of the adversarial perturbation 𝛿, i.e., ∥𝛿 ∥0, is bounded. Next, we theoretically derive the certified perturbation
size 𝐾, prove the tightness of the certified perturbation size, and
discuss how to compute the certified perturbation size in practice.

3.2 Theoretical Certification
3.2.1 Overview. Let 𝑋 = s ⊕ 𝜖 and 𝑌 = s ⊕ 𝛿 ⊕ 𝜖 be two random variables, where 𝜖 is the random binary noise drawn from the distribution defined in Equation 1. 𝑋 and 𝑌 represent random structure vectors obtained by adding random binary noise 𝜖 to the structure vector s and its perturbed version s ⊕ 𝛿, respectively.
Suppose, when taking 𝑋 as an input, the base GNN classifier 𝑓 correctly predicts the label 𝑐𝐴 with the largest probability. Then, our key idea is to guarantee that, when taking 𝑌 as an input, 𝑓 still predicts 𝑐𝐴 with the largest probability. Moreover, we denote 𝑐𝐵 as the predicted label by 𝑓 with the second largest probability.
Then, our goal is to find the maximum perturbation size such that the following inequality holds:

Pr(𝑓 (𝑌 ) = 𝑐𝐴) > Pr(𝑓 (𝑌 ) = 𝑐𝐵).

(3)

Note that it is challenging to compute the probabilities Pr(𝑓 (𝑌 ) = 𝑐𝐴) and Pr(𝑓 (𝑌 ) = 𝑐𝐵) exactly because 𝑓 is highly nonlinear in practice. To address the challenge, we first derive a lower bound of Pr(𝑓 (𝑌 ) = 𝑐𝐴) and an upper bound of Pr(𝑓 (𝑌 ) = 𝑐𝐵). Then, we require that the lower bound of Pr(𝑓 (𝑌 ) = 𝑐𝐴) is larger than the upper bound of Pr(𝑓 (𝑌 ) = 𝑐𝐵). Specifically, we derive the lower bound and upper bound by constructing certain regions in the graph structure space {0, 1}𝑛 such that the probabilities 𝑌 is in these regions can be efficiently computed for any ||𝛿 ||0 = 𝑘. Then, we iteratively search the maximum 𝑘 under the condition that the
lower bound is larger than the upper bound. Finally, we treat the
maximum 𝑘 as the certified perturbation size 𝐾.

3.2.2 Deriving the lower and upper bounds. Our idea is to divide the graph structure space {0, 1}𝑛 into regions in a novel way such

that we can apply the Neyman-Pearson Lemma [36] to derive the

lower bound and upper bound. First, for any data point z ∈ {0, 1}𝑛,

we have the density ratio

Pr(𝑋 =z) Pr(𝑌 =z)

=

𝛽 𝑤−𝑣
1−𝛽 based on the noise

distribution defined in Equation 1, where 𝑤 = ∥s − z∥0 and 𝑣 =

∥s ⊕ 𝛿 − z∥0 (please refer to Section 7.4 in for details). Therefore,

we have the density ratio

Pr(𝑋 =z) Pr(𝑌 =z)

=

𝛽 1−𝛽

𝑚

for any z

∈

{0, 1}𝑛,

where 𝑚 = −𝑛, −𝑛 + 1, · · · , 𝑛 − 1, 𝑛. Furthermore, we define a region

R (𝑚) as the set of data points whose density ratios are

𝛽 1−𝛽

𝑚
, i.e.,

R (𝑚)

= {z ∈

{0, 1}𝑛

:

Pr(𝑋 =z) Pr(𝑌 =z)

=

𝛽 1−𝛽

𝑚 }, and we denote by 𝑟 (𝑚)

the corresponding density ratio, i.e., 𝑟 (𝑚) =

𝛽𝑚
1−𝛽 . Moreover, we

rank the 2𝑛 + 1 regions in a descending order with respect to their

density ratios, and denote the ranked regions as R1, R2, · · · , R2𝑛+1.

Suppose we have a lower bound of the largest label probability Pr(𝑓 (𝑋 ) = 𝑐𝐴) for 𝑐𝐴 and denote it as 𝑝𝐴, and an upper bound of the remaining label probability Pr(𝑓 (𝑋 ) = 𝑐) for 𝑐 ≠ 𝑐𝐴 and denote it as 𝑝𝐵. Assuming there exist 𝑐𝐴 and 𝑝𝐴, 𝑝𝐵 ∈ [0, 1] such that

Pr ( 𝑓

(𝑋 )

=

𝑐𝐴)

≥

𝑝𝐴

≥

𝑝𝐵

≥

max Pr(𝑓
𝑐 ≠𝑐𝐴

(𝑋 )

=

𝑐).

(4)

Next, we construct two regions A and B such that Pr(𝑋 ∈ A) = 𝑝𝐴 and Pr(𝑋 ∈ B) = 𝑝𝐵, respectively. Specifically, we gradually add the regions R1, R2, · · · , R2𝑛+1 to A until Pr(𝑋 ∈ A) = 𝑝𝐴. Moreover, we gradually add the regions R2𝑛+1, R2𝑛, · · · , R1 to B until Pr(𝑋 ∈ B) = 𝑝𝐵. We construct the regions A and B in this
way such that we can apply the Neyman-Pearson Lemma [36] for

them. Formally, we define the regions A and B as follows:

𝑎★−1

A = R 𝑗 ∪ R𝑎★

(5)

𝑗 =1

2𝑛+1

B=

R 𝑗 ∪ R𝑏★,

(6)

𝑗 =𝑏★+1

where

𝑎

∑︁

★
𝑎

=

argmin

𝑎, 𝑠.𝑡 .

Pr(𝑋 ∈ R 𝑗 ) ≥ 𝑝𝐴,

𝑎 ∈ {1,2,··· ,2𝑛+1}

𝑗 =1

2𝑛+1

∑︁

★
𝑏

=

argmax

𝑏, 𝑠.𝑡 .

Pr(𝑋 ∈ R 𝑗 ) ≥ 𝑝𝐵 .

𝑏 ∈ {1,2,··· ,2𝑛+1}

𝑗 =𝑏

R𝑎★ is any subregion of R𝑎★ such that Pr(𝑋 ∈ R𝑎★) = 𝑝𝐴 −

𝑎 ∗ −1 𝑗 =1

Pr

(𝑋

∈

R 𝑗 ), and R𝑏★

is any subregion of R𝑏★ such that

Pr(𝑋 ∈ R𝑏★) = 𝑝𝐵 −

2𝑛+1 𝑗 =𝑏∗+1

Pr

(𝑋

∈

R 𝑗 ).

Finally, based on the Neyman-Pearson Lemma, we can derive a

lower bound of Pr(𝑓 (𝑌 ) = 𝑐𝐴) and an upper bound of Pr(𝑓 (𝑌 ) = 𝑐𝐵). Formally, we have the following lemma:

Lemma 1. We have the following bounds:

Pr(𝑓 (𝑌 ) = 𝑐𝐴) ≥ Pr(𝑌 ∈ A),

(7)

Pr(𝑓 (𝑌 ) = 𝑐𝐵) ≤ Pr(𝑌 ∈ B).

(8)

Proof. See Section 7.1.

□

3.2.3 Deriving the certified perturbation size. Given Lemma 1, we

can derive the certified perturbation size 𝐾 as the maximum 𝑘 such that the following inequality holds for ∀||𝛿 ||0 = 𝑘:

Pr(𝑌 ∈ A) > Pr(𝑌 ∈ B).

(9)

Formally, we have the following theorem:

Theorem 1 (Certified Perturbation Size). Let 𝑓 be any base node/graph classifier. The random noise vector 𝜖 is defined in Equation 1. The smoothed classifier 𝑔 is defined in Equation 2. Given a structure vector s ∈ {0, 1}𝑛, suppose there exist 𝑐𝐴 and 𝑝𝐴, 𝑝𝐵 ∈ [0, 1]
that satisfy Equation 4. Then, we have

𝑔(s ⊕ 𝛿) = 𝑐𝐴, ∀||𝛿 ||0 ≤ 𝐾,

(10)

where the certified perturbation size 𝐾 is the solution to the following optimization problem:

𝐾 = argmax 𝑘,

(11)

s.t. Pr(𝑌 ∈ A) > Pr(𝑌 ∈ B), ∀||𝛿 ||0 = 𝑘.

(12)

Proof. See Section 7.2.

□

Next, we show that our certified perturbation size is tight in the following theorem.

Theorem 2 (Tightness of the Certified Perturbation Size).
Assume 𝑝𝐴 ≥ 𝑝𝐵, 𝑝𝐴 + 𝑝𝐵 ≤ 1, and 𝑝𝐴 + (|C| − 1) · 𝑝𝐵 ≥ 1, where
|C| is the number of labels. For any perturbation 𝛿 with ||𝛿 ||0 > 𝐾, there exists a base classifier 𝑓 ∗ consistent with Equation 4 such that 𝑔(s ⊕ 𝛿) ≠ 𝑐𝐴 or there exist ties.

Proof. See Section 7.3.

□

We have several observations on our major theorems. • Our theorems are applicable to any base node/graph classifier. Moreover, although we focus on classifiers on graphs, our theorems are applicable to any base classifier that takes binary features as input. • Our certified perturbation size 𝐾 depends on 𝑝𝐴, 𝑝𝐵, and 𝛽. In particular, when the probability bounds 𝑝𝐴 and 𝑝𝐵 are tighter, our certified perturbation size 𝐾 is larger. We use the probability bounds 𝑝𝐴 and 𝑝𝐵 instead of their exact values, because it is challenging to compute 𝑝𝐴 and 𝑝𝐵 exactly. • When no assumptions on the base classifier are made and randomized smoothing with the noise distribution defined in Equation 1 is used, it is impossible to certify a perturbation size larger than 𝐾.

3.3 Certification in Practice

Computing our certified perturbation size 𝐾 in practice faces two

challenges. The first challenge is to estimate the probability bounds

𝑝𝐴 and 𝑝𝐵. The second challenge is to solve the optimization prob-

lem in Equation 11 to get 𝐾 with the given 𝑝𝐴 and 𝑝𝐵. We leverage the method developed in [21] to address the first challenge, and we

develop an efficient algorithm to address the second challenge.

Estimating 𝑝𝐴 and 𝑝𝐵: We view the probabilities 𝑝1, 𝑝2, · · · , 𝑝 | C | as a multinomial distribution over the label set C. If we sample

a noise 𝜖 from our noise distribution uniformly at random, then 𝑓 (s ⊕ 𝜖) can be viewed as a sample from the multinomial distribution. Then, estimating 𝑝𝐴 and 𝑝𝑐 for 𝑐 ∈ C \ {𝑐𝐴} is essentially a
one-sided simultaneous confidence interval estimation problem. In

particular, we leverage the simultaneous confidence interval esti-

mation method developed in [21] to estimate these bounds with a confidence level at least 1 − 𝛼. Specifically, we sample 𝑑 random noise, i.e., 𝜖1, 𝜖2, · · · , 𝜖𝑑 , from the noise distribution defined in

Equation 1. We denote by 𝑑𝑐 the frequency of the label 𝑐 predicted

by the base classifier for the 𝑑 noisy examples. Formally, we have

𝑑𝑐 =

𝑑 𝑗 =1

I(

𝑓

(s

⊕

𝜖𝑗)

=

𝑐)

for

each 𝑐

∈

C

and

I

is

the

indicator

function. Moreover, we assume 𝑐𝐴 has the largest frequency and the smoothed classifier predicts 𝑐𝐴 as the label, i.e., 𝑔(s) = 𝑐𝐴. Ac-

cording to [21], we have the following probability bounds with a

confidence level at least 1 − 𝛼:

𝛼

𝑝𝐴 = 𝐵 |C| ; 𝑑𝑐𝐴, 𝑑 − 𝑑𝑐𝐴 + 1

(13)

𝛼

𝑝𝑐 = 𝐵 1 − |C| ; 𝑑𝑐 + 1, 𝑑 − 𝑑𝑐 , ∀𝑐 ≠ 𝑐𝐴,

(14)

where 𝐵(𝑞; 𝑢, 𝑤) is the 𝑞th quantile of a beta distribution with shape parameters 𝑢 and 𝑤. Then, we estimate 𝑝𝐵 as follows:

𝑝𝐵 = min

max
𝑐 ≠𝑐𝐴

𝑝𝑐

,

1

−

𝑝𝐴

.

(15)

Computing 𝐾: After estimating 𝑝𝐴 and 𝑝𝐵, we solve the optimization problem in Equation 11 to obtain 𝐾. First, we have:

𝑎★−1

𝑎★−1

∑︁

∑︁

Pr(𝑌 ∈ A) = Pr(𝑌 ∈ R 𝑗 ) + (𝑝𝐴 − Pr(𝑋 ∈ R 𝑗 ))/𝑟𝑎★,

𝑗 =1

𝑗 =1

(16)

Table 1: Dataset statistics.

Dataset

Node Classification

Cora Citeseer Pubmed

#Nodes 2,708 3,327 19,717

#Edges 5,429 4,732 44,338

#Classes 7 6 3

Dataset

#Graphs Ave.#Nodes #Classes

MUTAG 188

17.9

2

Graph

Classification PROTEINS 1,113

39.1

2

IMDB

1,500

13.0

3

2𝑛+1

2𝑛+1

∑︁

∑︁

Pr(𝑌 ∈ B) =

Pr(𝑌 ∈ R 𝑗 ) + (𝑝𝐵 −

Pr(𝑋 ∈ R 𝑗 ))/𝑟𝑏★,

𝑗 =𝑏★+1

𝑗 =𝑏★+1

(17)

where 𝑟𝑎★ and 𝑟𝑏★ are the density ratios in the regions R𝑎★ and R𝑏★, respectively. Therefore, the key to solve the optimization problem is to compute Pr(𝑋 ∈ R (𝑚)) and Pr(𝑌 ∈ R (𝑚)) for each 𝑚 ∈ {−𝑛, −𝑛 + 1, · · · , 𝑛} when ||𝛿 ||0 = 𝑘. Specifically, we have:

min{𝑛,𝑛+𝑚 }

Pr(𝑋 ∈ R (𝑚)) =

∑︁

𝑛−
𝛽

(

𝑗

−𝑚)

(1

−

𝛽) ( 𝑗−𝑚)

·

𝑡 (𝑚,

𝑗)

(18)

𝑗=max{0,𝑚 }

min{𝑛,𝑛+𝑚 }

Pr(𝑌 ∈ R (𝑚)) =

∑︁

𝑛−𝑗
𝛽

(1

−

𝛽)𝑗

·

𝑡 (𝑚,

𝑗 ),

(19)

𝑗=max{0,𝑚 }

where 𝑡 (𝑚, 𝑗) is defined as follows:

0,





𝑡

(𝑚,

𝑗

)

=

 0,

if (𝑚 + 𝑘) mod 2 ≠ 0,

if 2𝑗 − 𝑚 < 𝑘,

(20)

 𝑛−𝑘 𝑘

 

2𝑗 −𝑚−𝑘

𝑘−𝑚 ,

otherwise.



2

2



See Section 7.4 for the details on obtaining Equation 18 and 19.

Then, we iteratively find the largest 𝑘 such that the constraint in

Equation 12 is satisfied.

4 EVALUATION 4.1 Experimental Setup
We evaluate our method on multiple GNNs and benchmark datasets for both node classification and graph classification.
Benchmark datasets and GNNs: We use benchmark graphs and GNNs for both node and graph classification. Table 1 shows the statistics of our graphs. • Node classification: We consider Graph Convolutional Net-
work (GCN) [26] and Graph Attention Network (GAT) [49] for node classification. Moreover, we use the Cora, Citeseer, and Pubmed datasets [43]. They are citation graphs, where nodes are documents and edges indicate citations between documents. In particular, an undirected edge between two documents is created if one document cites the other. The bag-of-words feature of a document is treated as the node feature. Each document also has a label.

Certiﬁed Accuracy

Certiﬁed Accuracy

1.0

1.0

1.0

β =0.5

β =0.5

β =0.5

0.8

β =0.7 β =0.9

0.8

β =0.7 β =0.9

0.8

β =0.7 β =0.9

0.6

0.6

0.6

Certiﬁed Accuracy

Certiﬁed Accuracy

0.4

0.4

0.4

0.2

0.2

0.2

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(a) Cora

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(b) Citeseer

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(c) Pubmed

Figure 1: Impact of 𝛽 on the certified accuracy of the smoothed GCN.

1.0

1.0

1.0

β =0.5

β =0.5

β =0.5

0.8

β =0.7 β =0.9

0.8

β =0.7 β =0.9

0.8

β =0.7 β =0.9

0.6

0.6

0.6

Certiﬁed Accuracy

Certiﬁed Accuracy

0.4

0.4

0.4

0.2

0.2

0.2

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(a) Cora

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(b) Citeseer

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(c) Pubmed

Figure 2: Impact of 𝛽 on the certified accuracy of the smoothed GAT.

1.0

1.0

1.0

β =0.5

β =0.5

β =0.5

0.8

β =0.7 β =0.9

0.8

β =0.7 β =0.9

0.8

β =0.7 β =0.9

0.6

0.6

0.6

Certiﬁed Accuracy

Certiﬁed Accuracy

0.4

0.4

0.4

0.2

0.2

0.2

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(a) MUTAG

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(b) PROTEINS

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(c) IMDB

Figure 3: Impact of 𝛽 on the certified accuracy of the smoothed GIN.

Certiﬁed Accuracy

• Graph classification: We consider Graph Isomorphism Network (GIN) [61] for graph classification. Moreover, we use the MUTAG, PROTEINS, and IMDB datasets [63]. MUTAG and PROTEINS are bioinformatics datasets. MUTAG contains 188 mutagenic aromatic and heteroaromatic nitro compounds, where each compound represents a graph and each label means whether or not the compound has a mutagenic effect on the Gramnegative bacterium Salmonella typhimurium. PROTEINS is a dataset where nodes are secondary structure elements and there is an edge between two nodes if they are neighbors in the amino-acid sequence or in three-dimensional space. Each protein is represented as a graph and is labeled as enzyme or non-enzyme. IMDB is a movie collaboration dataset. Each graph is an ego-network of an actor/actress, where nodes are actors/actresses and an edge between two actors/actresses indicates that they appear in the same movie. Each graph is obtained from a certain genre of movies, and the task is to classify the genre of a graph.

Training and testing: For each node classification dataset, following previous works [26, 49], we sample 20 nodes from each class uniformly at random as the training dataset. Moreover, we randomly sample 100 nodes for testing. For each graph classification dataset, we use 90% of the graphs for training and the remaining 10% for testing, similar to [61].
Parameter setting: We implement our method in pyTorch. To compute the certified perturbation size, our method needs to specify the noise parameter 𝛽, the confidence level 1 − 𝛼, and the number of samples 𝑑. Unless otherwise mentioned, we set 𝛽 = 0.7, 1 − 𝛼 = 99.9%, and 𝑑 = 10, 000. We also explore the impact of each parameter while fixing the other parameters to the default settings in our experiments. When computing the certified perturbation size for a node 𝑢 in node classification, we consider an attacker perturbs the connection status between 𝑢 and the remaining nodes in the graph. We use the publicly available source code for GCN, GAT, and GIN. We use our randomized smoothing to smooth each classifier.

Certiﬁed Accuracy Certiﬁed Accuracy Certiﬁed Accuracy Certiﬁed Accuracy

1.0

1 − α =99.0%

0.8

1 − α =99.9% 1 − α =99.99%

0.6

0.4

0.2

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(a) Smoothed GCN on Cora

1.0

1 − α =99.0%

0.8

1 − α =99.9% 1 − α =99.99%

0.6

0.4

0.2

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(b) Smoothed GIN on MUTAG

Figure 4: Impact of 1 − 𝛼 on the certified accuracy of (a) the smoothed GCN and (b) the smoothed GIN.

1.0

d =1K

0.8

d =10K d =50K

0.6

0.4

0.2

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(a) Smoothed GCN on Cora

1.0

d =1K

0.8

d =10K d =50K

0.6

0.4

0.2

0.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Perturbation Size
(b) Smoothed GIN on MUTAG

Figure 5: Impact of 𝑑 on the certified accuracy of (a) the smoothed GCN and (b) the smoothed GIN.

4.2 Experimental Results
Like Cohen et al. [9], we use certified accuracy as the metric to evaluate our method. Specifically, for a smoothed GNN classifier and a given perturbation size, certified accuracy is the fraction of testing nodes (for node classification) or testing graphs (for graph classification), whose labels are correctly predicted by the smoothed classifier and whose certified perturbation size is no smaller than the given perturbation size.
Impact of the noise parameter 𝛽: Figure 1 and Figure 2 respectively show the certified accuracy of the smoothed GCN and smoothed GAT vs. perturbation size for different 𝛽 on the three node classification datasets. Figure 3 shows the certified accuracy of GIN with randomized smoothing vs. perturbation size for different 𝛽 on the three graph classification datasets.
We have two observations. First, when 𝛽 = 0.5, the certified accuracy is independent with the perturbation size, which means that an attacker can not attack a smoothed GNN classifier via perturbing the graph structure. This is because 𝛽 = 0.5 essentially means that the graph is sampled from the space {0, 1}𝑛 uniformly at random. In other words, the graph structure does not contain information about the node labels and a smoothed GNN classifier is reduced to only using the node features. Second, 𝛽 controls a tradeoff between accuracy under no attacks and robustness. Specifically, when 𝛽 is larger, the accuracy under no attacks (i.e., perturbation size is 0) is larger, but the certified accuracy drops more quickly as the perturbation size increases. Impact of the confidence level 1 − 𝛼: Figure 4a and 4b show the certified accuracy of the smoothed GCN and smoothed GIN vs. perturbation size for different confidence levels, respectively. We observe that as the confidence level 1 − 𝛼 increases, the certified accuracy curve becomes slightly lower. This is because a higher confidence level leads to a looser estimation of the probability bound 𝑝𝐴 and 𝑝𝐵, which means a smaller certified perturbation size for a testing node/graph. However, the differences of the certified accuracies between different confidence levels are negligible when the confidence levels are large enough.
Impact of the number of samples 𝑑: Figure 5a and 5b show the certified accuracy of the smoothed GCN and smoothed GIN vs. perturbation size for different numbers of samples 𝑑, respectively. We observe that as the number of samples increases, the certified accuracy curve becomes higher. This is because a larger

number of samples makes the estimated probability bound 𝑝𝐴 and 𝑝𝐵 tighter, which means a larger certified perturbation size for a testing node/graph. On Cora, the smoothed GCN achieves certified accuracies of 0.55, 0.50, and 0.49 when 𝑑 = 50𝐾 and the attacker arbitrarily adds/deletes at most 5, 10, and 15 edges, respectively. On MUTAG, GIN with our randomized smoothing achieves certified accuracies of 0.45, 0.45, and 0.40 when 𝑑 = 50𝐾 and the attacker arbitrarily adds/deletes at most 5, 10, and 15 edges, respectively.
5 RELATED WORK
We review studies on certified robustness for classifiers on both non-graph data and graph data.
5.1 Non-graph Data
Various methods have been tried to certify robustness of classifiers. A classifier is certifiably robust if it predicts the same label for data points in a certain region around an input example. Existing certification methods leverage Satisfiability Modulo Theories [6, 13, 25, 42], mixed integer-linear programming [4, 8, 15], linear programming [57, 58], semidefinite programming [39, 40], dual optimization [11, 12], abstract interpretation [16, 34, 44], and layerwise relaxtion [56, 65]. However, these methods are not scalable to large neural networks and/or are only applicable to specific neural network architectures.
Randomized smoothing [5, 9, 21, 28, 29, 31–33, 41, 64] was originally developed to defend against adversarial examples. It was first proposed as an empirical defense [5, 33]. For instance, Cao and Gong [5] proposed to use uniform random noise from a hypercube centered at an input example to smooth a base classifier. Lecuyer et al. [28] derived a certified robustness guarantee for randomized smoothing with Gaussian noise or Laplacian noise via differential privacy techniques. Li et al. [32] leveraged information theory to derive a tighter certified robustness guarantee. Cohen et al. [9] leveraged the Neyman-Pearson Lemma [36] to obtain a tight certified robustness guarantee under 𝐿2-norm for randomized smoothing with Gaussian noise. Specifically, they showed that a smoothed classifier verifiably predicts the same label when the 𝐿2-norm of the adversarial perturbation to an input example is less than a threshold. Salman et al. [41] designed an adaptive attack against smoothed classifiers and used the attack to train classifiers in an adversarial training paradigm to enlarge the certified robuseness

under 𝐿2-norm. Zhai et al. [64] proposed to explicitly maximize the certified robustness via a new training strategy. All these methods focused on top-1 predictions. Jia et al. [21] extended [9] to derive the first certification of top-𝑘 predictions against adversarial examples. Compared to other methods for certified robustness, randomized smoothing has two key advantages: 1) it is scalable to large neural networks, and 2) it is applicable to any base classifier.
5.2 Graph Data
Several studies [1, 7, 10, 45, 50, 59, 60, 69, 70] have shown that attackers can fool GNNs via manipulating the node features and/or graph structure. [14, 47, 48, 59, 60, 67] proposed empirical defenses without certified robustness guarantees. A few recent works [2, 3, 24, 71, 72] study certified robustness of GNNs. Zügner and Günnemann [71] considered node feature perturbation. In particular, they derived certified robustness for a particular type of GNN called graph convolutional network [26] against node feature perturbation. More specifically, they formulated the certified robustness as a linear programming problem, where the objective is to require that a node’s prediction is constant within an allowable node feature perturbation. Then, they derived the robustness guarantee based on the dual form of the optimization problem, which is motivated by [57]. [2, 24, 72] considered structural perturbation for a specific GNN. Specifically, [2] required that GNN prediction is a linear function of (personalized) PageRank [27] and [24, 72] could only certify the robustness of GCNs. For example, [24] derived the certified robustness of GCN for graph classification via utilizing dualization and convex envelope tools.
Two recent works [3, 22] leveraged randomized smoothing to provide a model-agnostic certification against structural perturbation. Jia et al. [22] certified the robustness of community detection against structural perturbation. They essentially model the problem as binary classification, i.e., whether a set of nodes are in the same community or not, and design a randomized smoothing technique to certify its robustness. Bojchevski et al. [3] generalize the randomized smoothing technique developed in [29] to the sparse setting and derive certified robustness for GNNs. They did not formally prove that their multi-class certificate is tight. However, in the special case of equal flip probabilities, which is equivalent to our certificate, our tightness proof (Theorem 2) applies.
6 CONCLUSION AND FUTURE WORK
In this work, we develop the certified robustness guarantee of GNNs for both node and graph classifications against structural perturbation. Our results are applicable to any GNN classifier. Our certification is based on randomized smoothing for binary data which we develop in this work. Moreover, we prove that our certified robustness guarantee is tight when randomized smoothing is used and no assumptions on the GNNs are made. An interesting future work is to incorporate the information of a given GNN to further improve the certified robustness guarantee.
7 PROOFS 7.1 Proof of Lemma 1
We first describe the Neyman-Pearson Lemma for binary random variables, which we use to prove Lemma 1.

Lemma 2 (Neyman-Pearson Lemma for Binary Random Variables).

Let 𝑋 and 𝑌 be two random variables in the discrete space {0, 1}𝑛

with probability distributions Pr(𝑋 ) and Pr(𝑌 ), respectively. Let ℎ :

{0, 1}𝑛 → {0, 1} be a random or deterministic function.

•

Let 𝑆1

= {z ∈ {0, 1}𝑛

:

Pr(𝑋 =z) Pr(𝑌 =z)

> 𝑟 } and 𝑆2

= {z ∈

{0, 1}𝑛

:

Pr(𝑋 =z) Pr(𝑌 =z)

= 𝑟 } for some 𝑟

>

0. Assume 𝑆3

⊆

𝑆2 and 𝑆

= 𝑆1

𝑆3.

If Pr(ℎ(𝑋 ) = 1) ≥ Pr(𝑋 ∈ 𝑆), then Pr(ℎ(𝑌 ) = 1) ≥ Pr(𝑌 ∈

𝑆).

•

Let 𝑆1

= {z ∈ {0, 1}𝑛

:

Pr(𝑋 =z) Pr(𝑌 =z)

< 𝑟 } and 𝑆2

= {z ∈

{0, 1}𝑛

:

Pr(𝑋 =z) Pr(𝑌 =z)

= 𝑟 } for some 𝑟

>

0. Assume 𝑆3

⊆

𝑆2 and 𝑆

= 𝑆1

𝑆3.

If Pr(ℎ(𝑋 ) = 1) ≤ Pr(𝑋 ∈ 𝑆), then Pr(ℎ(𝑌 ) = 1) ≤ Pr(𝑌 ∈

𝑆).

Proof. The proof can be found in a standard statistics textbook,

e.g., [30]. For completeness, we include the proof here. Without loss

of generality, we assume that ℎ is random and denote ℎ(1|𝑧) (resp.

ℎ(0|𝑧)) as the probability that ℎ(𝑧) = 1 (resp. ℎ(𝑧) = 0)). We denote

𝑆𝑐 as the complement of 𝑆, i.e., 𝑆𝑐 = {0, 1}𝑛 \ 𝑆. For any z ∈ 𝑆, we

have

Pr(𝑋 =z) Pr(𝑌 =z)

≥ 𝑟 , and for any z ∈ 𝑆𝑐 , we have

Pr(𝑋 =z) Pr(𝑌 =z)

≤ 𝑟 . We

prove the first part, and the second part can be proved similarly.

Pr(ℎ(𝑌 ) = 1) − Pr(𝑌 ∈ 𝑆) =

∑︁

∑︁

ℎ(1|z)Pr(𝑌 = z) − Pr(𝑌 = z)

z∈ {0,1}𝑛

z ∈𝑆

∑︁

∑︁

=

ℎ(1|z)Pr(𝑌 = z) + ℎ(1|z)Pr(𝑌 = z)

z ∈𝑆 𝑐

z ∈𝑆

∑︁

∑︁

− ℎ(1|z)Pr(𝑌 = z) + ℎ(0|z)Pr(𝑌 = z)

z ∈𝑆

z ∈𝑆

∑︁

∑︁

= ℎ(1|z)Pr(𝑌 = z) − ℎ(0|z)Pr(𝑌 = z)

z ∈𝑆 𝑐

z ∈𝑆

1 ∑︁

∑︁

≥

ℎ(1|z)Pr(𝑋 = z) − ℎ(0|z)Pr(𝑋 = z)

𝑟

z ∈𝑆 𝑐

z ∈𝑆

1 ∑︁

∑︁

=

ℎ(1|z)Pr(𝑋 = z) + ℎ(1|z)Pr(𝑋 = z)

𝑟

z ∈𝑆 𝑐

z ∈𝑆

∑︁

∑︁

− ℎ(1|z)Pr(𝑋 = z) + ℎ(0|z)Pr(𝑋 = z)

z ∈𝑆

z ∈𝑆

1 ∑︁

∑︁

=

ℎ(1|z)Pr(𝑋 = z) − Pr(𝑋 = z)

𝑟

z∈ {0,1}𝑛

z ∈𝑆

1 = Pr(ℎ(𝑋 ) = 1) − Pr(𝑋 ∈ 𝑆)
𝑟
≥ 0.

□

Next, we restate Lemma 1 and show our proof.

Lemma 1. We have the following bounds:

Pr(𝑓 (𝑌 ) = 𝑐𝐴) ≥ Pr(𝑌 ∈ A),

(7)

Pr(𝑓 (𝑌 ) = 𝑐𝐵) ≤ Pr(𝑌 ∈ B).

(8)

Proof. Based on the regions A and B defined in Equation 5 and Equation 6, we have Pr(𝑋 ∈ A) = 𝑝𝐴 and Pr(𝑋 ∈ B) = 𝑝𝐵.

Moreover, based on the conditions in Equation 4, we have:

Pr(𝑓 (𝑋 ) = 𝑐𝐴) ≥ 𝑝𝐴 = Pr(𝑋 ∈ A);

Pr(𝑓 (𝑋 ) = 𝑐𝐵) ≤ 𝑝𝐵 = Pr(𝑋 ∈ B). Next, we leverage Lemma 2 to derive the condition for Pr(𝑓 (𝑌 ) = 𝑐𝐴) > Pr(𝑓 (𝑌 ) = 𝑐𝐵). Specifically, we define ℎ(z) = I(𝑓 (z) = 𝑐𝐴). Then, we have:

Pr(ℎ(𝑋 ) = 1) = Pr(𝑓 (𝑋 ) = 𝑐𝐴) ≥ Pr(𝑋 ∈ A).

Pr(𝑋 =z)
Moreover, we have Pr(𝑌 =z)

> 𝑟𝑎★ for any z ∈

𝑎★−1 𝑗 =1

R

𝑗

and

Pr(𝑋 =z) Pr(𝑌 =z)

= 𝑟𝑎★

for any z

∈

R𝑎★, where 𝑟𝑎★

is the probability density

ratio in the region R𝑎★. Therefore, according to the first part of

Lemma 2, we have Pr(𝑓 (𝑌 ) = 𝑐𝐴) ≥ Pr(𝑌 ∈ A). Similarly, based

on the second part of Lemma 2, we have Pr(𝑓 (𝑌 ) = 𝑐𝐵) ≤ Pr(𝑌 ∈

B).

□

7.2 Proof of Theorem 1
Recall that our goal is to make Pr(𝑓 (𝑌 ) = 𝑐𝐴) > Pr(𝑓 (𝑌 ) = 𝑐𝐵). Based on Lemma 1, it is sufficient to require Pr(𝑌 ∈ A) > Pr(𝑌 ∈ B). Therefore, we derive the certified perturbation size 𝐾 as the maximum 𝑘 such that the above inequality holds for ∀||𝛿 ||0 = 𝑅.

7.3 Proof of Theorem 2
Our idea is to construct a base classifier 𝑓 ∗ consistent with the con-
ditions in Equation 4, but the smoothed classifier is not guaranteed
to predict 𝑐𝐴. Let disjoint regions A and B be defined as in Equation 5 and Equation 6. We denote Γ = {1, 2, · · · , 𝑐} \ {𝑐𝐴, 𝑐𝐵 }. Then, for each label 𝑐𝑖 in Γ, we can find a region C𝑖 ⊆ {0, 1}𝑛 \ (A ∪ B) such that C𝑖 ∩ C𝑗 = ∅, ∀𝑖, 𝑗 ∈ Γ, 𝑖 ≠ 𝑗 and Pr(𝑋 ∈ C𝑖 ) ≤ 𝑝𝐵. We can construct such regions because 𝑝𝐴 + (|C| − 1) · 𝑝𝐵 ≥ 1. Given those
regions, we construct the following base classifier:

𝑐𝐴 if z ∈ A



𝑓

∗ (z)

=

 𝑐𝐵

if z ∈ B

 𝑐𝑖

if z ∈ C𝑖



By construction, we have Pr(𝑓 ∗ (𝑋 ) = 𝑐𝐴) = 𝑝𝐴, Pr(𝑓 ∗ (𝑋 ) = 𝑐𝐵) =

𝑝𝐵, and Pr(𝑓 ∗ (𝑋 ) = 𝑐𝑖 ) ≤ 𝑝𝐵 for any 𝑐𝑖 ∈ Γ, which are consistent

with Equation 4. From our proof of Theorem 1, we know that when

||𝛿 ||0 > 𝐾, we have:

Pr(𝑌 ∈ A) ≤ Pr(𝑌 ∈ B),

or equivalently we have: Pr(𝑓 ∗ (𝑌 ) = 𝑐𝐴) ≤ Pr(𝑓 ∗ (𝑌 ) = 𝑐𝐵)
Therefore, we have either 𝑔(s ⊕ 𝛿) ≠ 𝑐𝐴 or there exist ties when ||𝛿 ||0 > 𝐾.

7.4 Computing Pr(𝑋 ∈ R (𝑚)) and Pr(𝑌 ∈ R (𝑚))
We first define the following regions:
R (𝑤, 𝑣) = {z ∈ {0, 1}𝑛 : ||z − s||0 = 𝑤 and ||z − s ⊕ 𝛿 ||0 = 𝑣 },
for 𝑤, 𝑣 ∈ {0, 1, · · · , 𝑛}. Intuitively, R (𝑤, 𝑣) includes the binary vectors that are 𝑤 bits different from s and 𝑣 bits different from s ⊕𝛿. Next, we compute the size of the region R (𝑤, 𝑣) when ||𝛿 ||0 = 𝑘. Without loss of generality, we assume s = [0, 0, · · · , 0] as a zero vector and s ⊕ 𝛿 = [1, 1, · · · , 1, 0, 0, · · · , 0], where the first 𝑘 entries

are 1 and the remaining 𝑛 − 𝑘 entries are 0. We construct a binary vector z ∈ R (𝑤, 𝑣). Specifically, suppose we flip 𝑖 zeros in the last 𝑛−𝑘 zeros in both s and s⊕𝛿. Then, we flip 𝑤 −𝑖 of the first 𝑘 bits of s and flip the rest 𝑘 −𝑤 +𝑖 bits of the first 𝑘 bits of s⊕𝛿. In order to have z ∈ R (𝑤, 𝑣), we need 𝑘 −𝑤 +𝑖 +𝑖 = 𝑣, i.e., 𝑖 = (𝑤 +𝑣 −𝑘)/2. Therefore, we have the size |R (𝑤, 𝑣)| of the region R (𝑤, 𝑣) as follows:

0,





|R (𝑤,

𝑣)|

=

 0,

if (𝑤 + 𝑣 − 𝑘) mod 2 ≠ 0, if 𝑤 + 𝑣 < 𝑘,

 𝑛−𝑘 𝑘

 

𝑤+𝑣−𝑘

𝑤−𝑣+𝑘 ,

otherwise

2

2

Moreover, for each z ∈ R (𝑤, 𝑣), we have Pr(𝑋 = z) = 𝛽𝑛−𝑤 (1−𝛽)𝑤 and Pr(𝑌 = z) = 𝛽𝑛−𝑣 (1 − 𝛽)𝑣. Therefore, we have:

Pr(𝑋

∈

R (𝑤, 𝑣))

=

𝑛−𝑤
𝛽

(1

−

𝛽)𝑤

·

|R (𝑤, 𝑣)|,

Pr(𝑌

∈

R (𝑤, 𝑣))

=

𝑛−𝑣
𝛽

(1

−

𝛽)𝑣

·

|R (𝑤, 𝑣)|.

Note that R (𝑚) = ∪𝑣−𝑤=𝑚R (𝑤, 𝑣). Therefore, we have:

Pr(𝑋 ∈ R (𝑚))

=Pr(𝑋 ∈ ∪𝑣−𝑤=𝑚 R (𝑤, 𝑣))

=Pr(𝑋 ∈ ∪m𝑗=imn(a𝑛x,𝑛(0+,𝑚𝑚)) R ( 𝑗 − 𝑚, 𝑗 ))

min (𝑛,𝑛+𝑚)

∑︁

=

Pr(𝑋 ∈ R ( 𝑗 − 𝑚, 𝑗))

𝑗 =max ( 0,𝑚)

min (𝑛,𝑛+𝑚)

=

∑︁

𝑛−( 𝑗−𝑚)
𝛽

(1

−

𝛽) 𝑗−𝑚

·

|R ( 𝑗

−

𝑚,

𝑗)|

𝑗 =max ( 0,𝑚)

min (𝑛,𝑛+𝑚)

=

∑︁

𝑛−( 𝑗−𝑚)
𝛽

(1

−

𝛽) 𝑗−𝑚

·

𝑡

(𝑚,

𝑗 ),

𝑗 =max ( 0,𝑚)

where 𝑡 (𝑚, 𝑗) = |R ( 𝑗 − 𝑚, 𝑗)|. Similarly, we have

min (𝑛,𝑛+𝑚)

Pr(𝑌 ∈ R (𝑚)) =

∑︁

𝑛−𝑗
𝛽

(1

−

𝛽)𝑗

·

𝑡 (𝑚,

𝑗).

𝑗 =max ( 0,𝑚)

Acknowledgements. We thank the anonymous reviewers for their constructive comments. This work is supported by the National Science Foundation under Grants No. 1937786 and 1937787 and the Army Research Office under Grant No. W911NF2110182. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funding agencies.

REFERENCES
[1] Aleksandar Bojchevski and Stephan Günnemann. 2019. Adversarial Attacks on Node Embeddings via Graph Poisoning. In ICML.
[2] Aleksandar Bojchevski and Stephan Günnemann. 2019. Certifiable Robustness to Graph Perturbations. In NeurIPS.
[3] Aleksandar Bojchevski, Johannes Klicpera, and Stephan Günnemann. 2020. Effi-
cient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more. In ICML. [4] Rudy R Bunel, Ilker Turkaslan, Philip Torr, Pushmeet Kohli, and Pawan K
Mudigonda. 2018. A unified view of piecewise linear neural network verification. In NeurIPS. [5] Xiaoyu Cao and Neil Zhenqiang Gong. 2017. Mitigating evasion attacks to deep neural networks via region-based classification. In ACSAC. [6] Nicholas Carlini, Guy Katz, Clark Barrett, and David L Dill. 2017. Provably minimally-distorted adversarial examples. arXiv (2017).

[7] Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng
Cui, Wenwu Zhu, and Junzhou Huang. 2020. A restricted black-box adversarial framework towards attacking graph embedding models. In AAAI. [8] Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. 2017. Maximum resilience of artificial neural networks. In ATVA. [9] Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. 2019. Certified adversarial robustness via randomized smoothing. In ICML. [10] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. 2018. Adversarial attack on graph structured data. In ICML. [11] Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, and et al. 2018. Training verified learners with learned verifiers. arXiv (2018). [12] Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, and et al. 2018. A Dual Approach to Scalable Verification of Deep Networks.. In UAI. [13] Ruediger Ehlers. 2017. Formal verification of piece-wise linear feed-forward neural networks. In ATVA. [14] Negin Entezari, Saba A Al-Sayouri, Amirali Darvishzadeh, and Evangelos E
Papalexakis. 2020. All You Need Is Low (Rank) Defending Against Adversarial Attacks on Graphs. In WSDM. [15] Matteo Fischetti and Jason Jo. 2018. Deep neural networks and mixed integer linear optimization. Constraints (2018). [16] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin Vechev. 2018. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In IEEE S & P. [17] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry. In ICML. [18] Neil Zhenqiang Gong, Mario Frank, and Prateek Mittal. 2014. Sybilbelief: A semi-supervised learning approach for structure-based sybil detection. IEEE TIFS (2014).
[19] Neil Zhenqiang Gong and Bin Liu. 2016. You are who you know and how you
behave: Attribute inference attacks via users’ social friends and behaviors. In {USENIX} Security Symposium. [20] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In NIPS. [21] Jinyuan Jia, Xiaoyu Cao, Binghui Wang, and Neil Zhenqiang Gong. 2020. Cer-
tified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing. In ICLR. [22] Jinyuan Jia, Binghui Wang, Xiaoyu Cao, and Neil Zhenqiang Gong. 2020. Certified
Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing. In The Web Conference. [23] Jinyuan Jia, Binghui Wang, Le Zhang, and Neil Zhenqiang Gong. 2017. AttriInfer:
Inferring user attributes in online social networks using markov random fields. In WWW. [24] Hongwei Jin, Zhan Shi, Venkata Jaya Shankar Ashish Peruri, and Xinhua Zhang.
2020. Certified Robustness of Graph Convolution Networks for Graph Classification under Topological Attacks. In NeurIPS. [25] Guy Katz, Clark Barrett, David L Dill, and et al. 2017. Reluplex: An efficient SMT solver for verifying deep neural networks. In CAV. [26] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. In ICLR. [27] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. 2019. Predict then propagate: Graph neural networks meet pagerank. In ICLR. [28] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
Jana. 2019. Certified robustness to adversarial examples with differential privacy. In IEEE S & P. [29] GuangHe Lee, Yang Yuan, Shiyu Chang, and Tommi Jaakkola. 2019. Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers. In NeurIPS. [30] Erich L Lehmann and Joseph P Romano. 2006. Testing statistical hypotheses. Springer Science & Business Media.
[31] Alexander Levine and Soheil Feizi. 2020. Robustness Certificates for Sparse Adversarial Attacks by Randomized Ablation. In AAAI.
[32] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. 2019. Certified Adversarial Robustness with Additive Noise. NeurIPS.
[33] Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. 2018. Towards robust neural networks via random self-ensemble. In ECCV.
[34] Matthew Mirman, Timon Gehr, and Martin Vechev. 2018. Differentiable abstract interpretation for provably robust neural networks. In ICML.
[35] Alan Mislove, Bimal Viswanath, Krishna P Gummadi, and Peter Druschel. 2010.
You are who you know: inferring user profiles in online social networks. In WSDM. [36] Jerzy Neyman and Egon Sharpe Pearson. 1933. IX. On the problem of the most
efficient tests of statistical hypotheses. (1933).
[37] Shashank Pandit, Horng Chau, Samuel Wang, and Christos Faloutsos. 2007. Net-
probe: a fast and scalable system for fraud detection in online auction networks. In WWW. [38] Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann. [39] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018. Certified defenses against adversarial examples. In ICLR.

[40] Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. 2018. Semidefinite relaxations for certifying robustness to adversarial examples. In NeurIPS.
[41] Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Se-
bastien Bubeck, and Greg Yang. 2019. Provably robust deep learning via adversarially trained smoothed classifiers. In NeurIPS. [42] Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker. 2015. Towards Verification of Artificial Neural Networks.. In MBMV. [43] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, and et al. 2008. Collective classification in network data. AI magazine (2008). [44] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Martin Vechev. 2018. Fast and effective robustness certification. In NeurIPS. [45] Yiwei Sun, Suhang Wang, Xianfeng Tang, Tsung-Yu Hsieh, and Vasant Honavar.
2020. Adversarial Attacks on Graph Neural Networks via Node Injections: A Hierarchical Reinforcement Learning Approach. In The Web Conference. [46] Acar Tamersoy, Kevin Roundy, and Duen Horng Chau. 2014. Guilt by association: large scale malware detection by mining file-relation graphs. In KDD. [47] Xianfeng Tang, Yandong Li, Yiwei Sun, Huaxiu Yao, Prasenjit Mitra, and Suhang
Wang. 2020. Transferring Robustness for Graph Neural Network Against Poisoning Attacks. In WSDM. [48] Shuchang Tao, Huawei Shen, Qi Cao, Liang Hou, and Xueqi Cheng. 2021. Adversarial Immunization for Certifiable Robustness on Graphs. In WSDM. [49] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph attention networks. In ICLR. [50] Binghui Wang and Neil Zhenqiang Gong. 2019. Attacking Graph-based Classification via Manipulating the Graph Structure. In CCS. [51] Binghui Wang, Neil Zhenqiang Gong, and Hao Fu. 2017. GANG: Detecting
fraudulent users in online social networks via guilt-by-association on directed graphs. In ICDM. [52] Binghui Wang, Jinyuan Jia, and Neil Zhenqiang Gong. 2019. Graph-based security
and privacy analytics via collective classification with joint weight learning and propagation. In NDSS. [53] Binghui Wang, Jinyuan Jia, Le Zhang, and Neil Zhenqiang Gong. 2018. Structurebased sybil detection in social networks via local rule-based propagation. IEEE TNSE (2018). [54] Binghui Wang, Le Zhang, and Neil Zhenqiang Gong. 2017. SybilSCAR: Sybil detection in online social networks via local rule based propagation. In INFOCOM. [55] Mark Weber, Giacomo Domeniconi, Jie Chen, and et al. 2019. Anti-Money
Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics. In KDD Workshop. [56] Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane
Boning, Inderjit S Dhillon, and Luca Daniel. 2018. Towards fast computation of certified robustness for relu networks. In ICML. [57] Eric Wong and J Zico Kolter. 2018. Provable defenses against adversarial examples via the convex outer adversarial polytope. In ICML. [58] Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. 2018. Scaling provable adversarial defenses. In NeurIPS. [59] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming
Zhu. 2019. Adversarial Examples on Graph Data: Deep Insights into Attack and Defense. In IJCAI. [60] Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong,
and Xue Lin. 2019. Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective. In IJCAI. [61] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How powerful are graph neural networks?. In ICLR. [62] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs with jumping knowledge networks. In ICML. [63] Pinar Yanardag and SVN Vishwanathan. 2015. Deep graph kernels. In KDD. [64] Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar,
Cho-Jui Hsieh, and Liwei Wang. 2020. MACER: Attack-free and Scalable Robust Training via Maximizing Certified Radius. In ICLR. [65] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.
2018. Efficient neural network robustness certification with general activation functions. In NeurIPS. [66] Elena Zheleva and Lise Getoor. 2009. To join or not to join: the illusion of privacy in social networks with mixed public and private user profiles. In WWW. [67] Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2019. Robust Graph Convolutional Networks Against Adversarial Attacks. In KDD. [68] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. 2003. Semi-supervised learning using gaussian fields and harmonic functions. In ICML. [69] Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann. 2018. Adversarial attacks on neural networks for graph data. In KDD. [70] Daniel Zügner and Stephan Günnemann. 2019. Adversarial attacks on graph neural networks via meta learning. In ICLR. [71] Daniel Zügner and Stephan Günnemann. 2019. Certifiable Robustness and Robust Training for Graph Convolutional Networks. In KDD. [72] Daniel Zügner and Stephan Günnemann. 2020. Certifiable Robustness of Graph Convolutional Networks under Structure Perturbations. In KDD.

