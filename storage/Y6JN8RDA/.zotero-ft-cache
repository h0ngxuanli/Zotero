AFLGuard: Byzantine-robust Asynchronous Federated Learning

arXiv:2212.06325v1 [cs.CR] 13 Dec 2022

Minghong Fang
The Ohio State University and Duke University
Neil Zhenqiang Gong
Duke University
ABSTRACT
Federated learning (FL) is an emerging machine learning paradigm, in which clients jointly learn a model with the help of a cloud server. A fundamental challenge of FL is that the clients are often heterogeneous, e.g., they have different computing powers, and thus the clients may send model updates to the server with substantially different delays. Asynchronous FL aims to address this challenge by enabling the server to update the model once any clientâ€™s model update reaches it without waiting for other clientsâ€™ model updates. However, like synchronous FL, asynchronous FL is also vulnerable to poisoning attacks, in which malicious clients manipulate the model via poisoning their local data and/or model updates sent to the server. Byzantine-robust FL aims to defend against poisoning attacks. In particular, Byzantine-robust FL can learn an accurate model even if some clients are malicious and have Byzantine behaviors. However, most existing studies on Byzantine-robust FL focused on synchronous FL, leaving asynchronous FL largely unexplored. In this work, we bridge this gap by proposing AFLGuard, a Byzantine-robust asynchronous FL method. We show that, both theoretically and empirically, AFLGuard is robust against various existing and adaptive poisoning attacks (both untargeted and targeted). Moreover, AFLGuard outperforms existing Byzantine-robust asynchronous FL methods.
CCS CONCEPTS
â€¢ Security and privacy â†’ Systems security.
KEYWORDS
Federated Learning, Poisoning Attacks, Byzantine Robustness
ACM Reference Format: Minghong Fang, Jia Liu, Neil Zhenqiang Gong, and Elizabeth S. Bentley. 2022. AFLGuard: Byzantine-robust Asynchronous Federated Learning. In Annual Computer Security Applications Conference (ACSAC â€™22), December 5â€“9, 2022, Austin, TX, USA. ACM, New York, NY, USA, 15 pages. https: //doi.org/10.1145/3564625.3567991
1 INTRODUCTION
Background and Motivation: Federated learning (FL) [27, 32] is an emerging distributed learning framework, which enables clients
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA Â© 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-9759-9/22/12. . . $15.00 https://doi.org/10.1145/3564625.3567991

Jia Liu
The Ohio State University
Elizabeth S. Bentley
Air Force Research Laboratory
(e.g., smartphone, IoT device, edge device) to jointly train a global model under the coordination of a cloud server. Specifically, the server maintains the global model and each client maintains a local model. In each iteration, the server sends the current global model to the clients; a client trains its local model via fine-tuning the global model using its local data, and the client sends the model update (i.e., the difference between global model and local model) to the server; and the server aggregates the clientsâ€™ model updates and uses them to update the global model.
Most existing FL methods are synchronous [7, 9, 22, 33, 35, 56]. Specifically, in each iteration of a synchronous FL, the server waits for the model updates from a large number of clients before aggregating them to update the global model. However, synchronous FL faces two key challenges. The first challenge is the so-called straggler problem. Specifically, due to clientsâ€™ unpredictable communication latency and/or heterogeneous computing capabilities, some clients (i.e., stragglers) send their model updates to the server much later than others in each iteration, which substantially delays the update of the global model. Simply ignoring the stragglersâ€™ model updates would waste clientsâ€™ computing resources and hurt accuracy of the global model [44]. The second challenge is that synchronous FL is difficult to implement due to the high complexity in maintaining a perfectly synchronized global common clock.
Asynchronous FL aims to address the challenges of synchronous FL. Specifically, in asynchronous FL, the server updates the global model immediately upon receiving a clientâ€™s model update without waiting for other clientsâ€™ model updates. Due to the advantages of asynchronous FL, it has been widely incorporated in deep learning frameworks such as TensorFlow [3] and PyTorch [38], as well as deployed by industries, e.g., Meta [26, 36]. However, like synchronous FL, asynchronous FL is also vulnerable to poisoning attacks [17, 53, 55], in which malicious clients poison their local data and/or model updates to guide the training process to converge to a bad global model. Specifically, in untargeted poisoning attacks [6, 7, 18, 41], the bad global model simply has a large error rate for indiscriminate testing inputs. In targeted poisoning attacks [5, 14, 40, 43], the bad global model predicts attacker-chosen label for attacker-chosen testing inputs, but its predictions for other testing inputs are unaffected. For instance, in backdoor attacks (one type of targeted poisoning attacks) [5, 37, 43, 49, 51], the attackerchosen testing inputs are inputs embedded with a backdoor trigger.
Byzantine-robust asynchronous FL aims to defend against poisoning attacks. However, it is highly challenging to design Byzantine-robust asynchronous FL. To date, most existing Byzantine-robust FL methods (e.g., [7, 9, 12, 33, 35, 56]) are designed for synchronous FL. Compared to synchronous FL, the key challenge in designing Byzantine-robust asynchronous FL stems from the fact that noisy model updates are inevitable. Specifically,

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA

Minghong Fang, Jia Liu, Neil Zhenqiang Gong, and Elizabeth S. Bentley

ğœƒ0
Server Client 1 Download

Compute

Upload

ğœƒ1
Time Idle

ğœƒ0
Server Client 1 Download

ğœƒ1

ğœƒ2

Compute

Upload

ğœƒ3
Time

Client 2 Download

Compute

Upload

Client 2 Download

Compute

Upload

Client 3 Download Compute Upload

Idle

(a) Synchronous FL

Client 3 Download Compute Upload (b) Asynchronous FL

Figure 1: Synchronous vs. asynchronous FL. â€œDownloadâ€ means downloading the global model from the server. â€œComputeâ€ means training a local model. â€œUploadâ€ means sending the model update to the server.

when a client sends its model update calculated based on a global model to the server, other clients may have already sent their model updates calculated based on the same global model to the server and thus the global model could have already been updated several times. As a result, delayed model updates are inevitably noisy with respect to the current global model. This asynchrony makes it difficult to distinguish between poisoned model updates from malicious clients and the â€œnoisyâ€ model updates from benign clients.
Our Work: In this work, we propose AFLGuard, a Byzantine-robust asynchronous FL framework that addresses the aforementioned challenges. In AFLGuard, our key idea to handle the asynchrony complications is to equip the server with a small but clean training dataset, which we call trusted dataset. The server (e.g., Meta, Google) can manually collect the trusted dataset for the learning task. When receiving a model update from a client, the server computes a model update (called server model update) based on its trusted dataset and the current global model. The server accepts the clientâ€™s model update only if it does not deviate far from the server model update with respect to both direction and magnitude. In particular, if the magnitude of the difference vector between the client and server model updates is less than a certain fraction of the magnitude of the server model update, then the server uses the clientâ€™s model update to update the global model. The updated global model is then sent to the client.
Interestingly, we show that this simple intuitive idea of AFLGuard enjoys strong theoretical guarantees. Specifically, under mild assumptions widely adopted by the Byzantine-robust FL community, we prove that the difference between the optimal global model parameters under no malicious clients and the global model parameters learnt by AFLGuard under an arbitrary number of malicious clients can be bounded. We also empirically evaluate AFLGuard and compare it with state-of-the-art Byzantine-robust asynchronous FL methods on a synthetic dataset and five real-world datasets. Our experimental results show that AFLGuard can defend against various existing and adaptive poisoning attacks when a large fraction of clients are malicious. Moreover, AFLGuard substantially outperforms existing Byzantine-robust asynchronous FL methods.
We summarize our main contributions as follows:
â€¢ We propose a Byzantine-robust asynchronous FL framework called AFLGuard to defend against poisoning attacks in asynchronous FL.

â€¢ We theoretically show that AFLGuard is robust against an arbitrary number of malicious clients under mild assumptions commonly adopted by the Byzantine-robust FL community.
â€¢ We conduct extensive experiments to evaluate AFLGuard and compare it with state-of-the-art Byzantine-robust asynchronous FL methods on one synthetic and five real-world datasets.

2 PRELIMINARIES AND RELATED WORK

2.1 Background on Federated Learning

Notations: We use âˆ¥Â·âˆ¥ to denote â„“2-norm. For any natural number ğ‘›, we use [ğ‘›] to denote the set {1, 2, Â· Â· Â· , ğ‘›}.

Setup of Federated Learning (FL): Suppose we have ğ‘› clients.

Client ğ‘– has a local training dataset ğ‘‹ğ‘– , where ğ‘– = 1, 2, Â· Â· Â· , ğ‘›. For

simplicity, we denote by ğ‘‹ =

ğ‘›
ğ‘–=1 ğ‘‹ğ‘– the joint training dataset

of the ğ‘› clients. An FL algorithm aims to solve an optimization

problem, whose objective function is to find an optimal global model ğœ½ âˆ— that minimizes the expected loss ğ¹ (ğœ½ ) as follows:

ğœ½ âˆ— = arg min ğ¹ (ğœ½ ) â‰œ arg min Eğ‘¥âˆ¼D [ğ‘“ (ğœ½, ğ‘¥)] ,

(1)

ğœ½ âˆˆÎ˜

ğœ½ âˆˆÎ˜

where Î˜ âŠ† Rğ‘‘ is the model-parameter space, ğ‘‘ is the dimension

of the model-parameter space, ğ‘“ is a loss function that evaluates

the discrepancy between an output of a global model and the cor-

responding ground truth, the expectation E is taken with respect to the distribution of a training example ğ‘¥ (including both feature vector and label), and D is the training data distribution. In practice, the expectation is often approximated as the average

loss of the training examples in the joint training dataset ğ‘‹ , i.e.,

Eğ‘¥ âˆ¼ D

[ğ‘“ (ğœ½, ğ‘¥)]

â‰ˆ

1 |ğ‘‹ |

ğ‘¥ âˆˆğ‘‹ ğ‘“ (ğœ½, ğ‘¥).

In FL, the clients iteratively learn a global model with the coordi-

nation of a cloud server. In each iteration, synchronous FL waits for

the information from multiple clients before using them to update

the global model, while asynchronous FL updates the global model

once the information from any client reaches it. Fig. 1 illustrates the difference between synchronous FL and asynchronous FL [2].

Synchronous FL: Synchronous FL performs three steps in each iteration. In the first step, the server sends the current global model

to the clients or a selected subset of them. In the second step, a

client trains its local model via fine-tuning the global model using its local training data, and it sends the model update (i.e., the differ-

ence between the global model and the local model) to the server.

AFLGuard: Byzantine-robust Asynchronous Federated Learning
Algorithm 1 AsyncSGD.
Server: 1: Initializes global model ğœ½ 0 âˆˆ Î˜ and sends it to all clients. 2: for ğ‘¡ = 0, 1, 2, Â· Â· Â· ,ğ‘‡ âˆ’ 1 do 3: Upon receiving a model update ğ’ˆğ‘¡âˆ’ğœğ‘– from client ğ‘–, updates
ğ‘–
ğœ½ğ‘¡+1 = ğœ½ğ‘¡ âˆ’ ğœ‚ğ’ˆğ‘¡âˆ’ğœğ‘– .
ğ‘–
4: Sends ğœ½ğ‘¡+1 to client ğ‘–. 5: end for
Client ğ‘–, ğ‘– âˆˆ [ğ‘›]: 6: repeat 7: Receives a global model ğœ½ğ‘¡ from the server. 8: Computes stochastic gradient ğ’ˆğ‘¡ based on ğœ½ğ‘¡ and a random
ğ‘–
mini-batch of its local training data. 9: Sends ğ’ˆğ‘¡ to the server.
ğ‘–
10: until Convergence

When all the selected clients have sent their model updates to the

server, the server aggregates them and uses the aggregated model

update to update the global model in the third step. For instance,

in FedAvg [32], the server computes the weighted average of the

clientsâ€™ model updates and uses it to update the global model in the

third step.

Asynchronous FL: Synchronous FL requires the server to wait

for the model updates from multiple clients before updating the

global model, which is vulnerable to the straggler problem and

delays the training process. In contrast, asynchronous FL updates

the global model upon receiving a model update from any client [13,

15, 26, 36, 46, 52, 54]. Specifically, the server initializes the global

model and sends it to all clients. Each client trains its local model

via fine-tuning the global model based on its local training data,

and sends the model update to the server. Upon receiving a model

update, the server immediately updates the global model and sends

the updated global model back to the client.
Formally, we denote by ğœ½ğ‘¡ the global model in the ğ‘¡th iteration. Moreover, we denote by ğ’ˆğ‘¡ the model update from client ğ‘– that
ğ‘–
is calculated based on the global model ğœ½ğ‘¡ . Suppose in the ğ‘¡th iteration, the server receives a model update ğ’ˆğ‘¡âˆ’ğœğ‘– from client ğ‘– that
ğ‘–
is calculated based on an earlier global model ğœ½ğ‘¡âˆ’ğœğ‘– in an earlier iteration ğ‘¡ âˆ’ ğœğ‘– , where ğœğ‘– is the delay for the model update. The
server updates the global model as follows:

ğœ½ğ‘¡+1 = ğœ½ğ‘¡ âˆ’ ğœ‚ğ’ˆğ‘¡âˆ’ğœğ‘– ,

(2)

ğ‘–

where ğœ‚ is the global learning rate. Asynchronous stochastic gradient descent (AsyncSGD) [58] is the
most popular asynchronous FL method in non-adversarial settings.

In AsyncSGD, a client simply fine-tunes the global model using

one mini-batch of its local training data to obtain a local model. In

other words, a client computes the gradient of the global model with

respect to a random mini-batch of its local training data as the model

update. Formally, ğ’ˆğ‘¡
ğ‘–

=

1 |ğµ |

ğ‘¥ âˆˆğµ â–½ğ‘“ (ğœ½ğ‘¡ , ğ‘¥), where ğµ is a mini-batch

randomly sampled from ğ‘‹ğ‘– . Algorithm 1 shows AsyncSGD, where

ğ‘‡ is the number of iterations. Note that for simplicity, we assume in

all the compared FL methods and our AFLGuard, a client uses such

gradient with respect to a random mini-batch of its local training

data as the model update.

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA
2.2 Byzantine-robust FL
Poisoning Attacks to FL: Poisoning attacks have been intensively studied in traditional ML systems, such as recommender systems [19, 21, 31], crowdsourcing systems [20, 34] and anomaly detectors [39]. Due to its distributed nature, FL is also vulnerable to poisoning attacks [5, 6, 10, 18], in which malicious clients poison the global model via carefully manipulating their local training data and/or model updates. The malicious clients can be fake clients injected into the FL system by an attacker or genuine clients compromised by an attacker. Depending on the attack goal, poisoning attacks can be categorized into untargeted and targeted. In untargeted attacks, a poisoned global model has a large error rate for indiscriminate testing examples, leading to denial-of-service. In targeted attacks, a poisoned global model predicts attacker-chosen labels for attacker-chosen testing inputs, but its predictions for other testing inputs are unaffected.
For instance, label flipping attack [56], Gaussian attack [7], and gradient deviation attack [18] are examples of untargeted attacks. In particular, in the label flipping attack, the malicious clients flip the label ğ‘¦ of a local training example to ğ¶ âˆ’ 1 âˆ’ ğ‘¦, where ğ¶ is the total number of labels and the labels are 0, 1, Â· Â· Â· , ğ¶ âˆ’ 1. In the Gaussian attack, the malicious clients draw their model updates from a Gaussian distribution with mean zero and a large standard deviation instead of computing them based on their local training data. In the gradient deviation attack, the model updates from the malicious clients are manipulated such that the global model update follows the reverse of the gradient direction (i.e., the direction where the global model should move without attacks).
Backdoor attack [5, 51] is a popular targeted attack. For instance, in the backdoor attack in [5], each malicious client replicates some of its local training examples; embeds a trigger (e.g., a patch on the right bottom corner of an image) into each replicated training input; and changes their labels to an attacker-chosen one. A malicious client calculates its model update based on its original local training data and the replicated ones. Moreover, the malicious client scales up the model update by a scaling factor before sending it to the server. The poisoned global model would predict the attackerchosen label for any input embedded with the same trigger, but the predictions for inputs without the trigger are not affected.
Byzantine-Robust Synchronous FL: Byzantine-robust FL aims to defend against poisoning attacks. Most existing Byzantine-robust FL methods focus on synchronous FL [7, 9, 56]. Recall that a synchronous FL method has three steps in each iteration. These Byzantinerobust synchronous FL methods adopt robust aggregation rules in the third step. Roughly speaking, the key idea of a robust aggregation rule is to filter out â€œoutlierâ€ model updates before aggregating them to update the global model. For example, the Krum aggregation rule [7] outputs the model update with the minimal sum of distances to its ğ‘› âˆ’ğ‘š âˆ’ 2 neighbors, where ğ‘› and ğ‘š are the numbers of total and malicious clients, respectively. Since these methods are designed to aggregate model updates from multiple clients, they are not applicable to asynchronous FL, which updates the global model using one model update. Other defenses for synchronous FL include provably secure defenses to prevent poisoning attacks [11] and methods to detect malicious clients [57].

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA
Byzantine-Robust Asynchronous FL: To the best of our knowledge, the works most related to ours are [17, 53, 55]. Specifically, Kardam [17] maintains a Lipschitz coefficient for each client based on its latest model update sent to the server. The server uses a model update from a client to update the global model only if its Lipschitz coefficient is smaller than the median Lipschitz coefficient of all clients. BASGD [55] is a non-conventional asynchronous FL method that uses multiple clientsâ€™ model updates to update the global model. Specifically, the server holds several buffers and maps each clientâ€™s model update into one of them. When all buffers are non-empty, the server computes the average of the model updates in each buffer, takes the median or trimmed-mean of the average model updates, and uses it to update the global model. In Zeno++ [53], the server filters clientsâ€™ model updates based on a trusted dataset. The server computes a server model update based on the trusted dataset. After receiving a model update from any client, the server computes the cosine similarity between the client model update and server model update. If the cosine similarity is positive, then the server normalizes the client model update. Note that FLTrust [9], a synchronous FL method, uses the similar technique as in Zeno++ to filter out malicious information.
Differences between AFLGuard and Zeno++: Both our AFLGuard and Zeno++ use a trusted dataset on the server. However, they use it in different ways. Zeno++ simply treats a clientâ€™s model update as benign if it is positively correlated with the server model update. Due to delays on both client and server sides and the distribution shift between the trusted and clientsâ€™ training data, the serverâ€™s and benign clientsâ€™ model updates may not be positively correlated. In AFLGuard, a clientâ€™s model update is considered benign if it does not deviate substantially from the serverâ€™s model update in both direction and magnitude.
3 PROBLEM FORMULATION
Threat Model: The attacker controls some malicious clients, which could be genuine clients compromised by the attacker or fake clients injected by the attacker. The attacker does not compromise the server. The malicious clients could send arbitrary model updates to the server. The attacker could have different degree of knowledge about the FL system [9, 18], i.e., partial knowledge and full knowledge. In the partial-knowledge setting, the attacker knows the local training data and model updates on the malicious clients. In the full-knowledge scenario, the attacker has full knowledge of the FL system. In particular, the attacker knows the local training data and model updates on all clients, as well as the FL method and its parameter settings. Note that the attacker in the full-knowledge setting is much stronger than that of partial-knowledge setting [18]. Following [9], we use the full-knowledge attack setting to evaluate the security of our defense in the worst case. In other words, our defense is more secure against weaker attacks.
Defense Goals: We aim to design an asynchronous FL method that achieves the following two goals: i) the method should be as accurate as AsyncSGD in non-adversarial settings. In other words, when all clients are benign, our method should learn as an accurate global model as AsyncSGD; and ii) the method should be robust against both existing and adaptive poisoning attacks in adversarial

Minghong Fang, Jia Liu, Neil Zhenqiang Gong, and Elizabeth S. Bentley

(a)

(b)

Figure 2: Illustration of our acceptance criteria. ğ’ˆğ‘¡âˆ’ğœğ‘– and

ğ‘–

ğ’ˆğ‘ ğ‘¡âˆ’ğœğ‘  are client model update and server model update, re-

spectively. (a) the direction of ğ’ˆğ‘¡âˆ’ğœğ‘– deviates substantially

ğ‘–

from

that

of

ğ’ˆğ‘ ğ‘¡âˆ’ğœğ‘  .

(b)

the

magnitude

of

ğ’ˆğ‘¡ âˆ’ğœğ‘–
ğ‘–

deviates

sub-

stantially from that of ğ’ˆğ‘ ğ‘¡âˆ’ğœğ‘  . The server rejects the client

model update in both cases.

settings. Adaptive poisoning attacks refer to attacks that are tailored to the proposed method. Serverâ€™s Capability and Knowledge: We assume the server holds a small clean dataset, which we call trusted dataset. This assumption is reasonable in practice because it is quite affordable for a service provider to collect and verify a small trusted dataset for the learning task. For instance, Google uses FL for the next word prediction in a virtual keyboard application called Gboard [1]; and Google can collect a trusted dataset from its employees. The trusted dataset does not need to follow the same distribution as the joint training dataset ğ‘‹ . As our experimental results will show, once the trusted dataset distribution does not deviate substantially from the joint training data distribution, our method is effective. We acknowledge that the trusted dataset should be clean, and our method may not be robust when the trusted dataset is poisoned.

4 AFLGUARD

Intuitions: The key of our AFLGuard is a criteria to decide whether the server should accept a clientâ€™s model update to update the global

model or not. Ideally, if a model update is from a malicious client

performing poisoning attacks, then the server should not use it to

update the global model. Our key observation is that, in poisoning

attacks, malicious clients often manipulate the directions and/or

the magnitudes of their model updates. Therefore, we consider

both the direction and magnitude of a clientâ€™s model update when

deciding whether it should be accepted to update the global model

or not. Specifically, the server computes a model update (called server model update) based on its own trusted dataset. When a

clientâ€™s model update deviates substantially from the server model

update with respect to direction and/or magnitude, it is rejected.

Acceptance Criteria: Suppose in the ğ‘¡th iteration, the server re-

ceives

a

model

update ğ’ˆğ‘¡âˆ’ğœğ‘–
ğ‘–

from

a

client ğ‘–

âˆˆ

[ğ‘›],

where ğœğ‘–

is

the

delay. Client ğ‘– calculated the model update ğ’ˆğ‘¡âˆ’ğœğ‘– based on the global

ğ‘–

model ğœ½ğ‘¡âˆ’ğœğ‘– , i.e., the server previously sent the global model ğœ½ğ‘¡âˆ’ğœğ‘–

to client ğ‘– in the (ğ‘¡ âˆ’ ğœğ‘– )th iteration. Moreover, in the ğ‘¡th iteration, the server has a model update ğ’ˆğ‘ ğ‘¡âˆ’ğœğ‘  based on its trusted dataset,
where ğœğ‘  is the delay (called server delay) for the server model up-

date. Specifically, the server trains a local model via fine-tuning the global model ğœ½ğ‘¡âˆ’ğœğ‘  using its trusted dataset, and the model update ğ’ˆğ‘ ğ‘¡âˆ’ğœğ‘  is the difference between the global model ğœ½ğ‘¡âˆ’ğœğ‘  and the local

model. We note that we assume the server model update can have

a delay ğœğ‘  , i.e., the server is not required to compute the model

AFLGuard: Byzantine-robust Asynchronous Federated Learning

Algorithm 2 Our AFLGuard.

Server:

1: Initializes global model ğœ½ 0 âˆˆ Î˜ and sends it to all clients.

2: for ğ‘¡ = 0, 1, 2, Â· Â· Â· ,ğ‘‡ âˆ’ 1 do

3: Upon receiving a model update ğ’ˆğ‘¡âˆ’ğœğ‘– from a client ğ‘–, re-
ğ‘–
trieves the server model update ğ’ˆğ‘ ğ‘¡âˆ’ğœğ‘  .

4:

if

ğ’ˆğ‘¡ âˆ’ğœğ‘–
ğ‘–

âˆ’

ğ’ˆğ‘ ğ‘¡ âˆ’ğœğ‘ 

â‰¤ ğœ† ğ’ˆğ‘ ğ‘¡âˆ’ğœğ‘ 

then

5:

Updates the global model ğœ½ğ‘¡+1 = ğœ½ğ‘¡ âˆ’ ğœ‚ğ’ˆğ‘¡âˆ’ğœğ‘– .

ğ‘–

6: else

7:

Does not update the global model, i.e., ğœ½ğ‘¡+1 = ğœ½ğ‘¡ .

8: end if 9: Sends the global model ğœ½ğ‘¡+1 to client ğ‘–.

10: end for

update using the global model ğœ½ğ‘¡ in the ğ‘¡th iteration. Instead, the

server can compute a model update in every ğœğ‘  iterations.

The server accepts ğ’ˆğ‘¡âˆ’ğœğ‘– if i) the direction of ğ’ˆğ‘¡âˆ’ğœğ‘– does not

ğ‘–

ğ‘–

deviate dramatically from that of ğ’ˆğ‘ ğ‘¡âˆ’ğœğ‘  and ii) the magnitude of

ğ’ˆğ‘¡ âˆ’ğœğ‘–
ğ‘–

is

similar

to

that

of

ğ’ˆğ‘ ğ‘¡âˆ’ğœğ‘  .

Formally,

the

server

accepts

ğ’ˆğ‘¡ âˆ’ğœğ‘–
ğ‘–

if the following inequality is satisfied:

ğ’ˆğ‘¡ âˆ’ğœğ‘–
ğ‘–

âˆ’

ğ’ˆğ‘ ğ‘¡ âˆ’ğœğ‘ 

â‰¤ ğœ† ğ’ˆğ‘ ğ‘¡âˆ’ğœğ‘  ,

(3)

where the parameter ğœ† > 0 can be viewed as a control knob: if ğœ† is too small, the server could potentially reject some model updates from benign clients; on the other hand, if ğœ† is too large, the server could falsely accept some model updates from malicious clients. Fig. 2 illustrates our acceptance criteria. Once a clientâ€™s model update is accepted, the server uses it to update the global model based on Eq. (2).
Algorithm of AFLGuard: We summarize our AFLGuard algorithm in Algorithm 2. Note that Algorithm 2 only shows the learning procedure of AFLGuard on the server side. The learning procedure on the client side is the same as that of Algorithm 1 and thus we omit it for brevity. In the ğ‘¡th iteration, the server decides whether to accept a clientâ€™s model update or not based on Eq. (3). If yes, the server uses it to update the global model and sends the updated global model back to the client. Otherwise the server does not update the global model and sends the current global model back to the client.

5 THEORETICAL SECURITY ANALYSIS

We theoretically analyze the security/robustness of AFLGuard. In

particular, we show that the difference between the optimal global model ğœ½ âˆ— under no malicious clients and the global model learnt

by AFLGuard with malicious clients can be bounded under some

assumptions. We note that simple models like regression can satisfy

these assumptions, while more complex models like neural net-

works may not. Therefore, in the next section, we will empirically

evaluate our method on complex neural networks.

For convenience, we define ğ‘½ as the ğ‘‘-dimensional unit vector

space ğ‘½

d=ef {v âˆˆ Rğ‘‘ : âˆ¥vâˆ¥ = 1}, âˆ‡ğ‘“ (ğœ½, ğ‘‹ )

=

1 |ğ‘‹ |

ğ‘¥ âˆˆğ‘‹ âˆ‡ğ‘“ (ğœ½, ğ‘¥),

and ğ‘(ğœ½, ğ‘‹ ) d=ef âˆ‡ğ‘“ (ğœ½, ğ‘‹ ) âˆ’ âˆ‡ğ‘“ (ğœ½ âˆ—, ğ‘‹ ). We use ğ‘‹ğ‘  to denote the

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA

trusted dataset at the server. Next, we first state the assumptions in our theoretical analysis and then describe our theoretical results.

Assumption 1. The expected loss ğ¹ (ğœ½ ) has ğ¿-Lipschitz continuous gradients and is ğœ‡-strongly convex, i.e., âˆ€ğœ½, ğœ½ â€² âˆˆ Î˜, the following inequalities hold:
âˆ‡ğ¹ (ğœ½ ) âˆ’ âˆ‡ğ¹ (ğœ½ â€²) â‰¤ ğ¿ ğœ½ âˆ’ ğœ½ â€² ,

ğ¹ (ğœ½) +

âˆ‡ğ¹ (ğœ½ ), ğœ½ â€² âˆ’ ğœ½

ğœ‡ +

ğœ½â€² âˆ’ğœ½

2 â‰¤ ğ¹ (ğœ½ â€²).

2

Assumption 2. There exist constants ğ›¼1 > 0 and ğœŒ1 > 0 such that for any v âˆˆ ğ‘½ , âŸ¨âˆ‡ğ‘“ (ğœ½ âˆ—, ğ‘‹ ), vâŸ© is sub-exponential. That is, âˆ€ |ğœ‘ | â‰¤
1/ğœŒ1, we have:

sup E

exp ğœ‘

âˆ‡

ğ‘“

(ğœ½

âˆ—
,

ğ‘‹

),

v

â‰¤

ğ›¼ 2ğœ‘ 2 /2
ğ‘’1 .

vâˆˆğ‘½

Assumption 3. There exist constants ğ›¼2 > 0, ğœŒ2 > 0 such that for any v âˆˆ ğ‘½ , ğœ½ âˆˆ Î˜ and ğœ½ â‰  ğœ½ âˆ—. âŸ¨ğ‘(ğœ½, ğ‘‹ ) âˆ’ E [ğ‘(ğœ½, ğ‘‹ )] , vâŸ© /âˆ¥ğœ½ âˆ’ ğœ½ âˆ— âˆ¥
is sub-exponential. That is, âˆ€ |ğœ‘ | â‰¤ 1/ğœŒ2, we have:

sup E exp ğœ‘ âŸ¨ğ‘(ğœ½, ğ‘‹ ) âˆ’E [ğ‘(ğœ½, ğ‘‹ )] , vâŸ©/ ğœ½ âˆ’ ğœ½ âˆ—

â‰¤

ğ›¼ 2ğœ‘ 2 /2
ğ‘’2 .

ğœ½ âˆˆÎ˜,vâˆˆV

Assumption 4. For any ğ›½ âˆˆ (0, 1), there exists a constant ğ» > 0 such that the following inequality holds:

ï£±

ï£´ ï£´ï£²
P sup ï£´ï£´ğœ½,ğœ½ â€² âˆˆÎ˜:ğœ½ â‰ ğœ½ â€²

1

âˆ‘ï¸

âˆ‡ğ‘“

(ğœ½,

ğ‘¥)

âˆ’âˆ‡ğ‘“

(ğœ½

â€²
,

ğ‘¥

)

|ğ‘‹ğ‘ 

|
ğ‘¥

âˆˆğ‘‹ğ‘ 

ï£³

â‰¥ 1 âˆ’ ğ›½/3.

ï£¼

â‰¤ğ»

ğœ½ âˆ’ğœ½ â€²

ï£´ ï£´ï£½

ï£´ ï£´ ï£¾

Assumption 5. Clientsâ€™ local training data are independent and identically distributed (i.i.d.). The trusted dataset held by the server and the overall training data are drawn from the same distribution, and the server delay ğœğ‘  = 0.

Remark. Assumption 1 is satisfied in many learning models (e.g., linear regression and quadratically regularized models). Note that we only assume that the expected loss is strongly-convex, while the empirical loss could still be non-convex. Assumptions 2-3 characterize sub-exponential properties on the gradient vectors. Assumption 2 is a standard assumption in the literature on convergence analysis, while Assumptions 2 and 3 are also widely used in Byzantine-robust FL community (see, e.g., [16, 42, 56]). Assumption 4 is satisfied if the model/loss function is Lipschitz-smooth (e.g., regressions, neural networks). Assumption 5 is a sufficient condition only needed in our theoretical analysis, which characterizes the statistical relations between the serverâ€™s trusted dataset and the overall training data. Note that we only need these assumptions to provide theoretical analysis of our proposed AFLGuard, and these assumptions are commonly used in the machine learning and security communities in order to establish the convergence of the FL methods [9, 16, 56]. In practice, some of these assumptions may not hold, e.g., clientsâ€™ local training data could be non-i.i.d., trusted data held by the server and the overall training data may come from different distributions. In Section 6, we will first use a synthetic dataset that satisfies all assumptions to evaluate the performance of our AFLGuard. Then, we will show that AFLGuard can still effectively defend against poisoning attacks in real-world datasets and complex models when some assumptions are violated. As a concrete example, the following lemma shows that linear regression

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA

Minghong Fang, Jia Liu, Neil Zhenqiang Gong, and Elizabeth S. Bentley

models satisfy Assumptions 1-4 with appropriate parameters, and the proof is shown in Appendix A.1.

Lemma 5.1. Let ğ‘¥ğ‘– = (ğ’–ğ‘–, ğ‘¦ğ‘– ) be the input data and define the loss

function as

ğ‘“ (ğœ½, ğ‘¥ğ‘– )

=

(

âŸ¨ğ’–ğ‘–

,ğœ½ âŸ©âˆ’ğ‘¦ğ‘– 2

)2

.

Suppose

that

ğ‘¦ğ‘–

is

generated by

a linear regression model ğ‘¦ğ‘– = âŸ¨ğ’–ğ‘–, ğœ½ âˆ—âŸ© + ğ‘’ğ‘–, where ğœ½ âˆ— is the unknown

true model, ğ’–ğ‘– âˆ¼ ğ‘ (0, ğ‘° ), ğ‘’ğ‘– âˆ¼ ğ‘ (0, 1) and ğ‘’ğ‘– is independent of

ğ’–ğ‘– . The linear regression model satisfies Assumptions 1-4 with the

following parameters: i) Assumption 1âˆšis satisfiâˆšed with ğ¿ = 1, ğœ‡ = 1; ii) Assumption 2 is satisâˆšfied with ğ›¼1 = 2, ğœŒ1 = 2; iii) Assumption 3 is satisfied with ğ›¼2 = 8, ğœŒ2 = 8; and iv) Assumption 4 is satisfied

with

ğ»

=

2log ( 4/ğ›½ )

+

âˆšï¸ 2ğ‘‘

log(4/ğ›½

)

+ ğ‘‘.

The following theorem shows the security of AFLGuard:

Theorem 1. Suppose Assumptions 1-5 are satisfied. If the global

learning rate in Algorithm 2 satisfies ğœ‚

â‰¤

2 ğœ‡+ğ¿

and each client uses

one mini-batch to calculate the model update, then for any number

of malicious clients, with probability at least 1 âˆ’ ğ›½, we have:

ğœ½ğ‘¡ âˆ’ ğœ½ âˆ— â‰¤ (1 âˆ’ ğ‘)ğ‘¡ ğœ½ 0 âˆ’ ğœ½ âˆ— + 4ğœ‚Î“(ğœ† + 1)/ğ‘,

(4)

âˆšï¸ƒ

where ğ‘ = 1âˆ’ (

1

âˆ’

2ğœ‚ ğœ‡ ğ¿ ğœ‡+ğ¿

+ğœ‚ğ¿ğœ† + 8ğœ‚ Î› (ğœ† + 1) ) ,

Î“

=

ğ›¼

âˆšï¸ 1 2ğ¾1

/|ğ‘‹ğ‘ 

|,

Î›

=

ğ›¼2âˆšï¸2(ğ¾2 + ğ¾3)/|ğ‘‹ğ‘  |, ğ¾1 = ğ‘‘ logâˆš6 + log(3/ğ›½), ğ¾2 = ğ‘‘ log(18ğ‘…/ğ›¼2),

ğ¾3

=

1 2

ğ‘‘

log(

|ğ‘‹ğ‘ 

|/ğ‘‘

)

+ log

6ğ›¼

2
ğœ–

2

|ğ‘‹ğ‘  |

ğœŒ2ğ›¼1ğ›½

, ğ‘… = max {ğ¿, ğ» }, ğœ– > 0 is a

constant, ğ‘‘ is the dimension of ğœ½ , and |ğ‘‹ğ‘  | is the trusted dataset size.

Proof. Please see Appendix A.2.

â–¡

Remark. Our Theorem 1 shows that the convergence of AFLGuard does not require the trusted dataset size to depend on the number of model parameters, the client dataset sizes, and the number of malicious clients. The trusted dataset size affects the convergence neighborhood size (the second term on the right-hand-side of Eq. (4)). The larger the trusted dataset size, the smaller the convergence neighborhood.

6 EMPIRICAL EVALUATION 6.1 Experimental Setup
6.1.1 Compared Methods. We compare our AFLGuard with the following asynchronous methods: 1) AsyncSGD [58]: In AsyncSGD, the server updates the global model according to Algorithm 1 upon receiving a model update from any client. 2) Kardam [17]: In Kardam, the server keeps an empirical Lipschitz coefficient for each client, and filters out potentially malicious model updates based on the Lipschitz filter. 3) BASGD [55]: In BASGD, the server holds several buffers. Upon receiving a model update from any client, the server stores it into one of these buffers according to a mapping table. When all buffers are non-empty, the server computes the average of model updates in each buffer, takes the median of all buffers, and uses it to update the global model. 4) Zeno++ [53]: In Zeno++, the server has a trusted dataset. Upon receiving a clientâ€™s model update, the server computes a server model update based on the trusted dataset. If the cosine similarity between the server model update and the clientâ€™s model update is

positive, then the server normalizes the clientâ€™s model update to have the same magnitude as the server model update and uses the normalized model update to update the global model.

6.1.2 Datasets. We evaluate AFLGuard and the compared methods using one synthetic dataset and five real-world datasets (MNIST, Fashion-MNIST, Human Activity Recognition (HAR), Colorectal Histology MNIST, CIFAR-10). The synthetic dataset is for linear regression, which satisfies the Assumptions 1-4 in Section 5 and is used to validate our theoretical results. Other datasets are used to train complex models, which aim to show the effectiveness of AFLGuard even if the Assumptions 1-4 are not satisfied. The details of these datasets are shown in Appendix A.3 due to limited space.

6.1.3 Poisoning Attacks. We use the following poisoning attacks in our experiments. 1) Label flipping (LF) attack [56]: In the LF attack, the label ğ‘¦ of each training example in the malicious clients is replaced by ğ¶ âˆ’ 1 âˆ’ ğ‘¦, where ğ¶ is the total number of classes. For instance, for the MNIST dataset, digit â€œ1â€ is replaced by digit â€œ8â€. 2) Gaussian (Gauss) attack [7]: In the Gauss attack, each model update from malicious clients is drawn from a zero-mean Gaussian distribution (we set the standard deviation to 200). 3) Gradient derivation (GD) attack [18]: In the GD attack adapted from [18], a malicious client computes a model update based on its local training data and then scales it by a negative constant (âˆ’10 in our experiments) before sending it to the server. 4) Backdoor (BD) attack [5, 9, 23]: BD attack is a targeted poisoning attack. We use the same strategy in [23] to embed the trigger in MNIST, Fashion-MNIST and Colorectal Histology MNIST datasets. Following [9], the target label is set to â€œWALKING UPSTAIRSâ€ and the trigger is generated by setting every 20th feature to 0 for the HAR dataset. For the CIFAR-10 dataset, the target label is set to â€œbirdâ€ and we use the same pattern trigger as suggested in [5]. 5) Adaptive (Adapt) attack: In [18], a general adaptive attack framework is proposed to attack FL with any aggregation rule. We apply this attack framework to construct an adaptive attack to our AFLGuard. In particular, the attack framework is designed for synchronized FL, in which the server aggregates model updates from multiple clients to update the global model. The key idea is to craft model updates at the malicious clients such that the aggregated model update deviates substantially from the beforeattack one. To apply this general attack framework to AFLGuard, we assume that the server accepts or rejects a clientâ€™s model update based on AFLGuard and computes the average of the accepted model updates. Then, we craft the model updates on the malicious clients based on the attack framework.

6.1.4 Evaluation Metrics. For the synthetic dataset, we use the

following two evaluation metrics since it is a regression prob-

lem: i) Mean Squared Error (MSE): MSE is computed as MSE =

1 ğ‘ğ‘¡

ğ‘ğ‘¡ ğ‘– =1

(ğ‘¦Ë†ğ‘–

âˆ’ ğ‘¦ğ‘– )2,

where

ğ‘¦Ë†ğ‘–

is

the

predicted

value,

ğ‘¦ğ‘–

is

the

true

value, and ğ‘ğ‘¡ is the number of testing examples; ii) Model Estima-

tion Error (MEE): MEE is computed as MEE = âˆ¥ğœ½Ë† âˆ’ ğœ½ âˆ— âˆ¥2, where ğœ½Ë† is

the learnt model and ğœ½ âˆ— is the true model. MEE is commonly used

in measuring the performance of linear regression [24, 45]. The five

real-world datasets represent classification tasks, and we consider

AFLGuard: Byzantine-robust Asynchronous Federated Learning

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA

Table 1: MSE and MEE of different defenses under different attacks on synthetic dataset. The results are in the form of â€œMSE / MEEâ€. â€œ> 1000â€ means the value is larger than 1000.

No attack LF attack Gauss attack GD attack Adapt attack

AsyncSGD 0.03 / 0.18 21.05 / 25.75 0.78 / 4.82 â€œ> 1000â€ / â€œ> 1000â€ â€œ> 1000â€ / â€œ> 1000â€

Kardam 0.03 / 0.18 0.04 / 0.60 0.03 / 0.36 30.14 / 30.65 â€œ> 1000â€ / â€œ> 1000â€

BASGD 0.09 / 1.43 16.71 / 22.70 0.85 / 5.32 â€œ> 1000â€ / â€œ> 1000â€ â€œ> 1000â€ / â€œ> 1000â€

Zeno++ 0.03 / 0.40 0.03 / 0.40 0.03 / 0.40 0.03 / 0.40 0.03 / 0.42

AFLGuard 0.03 / 0.18 0.03 / 0.18 0.03 / 0.18 0.03 / 0.18 0.03 / 0.18

Table 2: Test error rates and attack success rates of different defenses under different attacks on real-world datasets. The results of BD attack are in the form of â€œtest error rate / attack success rateâ€.

(a) MNIST

AsyncSGD Kardam BASGD Zeno++ AFLGuard

No attack 0.05

0.12

0.19

0.08

0.06

LF attack 0.09

0.15

0.26

0.09

0.07

Gauss attack 0.91

0.39

0.27

0.09

0.07

GD attack 0.90

0.90

0.89

0.09

0.07

BD attack 0.90 / 1.00 0.91 / 1.00 0.91 / 1.00 0.09 / 0.01 0.07 / 0.01

Adapt attack 0.91

0.91

0.90

0.10

0.07

(b) Fashion-MNIST

AsyncSGD Kardam BASGD Zeno++ AFLGuard

No attack 0.15

0.29

0.24

0.26

0.17

LF attack 0.19

0.29

0.24

0.29

0.21

Gauss attack 0.90

0.29

0.35

0.28

0.19

GD attack 0.90

0.90

0.90

0.29

0.21

BD attack 0.90 / 1.00 0.90 / 1.00 0.90 / 1.00 0.29 / 0.05 0.20 / 0.04

Adapt attack 0.90

0.90

0.90

0.29

0.21

(c) HAR

AsyncSGD Kardam BASGD Zeno++ AFLGuard

No attack 0.05

0.06

0.07

0.06

0.05

LF attack 0.19

0.22

0.08

0.08

0.05

Gauss attack 0.30

0.23

0.24

0.07

0.05

GD attack 0.83

0.48

0.67

0.08

0.05

BD attack 0.18 / 0.47 0.17 / 0.02 0.41 / 0.28 0.07 / 0.01 0.05 / 0.01

Adapt attack 0.93

0.52

0.90

0.08

0.05

(d) Colorectal Histology MNIST

AsyncSGD Kardam BASGD Zeno++ AFLGuard

No attack 0.21

0.28

0.29

0.31

0.22

LF attack 0.29

0.37

0.40

0.39

0.23

Gauss attack 0.65

0.44

0.61

0.43

0.22

GD attack 0.87

0.68

0.87

0.39

0.32

BD attack 0.75 / 0.84 0.67 / 0.02 0.85 / 0.84 0.44 / 0.02 0.27 / 0.02

Adapt attack 0.88

0.88

0.88

0.64

0.33

(e) CIFAR-10

AsyncSGD Kardam BASGD Zeno++ AFLGuard

No attack 0.26

0.29

0.47

0.41

0.26

LF attack 0.40

0.52

0.54

0.53

0.34

Gauss attack 0.88

0.63

0.81

0.52

0.33

GD attack 0.90

0.90

0.90

0.60

0.30

BD attack 0.76 / 0.99 0.82 / 1.00 0.74 / 0.98 0.49 / 0.06 0.29 / 0.01

Adapt attack 0.90

0.90

0.90

0.82

0.36

the following two evaluation metrics: 1) test error rate, which is the fraction of clean testing examples that are misclassified; and 2) attack success rate, which is the fraction of trigger-embedded testing inputs that are predicted as the attacker-chosen target label. Note that attack success rate is only applicable for targeted poisoning attack (i.e., BD attack in our experiments). The smaller the error (MSE, MEE, or test error rate) and attack success rate, the better the defense. Note that we do not consider targeted poisoning attacks on synthetic dataset since there are no such attacks designed for linear regression.
6.1.5 Parameter Setting. We assume 100 clients (ğ‘› = 100) for synthetic, MNIST, and Fashion-MNIST datasets, and 40 clients (ğ‘› = 40) for Colorectal Histology MNIST and CIFAR-10 datasets. The HAR dataset is collected from smartphones of 30 real-world users, and each user is considered as a client. Thus, there are 30 clients (ğ‘› = 30) in total for HAR. By default, we assume 20% of the clients are malicious. We train a convolutional neural network (CNN) on MNIST and Fashion-MNIST datasets, and its architecture is shown in Table 4 in Appendix. We train a logistic regression classifier on HAR dataset. We train a ResNet-20 [25] model for Colorectal Histology MNIST and CIFAR-10 datasets. We set 2,000, 2,000, 6,000, 1,000, 20,000 and 20,000 iterations for synthetic, MNIST, Fashion-MNIST, HAR, Colorectal Histology MNIST and CIFAR-10 datasets, respectively. The batch sizes for the six datasets are 16, 32, 64, 32, 32 and 64, respectively. The learning rates are set to 1/1600, 1/320 for synthetic and HAR datasets, respectively; and are set to 1/3200 for the other four datasets. We use different parameters for different datasets because they have different data statistics. In the synthetic dataset, we assume the clientsâ€™ local training data are i.i.d. However, the local training data non-i.i.d. across clients in the five real-world datasets. In particular, we use the approach in [18] to simulate the non-i.i.d. setting. The non-i.i.d. degree is set to 0.5 for MNIST, Fashion-MNIST, Colorectal Histology MNIST, and CIFAR-10 datasets. Note that each user is a client in HAR dataset, and thus the clientsâ€™ local training data are already heterogeneous for HAR.
In AFLGuard, the trusted dataset size is set to 100 for all six datasets. By default, for the synthetic data, we assume that the trusted dataset held by the server and the overall training data are generated from the same distribution. For the real-world datasets, we do not make this assumption. We will empirically show that our method works well even if the distribution of trusted data deviates from that of the overall training data, i.e., there exists a distribution shift (DS) between these two datasets. The larger the DS, the larger the deviation between the trusted and overall training datasets. In our experiments, we simulate DS in the following way: a fraction of samples in the trusted dataset are drawn from one particular

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA

Minghong Fang, Jia Liu, Neil Zhenqiang Gong, and Elizabeth S. Bentley

Test error rate

AsyncSGD w/o attacks

AsyncSGD

Kardam

BASGD

Zeno++

AFLGuard

1.0

1.0

1.0

Test error rate

Attack success rate Test error rate

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0.0F0ractio1n0 of m20alicio3u0s clie4n0ts (%45) 0.0F0ractio1n0 of m20alicio3u0s clie4n0ts (%45) 0.0F0ractio1n0 of m20alicio3u0s clie4n0ts (%45)

(a) LF attack

(b) Gauss attack

(c) GD attack

1.0

1.0

1.0

Test error rate

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0.0F0ractio1n0 of m20alicio3u0s clie4n0ts (%45) 0.0F0ractio1n0 of m20alicio3u0s clie4n0ts (%45) 0.0F0ractio1n0 of m20alicio3u0s clie4n0ts (%45)

(d) BD attack

(e) BD attack

(f) Adapt attack

Test error rate

Figure 3: Test error rates and attack success rates of different defenses under different attacks with different fraction of malicious clients on MNIST dataset.

class (the first class in our experiments) of the overall training data, and the remaining samples in the trusted dataset are drawn from the remaining classes of the overall training data uniformly at random. We use this fraction value as a proxy for DS. Note that when the trusted and overall training datasets are drawn from the same distribution, DS is equal to 1/ğ¶, where ğ¶ is the total number of classes. By default, we set DS to 0.5 for the five real-world datasets.
In our experiments, we use a separate validation dataset to tune the parameter ğœ† in AFLGuard. Note that this validation dataset is different from the trusted dataset held by the server. We use the validation dataset to tune the hyperparameter of AFLGuard, while the server in AFLGuard uses the trusted dataset to filter out potential malicious information. The size of the validation dataset is 200. The validation data and the overall training data (the union of the local training data of all clients) are from the same distribution. For example, there are 10 classes in MNIST dataset. We sample 20 training examples from each class of the overall training data uniformly at random. After fine tuning the parameter, we set ğœ† = 1.5 for synthetic, MNIST, and HAR datasets, and ğœ† = 1.8 for the other three datasets. The server updates ğ’ˆğ‘ ğ‘¡âˆ’ğœğ‘  every 10 (ğœğ‘  = 10) iterations by default. We use the approach in [53] to simulate asynchrony. We sample client delay from the interval [0, ğœmax] uniformly at random, where ğœmax is the maximum client delay. We set ğœmax = 10 by default.
6.2 Experimental Results
AFLGuard is Effective: We first show results on the linear regression model for synthetic dataset, which satisfies Assumptions 1-4 in Section 5 to support our theoretical results. The MSE and MEE of different methods under different attacks on synthetic dataset are shown in Table 1. We observe that AFLGuard is robust in both non-adversarial and adversarial settings. In particular, the MSE and MEE of AFLGuard under various attacks are the same as those of AsyncSGD without attacks. Moreover, we also observe that AFLGuard outperforms the compared methods. For instance, the MSE and MEE of BASGD are both larger than 1,000 under the GD and Adapt attacks.

Next, we show results on the five real-world datasets. The test error rates and attack success rates of different methods under different attacks are shown in Table 2. â€œNo attackâ€ in Table 2 represents the test error rate without any attacks. For the untargeted poisoning attacks (LF attack, Gauss attack, GD attack, and Adapt attack), the results are the test error rates; and for the targeted poisoning attacks (BD attack), the results are in the form of â€œtest error rate / attack success rateâ€. We note that only using the trusted data held by the server to update the global model can not achieve satisfactory accuracy. For instance, the test error rate is 0.21 when we only use the trusted data of the server to update the global model on the MNIST dataset. We also remark that our asynchronous AFLGuard algorithm achieves a performance similar to its synchronous counterpart. For instance, on MNIST, the test error rates of synchronous AFLGuard under LF, Gauss, and GD attacks are all 0.05.
First, we observe that AFLGuard is effective in non-adversarial settings. When there are no malicious clients, AFLGuard has similar test error rate as AsyncSGD. For instance, on MNIST, the test error rates without attacks are respectively 0.05 and 0.06 for AsyncSGD and AFLGuard, while the test error rates are respectively 0.12 and 0.19 for Kardam and BASGD. Second, AFLGuard is robust against various poisoning attacks and outperforms all baselines. For instance, the test error rate of Kardam increases to 0.90 under the GD attack on the MNIST and Fashion-MNIST datasets, while the test error rates are 0.07 and 0.21 for AFLGuard under the same setting. Likewise, the attack success rates of AFLGuard are at most 0.04 for all real-world datasets, while the attack success rates of AsyncSGD, Kardam, and BASGD are high. Note that in Table 1, we use synthetic data that satisfies Lemma 5.1 to evaluate the performance of our AFLGuard. Since the variance of the synthetic data is small, Zeno++ and AFLGuard have similar MSE and MEE. However, Table 2 shows that, for real-world datasets, AFLGuard significantly outperforms Zeno++.
Impact of the Fraction of Malicious Clients: Fig. 3 illustrates the test error rates and attack success rates of different methods

AFLGuard: Byzantine-robust Asynchronous Federated Learning

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA

Test error rate

AsyncSGD w/o attacks 1.0 0.8 0.6 0.4 0.2 0.05 10 1C5lien2t0dela2y5 30
(a) LF attack 1.0 0.8 0.6 0.4 0.2 0.05 10 1C5lien2t0dela2y5 30
(d) BD attack

Attack success rate Test error rate

AsyncSGD 1.0 0.8 0.6 0.4 0.2
50 0.05 1.0 0.8 0.6 0.4 0.2
50 0.05

Kardam

BASGD

1.0

Test error rate

0.8

0.6

0.4

0.2

10 1C5lien2t0dela2y5 30 50 0.05

(b) Gauss attack

1.0

Test error rate

0.8

0.6

0.4

0.2

10 1C5lien2t0dela2y5 30 50 0.05

(e) BD attack

Zeno++

AFLGuard

10 1C5lien2t0dela2y5 30 50 (c) GD attack

10 1C5lien2t0dela2y5 30 50 (f) Adapt attack

Test error rate

Figure 4: Test error rates and attack success rates of different defenses under different attacks with different client delays on

MNIST dataset.

Test error rate

AsyncSGD w/o attacks

Zeno++

1.0

1.0

Attack success rate Test error rate

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.010 20 (S3ae0)rLvFe5r0adtetal8ac0yk 100 200 0.010 20(b)S3Ge0ravue5sr0sdaelt8at0ayck100 200

1.0

1.0

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.010 20 (S3de0)rBvDe5r0adtetla8ac0yk 100 200 0.010 20 (S3ee0)rBvDe5r0adtetla8ac0yk 100 200

Test error rate

Test error rate

AFLGuard 1.0 0.8 0.6 0.4 0.2 0.010 20 (S3c)e0rGvDe5r0adtetla8ac0yk 100 200 1.0 0.8 0.6 0.4 0.2 0.010 20 (f)S3Ae0drvae5pr0tdaetl8taa0yck100 200

Test error rate

Figure 5: Test error rates and attack success rates of Zeno++ and AFLGuard under different attacks with different server delays

on MNIST dataset.

Test error rate

1.0 0.8 0.6

AsyncSGD w/o attacks LF attack Gauss attack

0.4 0.2

GD attack BD attack Adapt attack

0.00.5 1.0 1.2 1.5 2.0 3.0 5.0

(a) Test error rate

Attack success rate

1.0 0.8 0.6

AsyncSGD w/o attacks BD attack

0.4

0.2

0.00.5 1.0 1.2 1.5 2.0 3.0 5.0

(b) Attack success rate

Test error rate Attack success rate

1.0 0.8 0.6

AsyncSGD w/o attacks LF attack Gauss attack

0.4 0.2

GD attack BD attack Adapt attack

0.050 S1i(z0ae0)oTf1et5srut0setre2rd0o0rdarat3at0se0et 400

1.0 0.8 0.6

AsyncSGD w/o attacks BD attack

0.4

0.2

0.050 (bS1)iz0Ae0totaf1ct5rku0sstue2cd0c0edasst3a0rsa0ette400

Figure 6: Test error rates and attack success rates of AFLGuard under different attacks with different ğœ† on MNIST dataset.
under different attacks on the MNIST dataset, when the fraction of malicious clients increases from 0 to 45%. Note that Fig. 3(e) shows the attack success rates of different defenses under BD attack, while other figures are the test error rates of different defenses under untargeted and targeted poisoning attacks. We observe that

Figure 7: Test error rates and attack success rates of AFLGuard under different attacks with different size of trusted dataset on MNIST dataset.
AFLGuard achieves a test error rate similar to that of AsyncSGD without attacks, when 45% of clients are malicious. This shows that AFLGuard is robust against a large fraction of malicious clients. Impact of the Number of Clients: Fig. 8 in Appendix shows the results of different defenses under different attacks, when the total

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA

Minghong Fang, Jia Liu, Neil Zhenqiang Gong, and Elizabeth S. Bentley

Table 3: Test error rates and attack success rates of Zeno++ and AFLGuard under different attacks with different distribution shifts (DSs) between the trusted data and overall training data on MNIST dataset. The results of BD attack are in the form of â€œtest error rate / attack success rateâ€.

DS Attack No attack LF attack Gauss attack GD attack BD attack Adapt attack

0.1

Zeno++ AFLGuard

0.05

0.05

0.07

0.05

0.07

0.05

0.07

0.06

0.06 / 0.01 0.05 / 0.01

0.07

0.06

0.5

Zeno++ AFLGuard

0.08

0.06

0.09

0.07

0.09

0.07

0.09

0.07

0.09 / 0.01 0.07 / 0.01

0.10

0.07

0.6

Zeno++ AFLGuard

0.10

0.06

0.12

0.07

0.12

0.07

0.12

0.07

0.11 / 0.01 0.07 / 0.01

0.12

0.08

0.8

Zeno++ AFLGuard

0.55

0.06

0.86

0.07

0.59

0.07

0.78

0.08

0.55 / 0.03 0.08 / 0.01

0.88

0.10

1.0

Zeno++ AFLGuard

0.88

0.11

0.89

0.11

0.89

0.12

0.89

0.12

0.90 / 0.01 0.11 / 0.01

0.90

0.12

number of clients ğ‘› varies from 50 to 500. The fraction of malicious clients is set to 20%. We observe that our AFLGuard can effectively defend against various poisoning attacks for different number of clients. In particular, AFLGuard under attacks achieves test error rates similar to AsyncSGD without attack.
Impact of the Client Delay: A challenge and key feature in asynchronous FL is the delayed client model updates. In this experiment, we investigate the impact of the maximum client delays ğœmax on the test error rate of different defenses under different attacks on the MNIST dataset, where the server delay is set to 10. The results are shown in Fig. 4. We observe that AFLGuard is insensitive to the delays on the client side, and the test error rates remain almost unchanged when the client delay varies from 5 to 50. However, Kardam and BASGD are highly sensitive to client delays. For example, under the Gauss attack, the test error rate of Kardam increases from 0.14 to 0.39 when the client delay increases from 5 to 10. Moreover, under the GD and Adapt attacks, the test error rates of Kardam and BASGD are both 0.90 when the client delay is only 5.
Impact of the Server Delay: In both Zeno++ and our AFLGuard, the server uses server model update. In this experiment, we investigate the impact of server delays on the performance of Zeno++ and AFLGuard under different attacks, where the maximum client delay is set to 10. The results are shown in Fig. 5. We observe that AFLGuard can effectively defend against various poisoning attacks with different server delays. AFLGuard under attacks has test error rates similar to those of AsyncSGD under no attacks when the server delay ranges from 10 to 200. However, Zeno++ is highly sensitive to server delays. For instance, Zeno++ can only resist the Adapt attack up to 80 server delays.
Impact of ğœ†: Fig. 6 shows the test error rates and attack success rates of AFLGuard under different attacks with different ğœ† values on the MNIST dataset. We observe that if ğœ† is too small (e.g., ğœ† = 0.5), the test error rate of AFLGuard is large since the server rejects many benign model updates. When ğœ† is large (e.g., ğœ† = 5.0), the test error rates of AFLGuard under the GD and Adapt attacks are large. This is because the server falsely accepts some model updates from malicious clients.
Impact of the Trusted Dataset: The trusted dataset can be characterized by its size and distribution. Therefore, we explore the impact of both its size and distribution. Fig. 7 shows the results of AFLGuard under different attacks on the MNIST dataset, when the size of the trusted dataset increases from 50 to 400 (other parameters are set to their default values). We find that AFLGuard

only requires a small trusted dataset (e.g., 100 examples) to defend against different attacks.
Table 3 shows the results of Zeno++ and AFLGuard under different attacks when the DS value between the trusted data and overall training data varies on the MNIST dataset. The results on the other four real-world datasets are shown in Table 5 in Appendix. Note that, for the synthetic data, we assume that the trusted data and the overall training data are generated from the same distribution. Thus, there is no need to study the impact of DS on synthetic data. First, we observe that AFLGuard outperforms Zeno++ across different DS values in most cases, especially when DS is large. This is because Zeno++ classifies a clientâ€™s model update as benign if it is not negatively correlated with the (delayed) server model update. However, when the trusted dataset deviates substantially from the overall training dataset, it is very likely that the server model update and the model updates from benign clients are not positively correlated. Second, AFLGuard outperforms Zeno++ even if the trusted data has the same distribution as that of overall training data (corresponding to DS being 0.1 for MNIST, Fashion-MNIST and CIFAR-10 datasets, 0.167 for HAR dataset, and 0.125 for Colorectal Histology MNIST dataset). Third, AFLGuard can tolerate a large DS value, which means that AFLGuard does not require the trusted dataset to have similar distribution with the overall training data.
7 CONCLUSION AND FUTURE WORK
In this paper, we propose a Byzantine-robust asynchronous FL framework called AFLGuard to defend against poisoning attacks in asynchronous FL. In AFLGuard, the server holds a small and clean trusted dataset to assist the filtering of model updates from malicious clients. We theoretically analyze the security guarantees of AFLGuard. We extensively evaluate AFLGuard against state-ofthe-art and adaptive poisoning attacks on one synthetic and five real-world datasets. Our results show that AFLGuard effectively mitigates poisoning attacks and outperforms existing Byzantinerobust asynchronous FL methods. One interesting future work is to investigate the cases where the server has no knowledge of the training data domain.
ACKNOWLEDGMENTS
We thank the anonymous reviewers and shepherd Briland Hitaj for their constructive comments. This work was supported by NSF grant CAREER CNS-2110259, CNS-2112471, CNS-2102233, CNS2131859, CNS-2112562, CNS-2125977 and CCF-2110252, as well as ARO grant No. W911NF2110182.

AFLGuard: Byzantine-robust Asynchronous Federated Learning
REFERENCES
[1] [n.d.]. Federated Learning: Collaborative Machine Learning without Centralized Training Data. https://ai.googleblog.com/2017/04/federated-learningcollaborative.html
[2] [n.d.]. Making federated learning faster and more scalable: A new asynchronous method. https://ai.facebook.com/blog/asynchronous-federated-learning/
[3] MartÃ­n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.
2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016). [4] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra Perez, and Jorge Luis
Reyes Ortiz. 2013. A public domain dataset for human activity recognition using smartphones. In ESANN. [5] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. 2020. How to backdoor federated learning. In AISTATS. [6] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. 2019. Analyzing federated learning through an adversarial lens. In ICML. [7] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.
2017. Machine learning with adversaries: Byzantine tolerant gradient descent. In NeurIPS. [8] SÃ©bastien Bubeck. 2014. Convex optimization: Algorithms and complexity. arXiv preprint arXiv:1405.4980 (2014). [9] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. 2021. FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping. In NDSS. [10] Xiaoyu Cao and Neil Zhenqiang Gong. 2022. MPAF: Model Poisoning Attacks to Federated Learning based on Fake Clients. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3396â€“3404. [11] Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. 2021. Provably Secure Federated Learning against Malicious Clients. In AAAI. [12] Xinyang Cao and Lifeng Lai. 2019. Distributed gradient descent algorithm robust to an arbitrary number of byzantine attackers. In IEEE Transactions on Signal Processing. [13] Tianyi Chen, Xiao Jin, Yuejiao Sun, and Wotao Yin. 2020. Vafl: a method of vertical asynchronous federated learning. arXiv preprint arXiv:2007.06081 (2020). [14] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526 (2017). [15] Yujing Chen, Yue Ning, Martin Slawski, and Huzefa Rangwala. 2020. Asynchronous online federated learning for edge devices with non-iid data. In Big Data. [16] Yudong Chen, Lili Su, and Jiaming Xu. 2017. Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent. In POMACS. [17] Georgios Damaskinos, Rachid Guerraoui, Rhicheek Patra, Mahsa Taziki, et al. 2018. Asynchronous Byzantine machine learning (the case of SGD). In ICML. [18] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020. Local model poisoning attacks to Byzantine-robust federated learning. In 29th {USENIX} Security Symposium ({USENIX} Security 20). 1605â€“1622. [19] Minghong Fang, Neil Zhenqiang Gong, and Jia Liu. 2020. Influence function based data poisoning attacks to top-n recommender systems. In Proceedings of The Web Conference. [20] Minghong Fang, Minghao Sun, Qi Li, Neil Zhenqiang Gong, Jin Tian, and Jia
Liu. 2021. Data poisoning attacks and defenses to crowdsourcing systems. In Proceedings of The Web Conference. [21] Minghong Fang, Guolei Yang, Neil Zhenqiang Gong, and Jia Liu. 2018. Poisoning attacks to graph-based recommender systems. In ACSAC. [22] Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. 2020. The limitations of federated learning in sybil settings. In 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020). 301â€“316. [23] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733 (2017). [24] Nirupam Gupta and Nitin H Vaidya. 2019. Byzantine fault tolerant distributed linear regression. arXiv preprint arXiv:1903.08752 (2019). [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR. [26] Dzmitry Huba, John Nguyen, Kshitiz Malik, Ruiyu Zhu, Mike Rabbat, Ashkan
Yousefpour, Carole-Jean Wu, Hongyuan Zhan, Pavel Ustinov, Harish Srinivas, et al. 2022. Papaya: Practical, private, and scalable federated learning. Proceedings of Machine Learning and Systems 4 (2022), 814â€“832. [27] Peter Kairouz, H Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi Ben-
nis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al. 2019. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977 (2019). [28] Jakob Nikolas Kather, Cleo-Aron Weis, Francesco Bianconi, Susanne M Melchers,
Lothar R Schad, Timo Gaiser, Alexander Marx, and Frank Gerrit ZÃ¶llner. 2016. Multi-class texture analysis in colorectal cancer histology. Scientific reports 6 (2016), 27988.

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA
[29] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features
from tiny images. (2009).
[30] Yann LeCun, Corinna Cortes, and CJ Burges. 1998. MNIST handwritten digit database. Available: http://yann. lecun. com/exdb/mnist (1998).
[31] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. 2016. Data poisoning attacks on factorization-based collaborative filtering. In NeurIPS.
[32] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep networks from decentralized data. In AISTATS. [33] El Mahdi El Mhamdi, Rachid Guerraoui, and SÃ©bastien Rouault. 2018. The hidden vulnerability of distributed learning in byzantium. In ICML. [34] Chenglin Miao, Qi Li, Lu Su, Mengdi Huai, Wenjun Jiang, and Jing Gao. 2018.
Attack under disguise: An intelligent data poisoning attack mechanism in crowdsourcing. In Proceedings of The Web Conference. [35] Luis MuÃ±oz-GonzÃ¡lez, Kenneth T Co, and Emil C Lupu. 2019. Byzantine-robust federated machine learning through adaptive model averaging. arXiv preprint arXiv:1909.05125 (2019). [36] John Nguyen, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Mike Rab-
bat, Mani Malek, and Dzmitry Huba. 2022. Federated learning with buffered asynchronous aggregation. In AISTATS. [37] Thien Duc Nguyen, Phillip Rieger, Huili Chen, Hossein Yalame, Helen MÃ¶llering,
Hossein Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Shaza Zeitouni, et al. 2022. FLAME: Taming Backdoors in Federated Learning. In USENIX Security Symposium. [38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS. [39] Benjamin IP Rubinstein, Blaine Nelson, Ling Huang, Anthony D Joseph, Shing-
hon Lau, Satish Rao, Nina Taft, and J Doug Tygar. 2009. Antidote: understanding and defending against poisoning of anomaly detectors. In IMC. [40] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
Tudor Dumitras, and Tom Goldstein. 2018. Poison frogs! targeted clean-label poisoning attacks on neural networks. In NeurIPS. [41] Virat Shejwalkar and Amir Houmansadr. 2021. Manipulating the Byzantine:
Optimizing Model Poisoning Attacks and Defenses for Federated Learning. In NDSS. [42] Lili Su and Jiaming Xu. 2019. Securing distributed gradient descent in high dimensional statistical learning. In POMACS. [43] Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. 2019. Can you really backdoor federated learning? arXiv preprint arXiv:1911.07963 (2019).
[44] Rashish Tandon, Qi Lei, Alexandros G Dimakis, and Nikos Karampatziakis. 2017. Gradient coding: Avoiding stragglers in distributed learning. In ICML.
[45] Berkay Turan, Cesar A Uribe, Hoi-To Wai, and Mahnoosh Alizadeh. 2021. Robust Distributed Optimization With Randomly Corrupted Gradients. arXiv preprint arXiv:2106.14956 (2021).
[46] Marten van Dijk, Nhuong V Nguyen, Toan N Nguyen, Lam M Nguyen, Quoc Tran-
Dinh, and Phuong Ha Nguyen. 2020. Asynchronous Federated Learning with
Reduced Number of Rounds and with Differential Privacy from Less Aggregated Gaussian Noise. arXiv preprint arXiv:2007.09208 (2020). [47] Roman Vershynin. 2010. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027 (2010). [48] Martin J Wainwright. 2019. High-dimensional statistics: A non-asymptotic view-
point, Vol. 48. Cambridge University Press.
[49] Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh
Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. 2020. Attack of the tails: Yes, you really can backdoor federated learning. In NeurIPS. [50] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-MNIST:
a Novel Image Dataset for Benchmarking Machine Learning Algorithms.
arXiv:cs.LG/cs.LG/1708.07747
[51] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. 2019. Dba: Distributed backdoor attacks against federated learning. In ICLR.
[52] Cong Xie, Sanmi Koyejo, and Indranil Gupta. 2019. Asynchronous federated optimization. arXiv preprint arXiv:1903.03934 (2019).
[53] Cong Xie, Sanmi Koyejo, and Indranil Gupta. 2020. Zeno++: Robust fully asynchronous SGD. In ICML.
[54] Chenhao Xu, Youyang Qu, Yong Xiang, and Longxiang Gao. 2021. Asynchronous federated learning on heterogeneous devices: A survey. arXiv preprint arXiv:2109.04269 (2021).
[55] Yi-Rui Yang and Wu-Jun Li. 2021. BASGD: Buffered Asynchronous SGD for Byzantine Learning. In ICML.
[56] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. 2018. Byzantine-robust distributed learning: Towards optimal statistical rates. In ICML.
[57] Zaixi Zhang, Xiaoyu Cao, Jinayuan Jia, and Neil Zhenqiang Gong. 2022. FLDe-
tector: Defending Federated Learning Against Model Poisoning Attacks via Detecting Malicious Clients. In KDD. [58] Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma,
and Tie-Yan Liu. 2017. Asynchronous stochastic gradient descent with delay compensation. In ICML.

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA

A APPENDIX

A.1 Proof of Lemma 5.1

The proof of Lemma 5.1 is mainly from [16, 42]. We first check

Assumption 1. Consider the linear regression model defined in

Lemma 5.1, that is ğ‘¦ğ‘– = âŸ¨ğ’–ğ‘–, ğœ½ âˆ—âŸ© + ğ‘’ğ‘–, where ğœ½ âˆ— is the unknown true

model parameter, ğ’–ğ‘– âˆ¼ ğ‘ (0, ğ‘° ), ğ‘’ğ‘– âˆ¼ ğ‘ (0, 1), ğ‘’ğ‘– is independent of ğ’–ğ‘– .

The population risk of (1) is given by minğœ½

1 2

âˆ¥ğœ½

âˆ’

ğœ½

âˆ—

âˆ¥2

+

1 2

.

ğ¹

(ğœ½

)

â‰œ

E [ğ‘“ (ğœ½, ğ‘‹ )] = E

1 2

( âŸ¨ğ’–,

ğœ½

âŸ©

âˆ’

ğ‘¦)2

=E

1 2

( âŸ¨ğ’–,

ğœ½

âŸ©

âˆ’

âŸ¨ğ’–,

ğœ½

âˆ—âŸ©

âˆ’

ğ‘’)2

=

1 2

âˆ¥ğœ½

âˆ’

ğœ½ âˆ— âˆ¥2

+

1
.
2

Then

the

gradient

of

population

risk

is

âˆ‡ğ¹ (ğœ½ )

=

ğœ½ âˆ’ ğœ½ âˆ—. We can see that the population risk ğ¹ (Â·) is ğ¿-Lipschitz

continuous with ğ¿ = 1, and ğœ‡-strongly convex with ğœ‡ = 1.

We then check Assumption 2. Let ğ’– âˆ¼ ğ‘ (0, ğ‘° ), ğ‘’ âˆ¼ ğ‘ (0, 1) and ğ‘’ is independent of ğ’–, then one has âˆ‡ğ‘“ (ğœ½, ğ‘‹ ) = ğ’– âŸ¨ğ’–, ğœ½ âˆ’ ğœ½ âˆ—âŸ© âˆ’ ğ’–ğ‘’.

Let v âˆˆ ğ‘½ be the unit vector, we further have that:

âˆ‡

ğ‘“

(ğœ½

âˆ—
,

ğ‘‹

),

v

= âˆ’ğ‘’ âŸ¨ğ’–, vâŸ© .

(5)

Since ğ’– âˆ¼ ğ‘ (0, ğ‘° ), v is the unit vector and ğ’– is independent

of ğ‘’, so we have âŸ¨ğ’–, vâŸ© âˆ¼ ğ‘ (0, 1) and âŸ¨ğ’–, vâŸ© is independent of ğ‘’.

According

to

the

standard

conditioning

argument,

for

2
ğœ‘

â‰¤

1,

one

has:

E

exp

ğœ‘

âˆ‡ğ‘“

(ğœ½

âˆ—
,

ğ‘‹

),

v

(ğ‘)
= E [exp (âˆ’ğœ‘ğ‘’ âŸ¨ğ’–, vâŸ©)]

= E [E [exp (âˆ’ğœ‘ğ‘¦ âŸ¨ğ’–, vâŸ©) |ğœ‘ = ğ‘¦]]

(ğ‘)
=E

exp

ğœ‘

2
ğ‘’

2/2

(ğ‘)
=

1 âˆ’ ğœ‘2

âˆ’1/2

(ğ‘‘ )
â‰¤

ğœ‘2
ğ‘’,

(6)

where (ğ‘) is because of Eq. (5); (ğ‘) is obtained by applying the

moment generating function of Gaussian distribution; (ğ‘) is because

for

the

moment

generating

function

of

2
ğœ’

distribution,

we

have

E exp

1

âˆ’

2
ğœ‘

ğ‘¡ğœ‘2 = (1 âˆ’ 2ğ‘¡ )âˆ’1/2 for ğ‘¡

â‰¥

âˆ’2ğœ‘ 2
ğ‘’âˆš

for

|ğœ‘ |

âˆšâ‰¤

âˆš 1/ 2.

< 1/2; (ğ‘‘) is due to the fact that Therefore, Assumption 2 holds

when ğ›¼1 = 2 and ğœŒ1 = 2.

Next, we check Assumption 3. As âˆ‡ğ‘“ (ğœ½, ğ‘‹ ) =

ğ’– âŸ¨ğ’–, ğœ½ âˆ’ ğœ½ âˆ—âŸ© âˆ’ ğ’–ğ‘’, âˆ‡ğ‘“ (ğœ½ âˆ—, ğ‘‹ ) = âˆ’ğ’–ğ‘’, so ğ‘(ğœ½, ğ‘‹ ) =

âˆ‡ğ‘“

(ğœ½,

ğ‘‹)

âˆ’

âˆ‡ğ‘“

(ğœ½

âˆ—
,

ğ‘‹

)

=

ğ’–

âŸ¨ğ’–, ğœ½

âˆ’

ğœ½ âˆ—âŸ©.

As

E

[ğ‘ (ğœ½ ,

ğ‘‹ )]

=

ğœ½

âˆ’

ğœ½âˆ—,

so

âŸ¨ğ‘(ğœ½, ğ‘‹ ) âˆ’ E [ğ‘(ğœ½, ğ‘‹ )] , vâŸ© = âŸ¨ğ’–, ğœ½ âˆ’ ğœ½ âˆ—âŸ© âŸ¨ğ’–, vâŸ© âˆ’ âŸ¨ğœ½ âˆ’ ğœ½ âˆ—, vâŸ© .

For a fixed ğœ½ âˆˆ further decompose

ğš¯, ğœ½ â‰  ğœ½ âˆ—, we let ğœ½ âˆ’ ğœ½ âˆ— as ğœ½ âˆ’ ğœ½ âˆ— =

âˆšÃ° = ğ‘1v

âˆ¥ğœ½âˆšâˆ’ ğœ½ âˆ— âˆ¥ > 0. + ğ‘2vË†, where

We vË† is

an vector perpendicular to v, ğ‘1 + ğ‘2 = Ã°2. We further have that

âŸ¨ğ’–, vË†âŸ© âˆ¼ ğ‘ (0, 1) and:

ğ’–, ğœ½ âˆ’ ğœ½âˆ—

âŸ¨ğ’–, vâŸ© âˆ’

ğœ½

âˆ’

ğœ½

âˆ—
,

v

âˆš

âˆš

= ğ‘1 âŸ¨ğ’–, vâŸ©2 âˆ’ 1 + ğ‘2 âŸ¨ğ’–, vË†âŸ© âŸ¨ğ’–, vâŸ© .

(7)

One also has E [âŸ¨ğ’–, vË†âŸ© âŸ¨ğ’–, vâŸ©] = E vË†âŠ¤uuâŠ¤v = vË†âŠ¤E uuâŠ¤ v = 0, where uâŠ¤ is the transpose of u. Henceâˆš, âŸ¨ğ’–, vË†âŸ© and âŸ¨ğ’–, vâŸ© are mutually independent. For any ğœ‘ satisfies ğœ‘ ğ‘1 < 1/4 and ğœ‘2ğ‘2 < 1/4,
we have:

E [exp (ğœ‘ âŸ¨ğ‘(ğœ½, ğ‘‹ ) âˆ’ E [ğ‘(ğœ½, ğ‘‹ )] , vâŸ©)]

(ğ‘)
=E

exp

âˆš ğœ‘ ğ‘1

âŸ¨ğ’–, vâŸ©2 âˆ’ 1

âˆš + ğœ‘ ğ‘2 âŸ¨ğ’–, vË†âŸ© âŸ¨ğ’–, vâŸ©

âˆšï¸„
(ğ‘)
â‰¤E

âˆš 2ğœ‘ ğ‘1
ğ‘’

âŸ¨ğ’–,v âŸ© 2 âˆ’1

âˆš
E ğ‘’2ğœ‘ ğ‘2 âŸ¨ğ’–,vË† âŸ© âŸ¨ğ’–,vâŸ©

âˆšï¸„
âˆš
= ğ‘’âˆ’ğœ‘ ğ‘1 E

âˆš 2ğœ‘ ğ‘1
ğ‘’

âŸ¨ğ’–,vâŸ©2

âˆš
E ğ‘’2ğœ‘ ğ‘2 âŸ¨ğ’–,vË† âŸ© âŸ¨ğ’–,vâŸ©

Minghong Fang, Jia Liu, Neil Zhenqiang Gong, and Elizabeth S. Bentley

(ğ‘)
=

âˆš

âˆ’ğœ‘
ğ‘’

ğ‘1

âˆš 1 âˆ’ 4ğœ‘ ğ‘1

âˆ’1/4

1

âˆ’

4ğœ‘

2
ğ‘2

âˆ’1/4
,

(8)

where (ğ‘) holds by plugging in Eq. (7); (ğ‘) is true by applying

the Cauchy-Schwartzâ€™s inequality; (ğ‘) is true by applying the mo-

ment for 0

generating â‰¤ ğ‘¡ â‰¤ 1/2,

afunndcğ‘’tiâˆ’oğ‘¡n/âˆšo1f

ğœ’2 distribution. âˆ’ 2ğ‘¡ â‰¤ ğ‘’2ğ‘¡2 for

Since 1 âˆ’ |ğ‘¡ | â‰¤ 1/4.

ğ‘¡ â‰¥ğ‘’ Thus,

âˆ’4ğ‘¡
for

ğœ‘2 â‰¤ 1/(64Ã°2), one has E [exp (ğœ‘ âŸ¨ğ‘(ğœ½, ğ‘‹ ) âˆ’ E [ğ‘(ğœ½, ğ‘‹ )] , vâŸ©)] â‰¤

exp 4ğœ‘2 (ğ‘âˆš1 + ğ‘2) â‰¤ exp 4ğœ‘2Ã°2 . Therefore, Assumption 3 holds with ğ›¼2 = 8 and ğœŒ2 = 8.
Last, we check Assumption 4. As âˆ‡ğ‘“ (ğœ½, ğ‘‹ ) = ğ’– âŸ¨ğ’–, ğœ½ âˆ’ ğœ½ âˆ—âŸ© âˆ’ ğ’–ğ‘’,

then âˆ‡2 ğ‘“ (ğœ½, ğ‘‹ ) = ğ’–ğ’–âŠ¤, thus it suffices to show the following

P

1 |ğ‘‹ğ‘  |

ğ‘¥ âˆˆğ‘‹ğ‘  âˆ‡2 ğ‘“ (ğœ½, ğ‘¥) â‰¤ ğ» = P

1 |ğ‘‹ğ‘  |

|ğ‘‹ğ‘  | ğ‘— =1

ğ’–

ğ‘—

ğ’–âŠ¤
ğ‘—

â‰¤ğ»

â‰¥

1âˆ’ğ›½/3. Let ğ‘¼ = ğ’–1, ğ’–2, ..., ğ’– |ğ‘‹ğ‘  | âŠ‚ Rğ‘‘Ã—|ğ‘‹ğ‘  |, one has

|ğ‘‹ğ‘  | ğ‘— =1

ğ’–

ğ‘—

ğ’–

âŠ¤ ğ‘—

=

ğ‘¼ ğ‘¼ âŠ¤ and P

1 |ğ‘‹ğ‘  |

|ğ‘‹ğ‘  | ğ‘— =1

ğ’–ğ‘—

ğ’–âŠ¤
ğ‘—

â‰¤ğ»

=P

âˆ¥ğ‘¼ âˆ¥ â‰¤ âˆšï¸|ğ‘‹ğ‘  | ğ»

. Since

ğ‘¼

is

an

i.i.d.

standard

Gaussian

matrix,

then

according âˆš

to

[47],

for ğ‘¡ â‰¥ 0, we have that: P âˆ¥ğ‘¼ âˆ¥ â‰¤ âˆšï¸|ğ‘‹ğ‘  | + ğ‘‘ + ğ‘¡ â‰¥ 1 âˆ’

âˆš

2

exp âˆ’ğ‘¡2/2 . Setting ğ» = âˆšï¸|ğ‘‹ğ‘  | + ğ‘‘ + âˆšï¸2log(4/ğ›½) /|ğ‘‹ğ‘  | and

ğ‘¡ = âˆšï¸2log(4/ğ›½) to complete the proof.

A.2 Proof of Theorem 1

Since we assume that the server delay ğœğ‘  is zero, i.e., ğœğ‘  = 0. Then in the following, we use ğ’ˆğ‘ ğ‘¡ to denote the server model update.
If server updates the global model following the AFLGuard algo-

rithm, i.e., Algorithm 2, then for any ğ‘¡ > 0, ğœğ‘– > 0, we have:

ğœ½ğ‘¡+1 âˆ’ ğœ½ âˆ— = ğœ½ğ‘¡ âˆ’ ğœ‚ğ’ˆğ‘¡âˆ’ğœğ‘– âˆ’ ğœ½ âˆ—
ğ‘–

â‰¤ ğœ½ğ‘¡ âˆ’ ğœ‚âˆ‡ğ¹ (ğœ½ğ‘¡ ) âˆ’ ğœ½ âˆ— + ğœ‚ ğ’ˆğ‘¡âˆ’ğœğ‘– âˆ’ âˆ‡ğ¹ (ğœ½ğ‘¡ )
ğ‘–

(ğ‘)
â‰¤

ğœ½ğ‘¡ âˆ’ ğœ‚âˆ‡ğ¹ (ğœ½ğ‘¡ ) âˆ’ ğœ½ âˆ—

+ğœ‚ğœ†

âˆ‡ğ¹ (ğœ½ğ‘¡ ) âˆ’ âˆ‡ğ¹ (ğœ½ âˆ—)

â™£

â‹†

+ ğœ‚ (ğœ† + 1) ğ’ˆğ‘ ğ‘¡ âˆ’ âˆ‡ğ¹ (ğœ½ğ‘¡ )

â™¦

âˆšï¸„

(ğ‘)

2ğœ‚ ğœ‡ ğ¿

â‰¤ 1âˆ’

+ ğœ‚ğ¿ğœ† + 8ğœ‚Î›(ğœ† + 1)

ğœ‡+ğ¿

ğœ½ğ‘¡ âˆ’ ğœ½âˆ—

+ 4ğœ‚Î“(ğœ† + 1),

(9)

where (ğ‘) uses âˆ‡ğ¹ (ğœ½ âˆ—) = 0 and Lemma 1, (ğ‘) is true by plugging in
Lemma 2, Assumption 1 and Lemma 3 into â™£, â‹†, and â™¦, respectively. Telescoping, one has ğœ½ğ‘¡ âˆ’ ğœ½ âˆ— â‰¤ (1 âˆ’ ğ‘)ğ‘¡ ğœ½ 0 âˆ’ ğœ½ âˆ— +4ğœ‚Î“(ğœ† +1)/ğ‘,

where ğ‘ = 1 âˆ’

âˆšï¸ 1

âˆ’

2ğœ‚ ğœ‡ ğ¿/ ( ğœ‡

+

ğ¿)

+

ğœ‚ğ¿ğœ†

+

8ğœ‚Î›(ğœ†

+

1)

.

Next, we proof Lemma 1, Lemma 2 and Lemma 3 one by one.

Lemma 1. If the server uses the AFLGuard algorithm to update the global model, then for arbitrary number of malicious clients, one has:

ğ’ˆğ‘¡âˆ’ğœğ‘– âˆ’ âˆ‡ğ¹ (ğœ½ğ‘¡ )
ğ‘–

â‰¤ (ğœ† + 1)

ğ’ˆğ‘ ğ‘¡ âˆ’ âˆ‡ğ¹ (ğœ½ğ‘¡ )

+ğœ†

âˆ‡ğ¹ (ğœ½ğ‘¡ )

.

Proof.

ğ’ˆğ‘¡âˆ’ğœğ‘– âˆ’ âˆ‡ğ¹ (ğœ½ğ‘¡ )
ğ‘–

â‰¤

ğ’ˆğ‘¡ âˆ’ğœğ‘–
ğ‘–

âˆ’ ğ’ˆğ‘ ğ‘¡

+

ğ’ˆğ‘ ğ‘¡ âˆ’ âˆ‡ğ¹ (ğœ½ğ‘¡ )

AFLGuard: Byzantine-robust Asynchronous Federated Learning

(ğ‘)
â‰¤ ğœ† ğ’ˆğ‘ ğ‘¡ + ğ’ˆğ‘ ğ‘¡ âˆ’ âˆ‡ğ¹ (ğœ½ğ‘¡ )

â‰¤ ğœ† ğ’ˆğ‘ ğ‘¡ âˆ’ âˆ‡ğ¹ (ğœ½ğ‘¡ ) + ğœ† âˆ‡ğ¹ (ğœ½ğ‘¡ ) + ğ’ˆğ‘ ğ‘¡ âˆ’ âˆ‡ğ¹ (ğœ½ğ‘¡ )

= (ğœ† + 1) ğ’ˆğ‘ ğ‘¡ âˆ’ âˆ‡ğ¹ (ğœ½ğ‘¡ ) + ğœ† âˆ‡ğ¹ (ğœ½ğ‘¡ ) ,

(10)

where (ğ‘) is because for the AFLGuard algorithm, we have that

ğ’ˆğ‘¡ âˆ’ğœğ‘–
ğ‘–

âˆ’

ğ’ˆğ‘ ğ‘¡

â‰¤ ğœ† ğ’ˆğ‘ ğ‘¡ .

â–¡

Lemma 2. Suppose Assumption 1 holds, if the global learning rate

satisfies ğœ‚

â‰¤

2 ğœ‡+ğ¿

,

then

we have

the following:

ğœ½ğ‘¡ âˆ’ ğœ‚âˆ‡ğ¹ (ğœ½ğ‘¡ ) âˆ’ ğœ½ âˆ—

â‰¤

âˆšï¸ 1

âˆ’

2ğœ‚ ğœ‡ ğ¿/ ( ğœ‡

+

ğ¿)

ğœ½ğ‘¡ âˆ’ ğœ½âˆ—

.

Proof. ğœ½ğ‘¡ âˆ’ ğœ‚âˆ‡ğ¹ (ğœ½ğ‘¡ ) âˆ’ ğœ½ âˆ— 2 = ğœ½ğ‘¡ âˆ’ ğœ½ âˆ— 2 + ğœ‚2 âˆ‡ğ¹ (ğœ½ğ‘¡ ) 2 âˆ’ 2ğœ‚ ğœ½ğ‘¡ âˆ’ ğœ½ âˆ—, ğ¹ (ğœ½ğ‘¡ ) . According to [8], if ğ¹ (ğœ½ ) is ğ¿-smooth and

ğœ‡-strongly convex, for any ğœ½, ğœ½ â€²

âˆˆ

Î˜, one has

ğœ‡ğ¿ ğœ‡+ğ¿

âˆ¥ğœ½

âˆ’

ğœ½ â€²âˆ¥2

+

1 ğœ‡+ğ¿

âˆ¥âˆ‡ğ¹ (ğœ½ )

âˆ’

âˆ‡ğ¹ (ğœ½ â€²)âˆ¥2

â‰¤

âŸ¨âˆ‡ğ¹ (ğœ½ ) âˆ’ âˆ‡ğ¹ (ğœ½ â€²), ğœ½ âˆ’ ğœ½ â€²âŸ© . Setting

ğœ½

=

ğœ½ğ‘¡, ğœ½ â€²

=

ğœ½âˆ—,

since

âˆ‡ğ¹ (ğœ½ âˆ—)

=

0,

we

have

that

ğœ‡ğ¿ ğœ‡+ğ¿

ğœ½ğ‘¡ âˆ’ ğœ½âˆ— 2 +

1 ğœ‡+ğ¿

âˆ‡ğ¹ (ğœ½ğ‘¡ ) 2 â‰¤

âˆ‡ğ¹ (ğœ½ğ‘¡ ), ğœ½ğ‘¡ âˆ’ ğœ½ âˆ—

. Thus one has:

ğœ½ğ‘¡ âˆ’ ğœ‚âˆ‡ğ¹ (ğœ½ğ‘¡ ) âˆ’ ğœ½ âˆ— 2 â‰¤

ğœ½ğ‘¡

âˆ’ ğœ½âˆ—

2

+

2
ğœ‚

âˆ‡ğ¹ (ğœ½ğ‘¡ ) 2

âˆ’ 2ğœ‚

ğœ‡ğ¿ ğœ‡+ğ¿

ğœ½ğ‘¡

âˆ’ ğœ½âˆ—

2+

1 ğœ‡+ğ¿

âˆ‡ğ¹ (ğœ½ğ‘¡ ) 2

= (1 âˆ’ 2ğœ‚ğœ‡ğ¿/(ğœ‡ + ğ¿)) ğœ½ğ‘¡ âˆ’ ğœ½ âˆ— 2 + ğœ‚ (ğœ‚ âˆ’ 2/(ğœ‡ + ğ¿)) âˆ‡ğ¹ (ğœ½ğ‘¡ ) 2

(ğ‘)
â‰¤ (1 âˆ’ 2ğœ‚ğœ‡ğ¿/(ğœ‡ + ğ¿))

ğœ½ğ‘¡ âˆ’ ğœ½âˆ—

2
,

(11)

where (ğ‘) is because 0 < ğœ‚ â‰¤ 2/(ğœ‡ + ğ¿). ğœ½ğ‘¡ âˆ’ ğœ‚âˆ‡ğ¹ (ğœ½ğ‘¡ ) âˆ’ ğœ½ âˆ— â‰¤

âˆšï¸ 1

âˆ’

2ğœ‚

ğœ‡ğ¿/(ğœ‡

+

ğ¿)

ğœ½ğ‘¡ âˆ’ ğœ½âˆ—

.

â–¡

The proof of Lemma 3 is mainly motivated from [9, 16]. To

simplify the notation, we will ignore the superscript ğ‘¡ in ğ’ˆğ‘ ğ‘¡ . Define

âˆ‡ğ‘“Â¯ğ‘  (ğœ½ ) =

1 |ğ‘‹ğ‘  |

ğ‘¥ âˆˆğ‘‹ğ‘  âˆ‡ğ‘“ (ğœ½, ğ‘¥).

{ğœ½

Lemma : âˆ¥ğœ½ âˆ’ ğœ½

âˆ—

3. If âˆš Assumptions âˆ¥ â‰¤ ğœ– ğ‘‘ } holds for

2-4 some

hold and parameter ğœ–

Î˜âŠ‚ > 0. For

any

ğ›½

âˆˆ

(0, 1),

if

Î“

â‰¤

ğ›¼12/ğœŒ1

and

Î›

â‰¤

2
ğ›¼
2

/ğœŒ2

,

we

have

that:

P âˆ¥ğ’ˆğ‘  âˆ’ âˆ‡ğ¹ (ğœ½ ) âˆ¥ â‰¤ 8Î› ğœ½ âˆ’ ğœ½ âˆ— + 4Î“ â‰¥ 1 âˆ’ ğ›½,

where Î“, Î› are defined in Theorem 1.

âˆšï¸ƒ

âˆš

Proof. We define ğœ‰ = ğœŒ2ğ›¼1
2ğ›¼ 2

ğ‘‘ |ğ‘‹ğ‘  |

and let ğœ“

=

ğœ–

ğ‘‘/ğœ‰ . Then

for any integer 1

â‰¤

ğ‘™

â‰¤

2
ğœ“ , we define Î˜ğ‘™

=

{ğœ½

: âˆ¥ğœ½ âˆ’ ğœ½âˆ—âˆ¥

â‰¤ ğœ‰ğ‘™} .

For a given integer ğ‘™, we let ğœ½1, Â· Â· Â· , ğœ½ğœË† be an ğœ”-cover of Î˜ğ‘™ ,

where ğœ”

=

ğ›¼2ğœ‰ğ‘™ ğ‘…

âˆšï¸ ğ‘‘

/|ğ‘‹ğ‘ 

|,

where

ğ‘…

=

max {ğ¿, ğ» }. From [47], we

know that log ğœË† â‰¤ ğ‘‘ log(3ğœ‰ğ‘™/ğœ”). For any ğœ½ âˆˆ Î˜ğ‘™ , there exists a

1 â‰¤ ğ‘ â‰¤ ğœ” such that âˆ¥ğœ½ âˆ’ ğœ½ğ‘ âˆ¥ â‰¤ ğœ” holds. Then, based on the trian-

gle inequality, one has âˆ‡ğ‘“Â¯ğ‘  (ğœ½ ) âˆ’ âˆ‡ğ¹ (ğœ½ ) â‰¤ âˆ¥âˆ‡ğ¹ (ğœ½ ) âˆ’ âˆ‡ğ¹ (ğœ½ğ‘ ) âˆ¥ +

âˆ‡ğ‘“Â¯ğ‘  (ğœ½ ) âˆ’ âˆ‡ğ‘“Â¯ğ‘  (ğœ½ğ‘ ) + âˆ‡ğ‘“Â¯ğ‘  (ğœ½ğ‘ ) âˆ’ âˆ‡ğ¹ (ğœ½ğ‘ ) . By Assumption 1, one

has âˆ¥âˆ‡ğ¹ (ğœ½ ) âˆ’ âˆ‡ğ¹ (ğœ½ğ‘ ) âˆ¥ â‰¤ ğ¿ âˆ¥ğœ½ âˆ’ ğœ½ğ‘ âˆ¥ â‰¤ ğ¿ğœ”. We define event ğ¸1 as:

ğ¸1 = supğœ½,ğœ½â€² âˆˆÎ˜:ğœ½ â‰ ğœ½â€² âˆ‡ğ‘“Â¯ğ‘  (ğœ½ ) âˆ’âˆ‡ğ‘“Â¯ğ‘  (ğœ½ â€²) â‰¤ ğ» ğœ½ âˆ’ğœ½ â€² . (12)

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA

According to Assumption 4, we have P {ğ¸1} â‰¥ 1 âˆ’ ğ›½/3. One also

has supğœ½ âˆˆÎ˜ âˆ‡ğ‘“Â¯ğ‘  (ğœ½ again, and because

) âˆ’ âˆ‡ğ‘“Â¯ğ‘  E [ğ‘ (ğœ½

(ğœ½ğ‘ ) , ğ‘‹ )]

â‰¤ ğ»ğœ”. By = âˆ‡ğ¹ (ğœ½ ) âˆ’

the triangle âˆ‡ğ¹ (ğœ½ âˆ—), we

inequality have:

âˆ‡ğ‘“Â¯ğ‘  (ğœ½ğ‘ ) âˆ’ âˆ‡ğ¹ (ğœ½ğ‘ ) â‰¤ âˆ‡ğ‘“Â¯ğ‘  (ğœ½ âˆ—) âˆ’ âˆ‡ğ¹ (ğœ½ âˆ—)

+ âˆ‡ğ‘“Â¯ğ‘  (ğœ½ğ‘ ) âˆ’ âˆ‡ğ‘“Â¯ğ‘  (ğœ½ âˆ—) âˆ’ âˆ‡ğ¹ (ğœ½ğ‘ ) âˆ’ âˆ‡ğ¹ (ğœ½ âˆ—)

â‰¤

âˆ‡ğ‘“Â¯ğ‘  (ğœ½ âˆ—) âˆ’ âˆ‡ğ¹ (ğœ½ âˆ—)

+

1 âˆ‘ï¸ |ğ‘‹ğ‘  | ğ‘¥ âˆˆğ‘‹ğ‘  ğ‘(ğœ½ğ‘, ğ‘¥) âˆ’ E [ğ‘ (ğœ½ğ‘, ğ‘‹ )]

.

Define events ğ¸2 = âˆ‡ğ‘“Â¯ğ‘  (ğœ½ âˆ—) âˆ’ âˆ‡ğ¹ (ğœ½ âˆ—) â‰¤ 2Î“ and ğ¸ğ‘™ as:

1 âˆ‘ï¸ ğ¸ğ‘™ = sup1â‰¤ğ‘˜ â‰¤ğ‘ |ğ‘‹ğ‘  | ğ‘¥ âˆˆğ‘‹ğ‘  ğ‘(ğœ½ğ‘˜, ğ‘¥) âˆ’ E [ğ‘ (ğœ½ğ‘˜, ğ‘‹ )] â‰¤ 2Î›ğœ‰ğ‘™ .

By

Proposition

1,

Proposition

2,

since

Î“

â‰¤

2
ğ›¼
1

/ğœŒ1,

Î›

â‰¤

2
ğ›¼
2

/ğœŒ2

,

we have P {ğ¸2} â‰¥ 1 âˆ’ ğ›½/3, P {ğ¸ğ‘™ } â‰¥ 1 âˆ’ ğ›½/(3ğœ“ ). Thus, on event

ğ¸1 âˆ© ğ¸2 âˆ© ğ¸ğ‘™ , one has supğœ½ âˆˆÎ˜ğ‘™ âˆ‡ğ‘“Â¯ğ‘  (ğœ½ ) âˆ’ âˆ‡ğ¹ (ğœ½ ) â‰¤ ğ¿ğœ” + ğ»ğœ” + 2Î“ + 2Î›ğœ‰ğ‘™ â‰¤ 4Î›ğœ‰ğ‘™ + 2Î“. Thus, we have at least 1 âˆ’ ğ›½ that event

ğ¸ = ğ¸1 âˆ© exists an

ğ¸2 âˆ© 1â‰¤ğ‘™

(âˆ©ğœ“ğ‘™=1ğ¸ğ‘™ ). Also, â‰¤ ğœ“ such that

on event ğ¸, for any ğœ½ âˆˆ Î˜ğœ“ , there (ğ‘™ âˆ’ 1)ğœ‰ < âˆ¥ğœ½ âˆ’ ğœ½ âˆ— âˆ¥ â‰¤ ğœ‰ğ‘™ holds. If

ğ‘™ = 1, we have âˆ‡ğ‘“Â¯ğ‘  (ğœ½ ) âˆ’ âˆ‡ğ¹ (ğœ½ ) â‰¤ 4Î›ğœ‰ + 2Î“ â‰¤ 4Î“; and 2(ğ‘™ âˆ’ 1) â‰¥ ğ‘™ if ğ‘™ â‰¥ 2. Thus, one has âˆ‡ğ‘“Â¯ğ‘  (ğœ½ ) âˆ’ âˆ‡ğ¹ (ğœ½ ) â‰¤ 8Î› âˆ¥ğœ½ âˆ’ ğœ½ âˆ— âˆ¥ + 2Î“. On ğ¸, one has supğœ½ âˆˆÎ˜ğœ“ âˆ‡ğ‘“Â¯ğ‘  (ğœ½ ) âˆ’ âˆ‡ğ¹ (ğœ½ ) â‰¤ 8Î› âˆ¥ğœ½ âˆ’ ğœ½ âˆ— âˆ¥ + 4Î“. â–¡

The following proof of Proposition 1 is mainly motivated from [9, 16].

ğœ½

Propositionâˆš 1. âˆˆ Î˜, let Î“ = 2ğ›¼

Suppose

âˆšï¸
1

(ğ‘‘

log

Assumption 2 holds. For 6 + log(3/ğ›½))/|ğ‘‹ğ‘  |. If Î“

any ğ›½ âˆˆ

â‰¤

ğ›¼2
1

/ğœŒ1

(0, 1), , then

we have:

P

1 âˆ‘ï¸

âˆ‡ğ‘“

(ğœ½

âˆ—
,

ğ‘¥)

âˆ’

âˆ‡ğ¹

(ğœ½

âˆ—)

â‰¥ 2Î“

â‰¤ ğ›½/3.

|ğ‘‹ğ‘  | ğ‘¥ âˆˆğ‘‹

Proof. We let B

=

{v1,, Â· Â· Â· , vğœ }

be

one

1
-cover
2

of

the unit sphere V. By [47], one has log ğœ â‰¤ ğ‘‘ log 6 and

âˆ‡ğ‘“Â¯ğ‘  (ğœ½ âˆ—) âˆ’ âˆ‡ğ¹ (ğœ½ âˆ—) â‰¤ 2 supvâˆˆB âˆ‡ğ‘“Â¯ğ‘  (ğœ½ âˆ—) âˆ’ âˆ‡ğ¹ (ğœ½ âˆ—), v . If As-

sumption 2 and the condition Î“

â‰¤

ğ›¼2
1

/ğœŒ1

satisfy,

and

accord-

ing to the concentration inequalities for sub-exponential random

variables [48], we have that P âˆ‡ğ‘“Â¯ğ‘  (ğœ½ âˆ—) âˆ’ âˆ‡ğ¹ (ğœ½ âˆ—), v â‰¥ Î“ â‰¤

exp

âˆ’

|ğ‘‹ğ‘ 

|

Î“2/

(2ğ›¼ 2 )
1

. One has P

âˆ‡ğ‘“Â¯ğ‘  (ğœ½ âˆ—) âˆ’ âˆ‡ğ¹ (ğœ½ âˆ—) â‰¥ 2Î“

â‰¤

exp

âˆ’

|ğ‘‹ğ‘  |

Î“2 / (2ğ›¼ 2 )
1

+ğ‘‘

log 6

by the union bound. Put in Î“ finishes

the proof.

â–¡

ğœ½

âˆˆPrÎ˜o,pleotsÎ”iti=onâˆš22ğ›¼. 2Sâˆšuï¸(pğ‘‘polosge

Assumption 3 holds. For 6 + log(3/ğ›½))/|ğ‘‹ğ‘  |. If Î”

any ğ›½ âˆˆ (0, 1),

â‰¤

ğ›¼2
2

/ğœŒ2,

then

we have:

ï£±

ï£¼

ï£´ ï£´ï£² P ï£´ ï£´

1 âˆ‘ï¸ |ğ‘‹ğ‘  | ğ‘¥ âˆˆğ‘‹ğ‘  âˆ‡ğ‘(ğœ½, ğ‘¥) âˆ’ E [ğ‘(ğœ½, ğ‘‹ )]

â‰¥ 2Î”

ğœ½ âˆ’ğœ½âˆ—

ï£´ ï£´ï£½

â‰¤ ğ›½/3.

ï£´ ï£´

ï£³

ï£¾

Proof. The proof of Proposition 2 is similar to that of Proposi-

tion 1, and is omitted here for brevity.

â–¡

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA

Minghong Fang, Jia Liu, Neil Zhenqiang Gong, and Elizabeth S. Bentley

Test error rate

AsyncSGD w/o attacks

AsyncSGD

Kardam

BASGD

Zeno++

AFLGuard

1.0

1.0

1.0

Test error rate

Attack success rate Test error rate

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0.050 100Nu1(ma5)0bLe2Fr0oa0fttca3lic0ek0nts400 500 0.050 100N(bu1)m5G0baeu2rs0os0f ac3tlit0ea0nctks400 500 0.050 100Nu(1cm5)0bGeD2r0oa0ftct3ali0ec0nkts400 500

1.0

1.0

1.0

Test error rate

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0.050 100Nu(1dm5)0bBeD2r0oa0ftct3ali0ec0nkts400 500 0.050 100Nu(1em5)0bBeD2r0oa0ftct3ali0ec0nkts400 500 0.050 100N(fu1)m5A0bdea2rp0ot0fact3lti0ea0nctks400 500

Test error rate

Figure 8: Test error rates and attack success rates of different defenses under different attacks with different number of clients on MNIST dataset.

Table 4: The CNN architecture.

Layer Input Convolution + ReLU Max Pooling Convolution + ReLU Max Pooling Fully Connected + ReLU Softmax

Size 28 Ã— 28 Ã— 1 3 Ã— 3 Ã— 30
2Ã—2 3 Ã— 3 Ã— 50
2Ã—2 100
10

A.3 Datasets
1) Synthetic Dataset: We randomly generate 10,000 data samples of dimensions ğ‘‘ = 100. Each dimension follows the Gaussian distribution ğ‘ (0, 1) and noise ğ‘’ğ‘– is sampled from ğ‘ (0, 1). We use ğ‘ (0, 25) to generate each entry of ğœ½ âˆ—. We generate ğ‘¦ğ‘– according to the linear regression model in Lemma 5.1. We randomly draw 8,000 samples
for training and use the remaining 2,000 samples for testing.
2) MNIST [30]: MNIST is a 10-class handwritten digits image classification dataset, which contains 60,000 samples for training and
10,000 examples for testing.

3) Fashion-MNIST [50]: Fashion-MNIST is a dataset containing images of 70,000 fashion products from 10 classes. The training set has 60,000 images and the testing set has 10,000 images. 4) Human Activity Recognition (HAR) [4]: The HAR dataset aims to recognize 6 types of human activities and the dataset is collected from smartphones of 30 real-world users. There are 10,299 examples in total and each example includes 561 features. We randomly sample 75% of each clientâ€™s examples as training data and use the rest as test data. 5) Colorectal Histology MNIST [28]: Colorectal Histology MNIST is an 8-class dataset for classification of textures in human colorectal cancer histology. This dataset contains 5,000 images and each image has 64Ã—64 grayscale pixels. We randomly select 4,000 images for training and use the remaining 1,000 images for testing. 6) CIFAR-10 [29]: CIFAR-10 consists of 60,000 color images. This dataset has 10 classes, and there are 6,000 images of each class. The training set has 50,000 images and the testing set has 10,000 images.

AFLGuard: Byzantine-robust Asynchronous Federated Learning

ACSAC â€™22, December 5â€“9, 2022, Austin, TX, USA

Table 5: Test error rates and attack success rates of Zeno++ and AFLGuard under different attacks with different distribution shifts (DSs) on Fashion-MNIST, HAR, Colorectal Histology MNIST and CIFAR-10 datasets. The results of BD attack are in the form of â€œtest error rate / attack success rateâ€.

DS Attack No attack LF attack Gauss attack GD attack BD attack Adapt attack

0.1

Zeno++ AFLGuard

0.25

0.16

0.25

0.18

0.26

0.18

0.26

0.18

0.26 / 0.04 0.17 / 0.04

0.26

0.19

(a) Fashion-MNIST

0.5

0.6

Zeno++ AFLGuard Zeno++ AFLGuard

0.26

0.17

0.31

0.18

0.29

0.21

0.32

0.21

0.28

0.19

0.34

0.19

0.29

0.21

0.35

0.21

0.29 / 0.05 0.20 / 0.04 0.33 / 0.04 0.20 / 0.04

0.29

0.21

0.36

0.21

0.8

Zeno++ AFLGuard

0.58

0.21

0.72

0.22

0.58

0.22

0.58

0.21

0.58 / 0.03 0.20 / 0.03

0.72

0.22

1.0

Zeno++ AFLGuard

0.90

0.22

0.90

0.22

0.90

0.23

0.90

0.23

0.90 / 0.01 0.22 / 0.02

0.90

0.25

DS Attack No attack LF attack Gauss attack GD attack BD attack Adapt attack

0.167

Zeno++ AFLGuard

0.06

0.05

0.07

0.05

0.07

0.05

0.07

0.05

0.06 / 0.07 0.05 / 0.01

0.07

0.05

0.5

Zeno++ AFLGuard

0.06

0.05

0.08

0.05

0.07

0.05

0.08

0.05

0.07 / 0.01 0.05 / 0.01

0.08

0.05

(b) HAR

0.6

Zeno++ AFLGuard

0.08

0.05

0.09

0.05

0.09

0.05

0.09

0.06

0.09 / 0.02 0.05 / 0.01

0.09

0.06

0.8

Zeno++ AFLGuard

0.10

0.07

0.10

0.09

0.10

0.07

0.12

0.09

0.10 / 0.01 0.07 / 0.01

0.14

0.09

1.0

Zeno++ AFLGuard

0.43

0.12

0.43

0.36

0.43

0.36

0.43

0.42

0.43 / 0.01 0.36 / 0.01

0.55

0.54

DS Attack No attack LF attack Gauss attack GD attack BD attack Adapt attack

0.125

Zeno++ AFLGuard

0.25

0.18

0.31

0.21

0.35

0.21

0.29

0.21

0.42 / 0.02 0.22 / 0.02

0.44

0.29

(c) Colorectal Histology MNIST

0.5

0.6

Zeno++ AFLGuard Zeno++ AFLGuard

0.31

0.22

0.49

0.29

0.39

0.23

0.53

0.37

0.43

0.22

0.59

0.32

0.39

0.32

0.66

0.36

0.44 / 0.02 0.27 / 0.02 0.57 / 0.01 0.31 / 0.02

0.64

0.33

0.72

0.43

0.8

Zeno++ AFLGuard

0.62

0.32

0.63

0.41

0.70

0.35

0.74

0.49

0.62 / 0.01 0.32 / 0.01

0.77

0.51

1.0

Zeno++ AFLGuard

0.71

0.34

0.88

0.41

0.86

0.35

0.88

0.58

0.82 / 0.24 0.51 / 0.03

0.88

0.62

DS Attack No attack LF attack Gauss attack GD attack BD attack Adapt attack

0.1

Zeno++ AFLGuard

0.32

0.24

0.33

0.34

0.33

0.32

0.32

0.27

0.32 / 0.95 0.28 / 0.01

0.77

0.32

(d) CIFAR-10

0.5

0.6

Zeno++ AFLGuard Zeno++ AFLGuard

0.41

0.26

0.54

0.29

0.53

0.34

0.64

0.35

0.52

0.33

0.72

0.33

0.60

0.30

0.83

0.31

0.49 / 0.06 0.29 / 0.01 0.62 / 0.00 0.32 / 0.02

0.82

0.36

0.90

0.36

0.8

Zeno++ AFLGuard

0.68

0.31

0.71

0.35

0.76

0.35

0.85

0.33

0.80 / 0.01 0.34 / 0.01

0.90

0.36

1.0

Zeno++ AFLGuard

0.90

0.31

0.90

0.38

0.90

0.35

0.90

0.32

0.90 / 0.00 0.36 / 0.04

0.90

0.39

