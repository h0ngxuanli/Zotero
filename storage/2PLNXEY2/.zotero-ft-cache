
Skip to main content
Cornell University
We gratefully acknowledge support from
the Simons Foundation and member institutions.
arxiv logo > cs > arXiv:2201.08802

Help | Advanced Search
Search
Computer Science > Machine Learning
(cs)
[Submitted on 21 Jan 2022 ( v1 ), last revised 1 Feb 2022 (this version, v3)]
Title: Deconfounding to Explanation Evaluation in Graph Neural Networks
Authors: Ying-Xin Wu , Xiang Wang , An Zhang , Xia Hu , Fuli Feng , Xiangnan He , Tat-Seng Chua
Download a PDF of the paper titled Deconfounding to Explanation Evaluation in Graph Neural Networks, by Ying-Xin Wu and 6 other authors
Download PDF

    Abstract: Explainability of graph neural networks (GNNs) aims to answer "Why the GNN made a certain prediction?", which is crucial to interpret the model prediction. The feature attribution framework distributes a GNN's prediction to its input features (e.g., edges), identifying an influential subgraph as the explanation. When evaluating the explanation (i.e., subgraph importance), a standard way is to audit the model prediction based on the subgraph solely. However, we argue that a distribution shift exists between the full graph and the subgraph, causing the out-of-distribution problem. Furthermore, with an in-depth causal analysis, we find the OOD effect acts as the confounder, which brings spurious associations between the subgraph importance and model prediction, making the evaluation less reliable. In this work, we propose Deconfounded Subgraph Evaluation (DSE) which assesses the causal effect of an explanatory subgraph on the model prediction. While the distribution shift is generally intractable, we employ the front-door adjustment and introduce a surrogate variable of the subgraphs. Specifically, we devise a generative model to generate the plausible surrogates that conform to the data distribution, thus approaching the unbiased estimation of subgraph importance. Empirical results demonstrate the effectiveness of DSE in terms of explanation fidelity. 

Comments: 	18 pages
Subjects: 	Machine Learning (cs.LG)
Cite as: 	arXiv:2201.08802 [cs.LG]
  	(or arXiv:2201.08802v3 [cs.LG] for this version)
  	https://doi.org/10.48550/arXiv.2201.08802
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Ying-Xin Wu [ view email ]
[v1] Fri, 21 Jan 2022 18:05:00 UTC (8,117 KB)
[v2] Sun, 30 Jan 2022 17:06:01 UTC (10,764 KB)
[v3] Tue, 1 Feb 2022 05:41:39 UTC (10,765 KB)
Full-text links:
Download:

    Download a PDF of the paper titled Deconfounding to Explanation Evaluation in Graph Neural Networks, by Ying-Xin Wu and 6 other authors
    PDF
    Other formats 

( license )
Current browse context:
cs.LG
< prev   |   next >
new | recent | 2201
Change to browse by:
cs
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

DBLP - CS Bibliography
listing | bibtex
Xiang Wang
An Zhang
Xia Hu
Fuli Feng
Xiangnan He
â€¦
a export BibTeX citation Loading...
Bookmark
BibSonomy logo Mendeley logo Reddit logo ScienceWISE logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

