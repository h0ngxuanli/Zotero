ProtGNN: Towards Self-Explaining Graph Neural Networks
Zaixi Zhang1, Qi Liu1‚àó, Hao Wang1, Chengqiang Lu1, Cheekong Lee2*
1 Anhui Province Key Lab. of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China
2 Tencent America {zaixi, wanghao3, lunar}@mail.ustc.edu.cn, qiliuql@ustc.edu.cn
cheekonglee@tencent.com

arXiv:2112.00911v1 [cs.LG] 2 Dec 2021

Abstract
Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classiÔ¨Åcation. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space. Furthermore, for better interpretability and higher efÔ¨Åciency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the noninterpretable counterparts.
Introduction
Graph Neural Networks (GNNs) have become increasingly popular since many real-world relational data can be represented as graphs, such as social networks (Bian et al. 2020), molecules (Gilmer et al. 2017) and Ô¨Ånancial data (Yang et al. 2020). Following a message passing paradigm to learn node representations, GNNs have achieved state-of-the-art performance in node classiÔ¨Åcation, graph classiÔ¨Åcation, and link prediction (Kipf and Welling 2017; VelicÀákovic¬¥ et al. 2017; Xu et al. 2019). Despite the remarkable effectiveness of GNNs, explaining predictions made by GNNs remains a challenging open problem. Without understanding the rationales behind the predictions, these black-box models cannot be fully trusted and widely applied in critical areas such as medical diagnosis. In addition, model explanations can facilitate model debugging and error analysis. These indicate the necessity of investigating the explainability of GNNs.
*Qi Liu and Chee-Kong Lee are corresponding authors. Copyright ¬© 2022, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.

Recently, extensive efforts have been made to study explanation techniques for GNNs (Yuan et al. 2020b). These methods can explain the predictions of node or graph classiÔ¨Åcations of trained GNNs with different strategies. For example, GNNExplainer (Ying et al. 2019) and PGExplainer (Luo et al. 2020) are proposed to select a compact subgraph structure that maximizes the mutual information with the GNN‚Äôs predictions as the explanation. PGM-Explainer (Vu and Thai 2020) Ô¨Årstly obtains a local dataset by random node feature perturbation. Then it employs an interpretable Bayesian network to Ô¨Åt the local dataset and to explain the predictions of the original GNN model. In addition, XGNN (Yuan et al. 2020a) generates graph patterns to maximize the predicted probability for a certain class and provides model-level explanation. Despite the tremendous developments in the interpretation of GNNs, most existing approaches can be classiÔ¨Åed as post-hoc explanations where another explanatory model is used to provide explanations for a trained GNN. Post-hoc explanations can be inaccurate or incomplete in revealing the actual reasoning process of the original model (Rudin 2018). Therefore, it is more desirable to build models with inherent interpretability where the explanations are generated by the model themselves.
We leverage the concept of prototype learning to construct GNNs with built-in interpretability (i.e. self-explaining GNNs). In contrast to post-hoc explanation methods, the explanations generated by self-explaining GNNs are actually used during classiÔ¨Åcation and are not generated posthoc. Prototype learning is a form of case-based reasoning (Kolodner 1992; Schmidt et al. 2001), which makes the predictions for new instances by comparing them with several learned exemplar cases (i.e. prototypes). It is a natural practice in solving problems with graph-structured data. For example, chemists identify potential drug candidates based on known functional groups (i.e. key subgraphs) in molecular graphs (He et al. 2010; Zhang et al. 2021). Prototype learning provides better interpretability by imitating such a human problem-solving process. Recently the concept of the prototype has been incorporated in convolutional neural networks to build interpretable image classiÔ¨Åers (Chen et al. 2018; Rymarczyk et al. 2021). However, so far prototype learning is not yet explored for explaining GNNs.
Building self-explaining GNNs based on prototype learning brings unique challenges. First, the discreteness of the edges

makes the projection and visualization of the graph prototypes difÔ¨Åcult. Second, the combinatorial nature of graph structure makes it hard to build self-explaining models with both efÔ¨Åciency and high accuracy for graph modeling.
In this paper, we tackle the aforementioned challenges and propose Prototype Graph Neural Network (ProtGNN), which provides a new perspective on the explanations of GNNs. SpeciÔ¨Åcally, various popular GNN architectures can be employed as the graph encoder in ProtGNN. Prediction on a new input graph is performed based on its similarity to the prototypes in the prototype layer. Furthermore, we propose to employ the Monte Carlo tree search algorithm (Silver et al. 2017) to efÔ¨Åciently explore different subgraphs for prototype projection and visualization. In addition, in ProtGNN+, we design a conditional subgraph sampling module to identify which part of the input graph is most similar to each prototype for better interpretability and efÔ¨Åciency. Finally, extensive experiments on several real-world datasets show that ProtGNN/ProtGNN+ provides built-in interpretability while achieving comparable performance with the non-interpretable counterparts.

Related Work
Graph Neural Networks
Graph neural networks have demonstrated their effectiveness on various graph tasks. Let G = (V, E) denotes a graph with node attributes Xv for v ‚àà V and a set of edges E. GNNs leverage the graph connectivity as well as node and edge features to learn a representation vector (i.e., embedding) hv for each node v ‚àà V or a vector hG for the entire graph G. Generally, GNNs follows a message passing paradigm, in which the representation of node v is iteratively updated by aggregating the representations of v‚Äôs neighboring nodes N (v). Here we use Graph Convolutional Network (GCN) (Kipf and Welling 2017) as an example to illustrate such message passing procedures:

hkv+1 = œÉ

W khkuAÀúuv ,

(1)

u‚ààN (v)

where hku is the representation vector of node u at the k-

th

layer

and

AÀú

=

DÀÜ ‚àí

1 2

AÀÜDÀÜ ‚àí

1 2

is

the

normalized

adjacency

matrix. AÀÜ = A + I is the adjacency matrix of the graph G

with self connections added and DÀÜ is a diagonal matrix with

DÀÜii = j AÀÜij. œÉ(¬∑) in Eq. (1) is the ReLU function and W k

is the trainable weight matrix of the k-th layer.

Explainability in Graph Neural Networks
As the application of GNNs grows, understanding why GNNs make such predictions becomes increasingly critical. Recently, the study of the explainability in GNNs is experiencing rapid developments. As Suggested by a recent survey (Yuan et al. 2020b), existing methods for explaining GNNs can be categorized into several classes: gradients/featuresbased methods (Baldassarre and Azizpour 2019; Pope et al. 2019), perturbation-based methods (Ying et al. 2019; Luo et al. 2020; Yuan et al. 2021; Schlichtkrull, De Cao, and

Titov 2020), decomposition methods (Schwarzenberg et al. 2019; Schnake et al. 2020), and surrogate methods (Vu and Thai 2020; Huang et al. 2020).
SpeciÔ¨Åcally, the gradients/features-based methods employ the gradients or the feature values to indicate the importance of different input features. These methods simply adapt existing explanation techniques in the image domain to the graph domain without incorporating the properties of graph data. Perturbation-based methods monitor the changes in the predictions by perturbing different input features and identiÔ¨Åes the most inÔ¨Çuential features. Decomposition methods explain GNNs by decomposing the original model predictions into several terms and associating these terms with graph nodes or edges. Given an input example, surrogate methods Ô¨Årstly sample a dataset from the neighborhood of the given example and then Ô¨Åt a simple and interpretable model, e.g., a decision tree to the sampled dataset. The surrogate models are usually easier to interpret, shedding light into the inner-workings of more complex models.
However, all the above methods are post-hoc explanation methods. Compared with post-hoc explanation methods, built-in interpretability (Chen et al. 2018; Ming et al. 2019) is more desirable since post-hoc explanations usually do not Ô¨Åt the original model precisely (Rudin 2018). Therefore, it is necessary to build models with inherent interpretability and high accuracy.
The Proposed ProtGNN
In this section, We introduce the architecture of ProtGNN/ProtGNN+, formulate the learning objective and describe the training procedures.
ProtGNN Architecture We let {xi, yi}ni=1 be a labeled training dataset, where xi is the input attributed graph and yi ‚àà {1, ..., C} is the label of the graph. We aim to learn representative prototypical graph patterns that can be used for classiÔ¨Åcation references and analogical explanations. For a new input graph, its similarities with each prototype are measured in the latent space. Then, the prediction of the new instance can be derived and explained by its similar prototype graph patterns.
In Figure 1, we show the overview of the architecture of our proposed ProtGNN. The network consists of three key components: a graph encoder f , a prototype layer gP , and a fully connected layer c appended by softmax to output the probabilities in multi-class classiÔ¨Åcation tasks.
For a given input graph xi, the graph encoder f maps the whole graph into a single graph embedding h with a Ô¨Åxed length. The encoder could be any backbone GNN e.g., GCN, GAT or GIN. The graph embedding h could be obtained by taking a sum or max pooling of the last GNN layer.
In the prototype layer, we allocate a pre-determined number of prototypes m for each class. In the Ô¨Ånal trained ProtGNN, each class can be represented by a set of learned prototypes. The prototypes should capture the most relevant graph patterns for identifying graphs of each class. For each input

Figure 1: The architecture of our proposed ProtGNN/ProtGNN+. The model mainly consists of three parts: GNN encoder f , prototype layer gP , and the fully connected layer c appended by softmax to output probabilities in multi-class classiÔ¨Åcation tasks. ProtGNN calculates the similarity score (sim(pk, ¬∑) in the illustration) between the graph embedding and the learned prototypes in the prototype layer. For further interpretability, the conditional subgraph sampling module (in the dashed bounding
box) is incorporated in ProtGNN+ to output subgraphs most similar to each learned prototype.

graph xi and its embedding h, the prototype layer computes the similarity scores:

sim(pk, h) = log(

pk ‚àí h pk ‚àí h

2 2 2 2

+ +

1

)

(2)

where pk is the k-th prototype with the same dimension as the graph embedding h. The similarity function is designed to be monotonically decreasing to pk ‚àí h 2 and always greater than zero. In experiments, is set to a small value e.g., 1e-4. Finally, with the similarity scores, the fully connected layer with softmax computes the output probabilities for each class.

Learning Objective

Our goal is to learn a ProtGNN with both accuracy and in-

herent interpretability. For accuracy, we minimize the cross-

entropy

loss

on

the

training

dataset:

1 n

n i=1

CrsEnt(c

‚ó¶

gp

‚ó¶

f (xi), yi). For better interpretability, we consider several

constraints in constructing prototypes for the explanation.

Firstly, the cluster cost (Clst) encourages that each graph

embedding should at least be close to one prototype of its

own class. Secondly, the separation cost (Sep) encourages

that each graph embedding should stay far away from proto-

types not of its class. Finally, we found in experiments that

some learned prototypes are very close to each other in the

latent space. We encourage the diversity of the learned pro-

totypes by adding the diversity loss (Div) which penalizes

prototypes too close to each other.

To sum up, the objective function we aim to minimize is

1n n CrsEnt(c‚ó¶gp ‚ó¶f (xi), yi)+Œª1Clst+Œª2Sep+Œª3Div,
i=1

(3)

1n

Clst =

min

n i=1 j:pj ‚ààPyi

f (xi) ‚àí pj

2 2

(4)

1n

Sep = ‚àí

min

n i=1 j:pj ‚àà/Pyi

f (xi) ‚àí pj

2 2

(5)

C

Div =

max(0, cos(pi, pj) ‚àí smax) (6)

k=1 i=j
pi,pj ‚ààPk

where Œª1, Œª2, and Œª3 are hyper-parameters controlling the weights of the losses. Pyi is the set of prototypes belonging to class yi. smax is the threshold of the cosine similarity measured by cos(¬∑, ¬∑) in the diversity loss.

Prototype Projection
The learned prototypes are embedding vectors that are not directly interpretable. For better interpretation and visualization, we design a projection procedure performed in the training stage. SpeciÔ¨Åcally, we project each prototype pj (pj ‚àà Pk) onto the nearest latent training subgraph from the same class as that of pj (see Eq. (7)). In this way, we can conceptually equate each prototype with a subgraph, which is more intuitive and human-intelligible. To reduce the computational cost, the projection step is only performed every few training epochs:

pj ‚Üê arg min h ‚àí pj 2,

h‚ààHj

(7)

Hj = {h : f (x), x ‚àà Subgraph(xi) ‚àÄi s.t. yi = k}.

Unlike grid-like data such as images, the combinatorial characteristic of graph makes it unrealistic to Ô¨Ånd the nearest subgraph by enumeration (Chen et al. 2018). In graph prototype projection, we employ the Monte Carlo tree search algorithm (MCTS) (Silver et al. 2017) as the search algorithm to guide our subgraph explorations (see Figure 2). We build a search tree in which the root is associated with the input graph and each of other nodes corresponds to an explored subgraph. Formally, we deÔ¨Åne each node in the search tree

GNN Encoder

Root

Leaf

Update

Update

Update

Figure 2: An illustration of graph prototype projection with Monte Carlo Tree Search. The bottom shows one selected path from the root to leaves in the search tree, which corresponds to one iteration of MCTS. Nodes that are not selected are ignored for simplicity. For each node, its subgraph is evaluated by computing the similarity score via GNN Encoder and the similarity function. In this Ô¨Ågure, we show the computation of similarity score for the node N1 (shown in blue dashed box). In the backward pass, the model updates the statistics of each node.

T as Ni and N0 denotes the root node. The edges in the search tree represent the pruning actions. In the search tree, the graph associated with a child node can be obtained by performing node-pruning from the graph corresponding to its parent node. To limit the search space, we have added two additional constraints: Ni has to be a connected subgraph and the size of the projected subgraph should be small.

During the search process, the MCTS algorithm records the statistics of visiting counts and rewards to guide the exploration and reduce the search space. SpeciÔ¨Åcally, for the node and pruning action pair (Ni, aj), we assume that the subgraph Nj is obtained by action aj from Ni. The MCTS algorithm records four variables for (Ni, aj):
‚Ä¢ C(Ni, aj) denotes the number of counts for selecting action aj for node Ni.
‚Ä¢ W(Ni, aj) is the total reward for all (Ni, aj) visits.
‚Ä¢ Q(Ni, aj) is the averaged reward for multiple visits.
‚Ä¢ R(Ni, aj) is the immediate reward for selecting aj on Ni, which is measured by the similarity between the prototype and the subgraph embedding in this paper. The subgraph embedding is obtained by encoding the subgraph with the GNN encoder f .

Guided by these statistics, MCTS searches for the nearest subgraphs in multiple iterations. Each iteration consists of two phases. In the forward pass, MCTS selects a path starting from the root N0 to a leaf node Nl. To keep subgraphs connected, we select to prune peripheral nodes with minimum degrees. The leaf node can be deÔ¨Åned based on the numbers of nodes in subgraphs such that |Nl| ‚â§ Nmin. The action selection criteria at node Ni is:

a‚àó = argmax Q(Ni, aj) + U (Ni, aj)

(8)

aj

U (Ni, aj) = ŒªR(Ni, aj)

k C(Ni, ak) , 1 + C(Ni, aj)

(9)

where Œª is a hyper-parameter to control the trade-off between exploration and exploitation. The strategy initially prefers to select child nodes with low visit counts to explore different pruning actions, but asympotically prefers actions leading to higher similarity scores.

In the backward pass, the statistics of all node and action pairs selected in this path are updated:

C(Ni, aj) = C(Ni, aj) + 1

(10)

W (Ni, aj) = W (Ni, aj) + sim(pk, hNl ), (11)
where hNl is the embedding of the subgraph associated to the leaf node Nl. In the end, we select the subgraph with the highest similarity score from all the expanded nodes as the new projected prototype.

Conditional Subgraph Sampling module
We further propose ProtGNN+ with a novel conditional subgraph sampling module to provide better interpretation. In ProtGNN+, we not only show the similarity scores to prototypes, but also identify which part of the input graph is most similar to each prototype as part of the reasoning process. In Figure 1, the conditional subgraph sampling module outputs different subgraph embeddings for each prototype. While this task can also be accomplished by MCTS, the exponentially-growing time complexity to the graph size and the difÔ¨Åculty of parallelization and generalization make MCTS algorithm an undesirable choice. Instead, we adopt a parameterized method for efÔ¨Åcient similar subgraph selection conditioned on given prototypes.

Formally, we let eij ‚àà {0, 1} be the binary variable indicating whether the edge between node i and j is selected. The matrix of eij is denoted as E. The optimization objective of the conditional subgraph sampling module is:

max sim(pk, f (Gs)) s.t. |Gs| ‚â§ B,

(12)

E

where Gs is the selected subgraph whose adjacency matrix is E. B is the maximum size of the subgraph.

The combinatorial and discrete nature of graph makes the direct optimization of the above objective function intractable. We Ô¨Årst consider a relaxation by assuming that the explanatory graph is a Gilbert random graph (Gilbert 1959) where the state of each edge is independent to each other. Furthermore, for ease of gradient computation and update, we relax E ‚àà {0, 1}N√óN into convex space E ‚àà [0, 1]N√óN . N is the number of nodes in the input graph. For efÔ¨Åciency and generalizability, we adopt deep neural networks to learn eij:

eij = œÉ(MLPŒ∏([zi; zj; pk])),

(13)

where œÉ(¬∑) here is the Sigmoid function. MLP is a multilayer neural network parameterized with Œ∏ and [¬∑; ¬∑; ¬∑] is the concatenation operation. zi and zj are node embedding obtained from the GNN Encoder, which encodes the feature

and structure information of the nodes‚Äô neighborhood. Then the objective in Eq. (12) becomes

max sim(pk, f (Gs)) ‚àí ŒªbRb
Œ∏

Rb = ReLU( eij ‚àí B),

(14)

eij ‚ààE

where Œªb is the weight for the budget regularization Rb. In our experiments, we adopt stochastic gradient descent to optimize the objective function.

Comparison with MCTS: Our designed conditional subgraph sampling module is much more efÔ¨Åcient than MCTS and easier for parallel computation. The parameters of our conditional subgraph sampling module are Ô¨Åxed and independent of the graph size. To sample from a graph with |E| edges, the time complexity of our method is O(|E|). One limitation of the conditional subgraph sampling module is that it requires additional training. Therefore, MCTS is still used in the prototype projection step of ProtGNN+ for the stability of optimization.

Theorem on Subgraph Sampling

To provide more understandable visualization, ProtGNN+

prunes the input graph to Ô¨Ånd the subgraphs most simi-

lar to prototypes and then calculates the similarity scores.

Compared with ProtGNN, the subgraph sampling procedure

may affect the classiÔ¨Åcation accuracy. The following theo-

rem provides some theoretical understanding of how input

graph sampling affects classiÔ¨Åcation accuracy.

Theorem 1: Let c ‚ó¶ gp ‚ó¶ f be a ProtoGNN. The embedding of the input graph is h. We assume that the number of pro-

totypes is the same for each class, and is denoted as m. For

each class k, the weight connection in the last layer c be-

tween a class k prototype and the class k logit is 1, and that

between a non-class k prototype and the class k logit is 0. We denote pkl as the l-th prototype for class k and hkl the embedding of the pruned subgraph. ProtGNN and ProtGNN+ has

the same graph encoder f . We make the following assump-

tions: there exists some Œ¥ with 0 < Œ¥ < 1,

‚àö ‚Ä¢ for the correct class, we have ‚àöh ‚àí hkl 2 ‚â§ ( 1 + Œ¥ ‚àí
1) h ‚àí pkl 2 and h ‚àí pkl 2 ‚â§ 1 ‚àí Œ¥;

‚Ä¢ for the inc‚àöorrect classes,

h ‚àí hkl

2 ‚â§ Œ∏ h ‚àí pkl

‚àö 2‚àí ,

Œ∏ = min(

1

+

Œ¥

‚àí

1,

1

‚àí

‚àö1 2‚àíŒ¥

).

For one correctly classiÔ¨Åed input graph in ProtGNN, if the output logits between the top-2 classes are at least 2mlog((1+Œ¥)(2‚àíŒ¥)), then ProtGNN+ can classify the input graph correctly as well.

The intuition behind Theorem 1 is that if the subgraph sampling does not change the graph embedding too much, ProtGNN+ will have the same correct predictions as ProtGNN. The proof is included in the appendix.

Training Procedures
In Algorithm 1, we show the training procedure of ProtGNN/ProtGNN+. Before training starts, we randomly initialize

Algorithm 1: Overview of ProtGNN/ProtGNN+ Training Input: Training dataset {xi, yi}ni=1 Parameter: Training epochs T , Warm-up epoch Tw, Projection epoch Tp, Prototype projection period œÑ , ProtGNN+ 1: Initialize model parameters. 2: for training epochs t = 1, 2, ¬∑ ¬∑ ¬∑ , T do 3: Optimizing objective function in Eq. (3) 4: if t>Tp and t%œÑ = 0 then 5: Performing prototype projection with MCTS 6: end if 7: if ProtGNN+ enabled and t>Tw then 8: Optimizing the objective function in Eq. (14). 9: end if 10: end for
Output: Trained model, prototype visualization
the model parameters. We let wc be the weight matrix of the fully connected layer c and wc(k,j) be the weight connection between the output of the j-th prototype and the logit of class k. In particular, for a class k, we set wc(k,j) = 1 for all j with pj ‚àà Pk and wc(k,j) = 0 for all j with pj ‚àà/ Pk. Intuitively, such initialization of wh encourages prototypes belonging to class k to learn semantic concepts that are characteristic to class k. After training begins, we employ gradient descents to optimize the objective function in Eq. (3). If the training epoch is larger than the projection epoch Tp, we perform the prototype projection step every few training epochs. Furthermore, if we train ProtGNN+, the conditional subgraph sampling module and ProtGNN are iteratively optimized after the warm-up epoch Tw when the optimization of GNN encoder and prototypes are stabilized.
ProtGNN for Generic Graph Tasks
In the above sections and illustrations, we have described ProtGNN/ProtGNN+ using graph classiÔ¨Åcation as an example. It is worth noting that ProtGNN/ProtGNN+ can be easily generalized to other graph tasks, such as node classiÔ¨Åcation and link prediction. For example, in the node classiÔ¨Åcation task, the explanation target is to provide the reasoning process behind the prediction of node vi. Assuming the GNN encoder has L layers, the prediction of node vi only relies on its L-hop computation graph. Therefore, prototype projection and conditional subgraph sampling are all performed in the L-hop computation graph.
Experimental Evaluation
Datasets and Experimental Settings
Datasets: We conduct extensive experiments on different datasets and GNN models to demonstrate the effectiveness of our proposed model. These datasets are listed as below:
‚Ä¢ MUTAG (Debnath et al. 1991) and BBBP (Wu et al. 2018) are molecule datasets for graph classiÔ¨Åcation. In these datasets, nodes represent atoms and edges denote

(a)

(b)
Figure 3: The reasoning process of ProtGNN+ in deciding whether the molecular graph is mutagenic (a) or the sentiment of input text graph is positive (b). The predictions are based on the similarity between the latent input representations against the prototypes. The network tries to Ô¨Ånd evidence by looking at which subgraph was mostly similar to the prototypes. The selected subgraphs are highlighted. Due to space constraint, we only show several prototypes with the largest weights for each class.

chemical bonds. The labels of molecular graphs are determined by the molecular compositions.
‚Ä¢ Graph-SST2 (Socher et al. 2013) and Graph-Twitter (Dong et al. 2014) are sentiment graph datasets for graph classiÔ¨Åcation. They convert sentences to graphs with BiafÔ¨Åne parser (Gardner et al. 2018) that nodes denote words and edges represent the relationships between words. The node embeddings are initialized with Bert word embeddings (Devlin et al. 2018). The labels are determined by the sentiment of text sentences.
‚Ä¢ BA-Shape is a synthetic node classiÔ¨Åcation dataset. Each graph contains a base graph obtained from the Baraba¬¥siAlbert (BA) mode (Albert and Baraba¬¥si 2002) and a house-like Ô¨Åve-node motif attached to the base graph. Each node is labeled based on whether it belongs to the base graph or the different spatial locations of the motif.
Experimental Settings: In our evaluation, we use three variants of GNNs, i.e. GCN, GAT, and GIN. The split for

train/validation/test sets is 80% : 10% : 10%. All models are trained for 500 epochs with an early stopping strategy based on accuracy on the validation set. We adopt the ADAM optimizer with a learning rate of 0.005. In Eq.(3), the hyperparameters Œª1, Œª2, and Œª3 are set to 0.10, 0.05, and 0.01 respectively. smax is set to 0.3 in Eq. (6). The number of prototypes per class m is set to 5. In MCTS for prototype projection, we set Œª in Eq. (9) to 5 and the number of iterations to 20. Each node in the Monte Carlo Tree can expand up to 10 child nodes and Nmin is set to 5. The prototype projection period œÑ is set to 50 and the projection epoch Tp is set to 100. In the training of ProtGNN+, the warm-up epoch Tw is set to 200. We employ a three-layer neural network to learn edge weights. In Eq. (14), Œªb is set to 0.01 and B is set to 10. We select hyper-parameters based on related works or grid search, an analysis on hyper-parameters is included in the appendix. All our experiments are conducted with one Tesla V100 GPU.

Table 1: The classiÔ¨Åcation accuracies and standard deviations (%) of ProtGNN, ProtGNN+, and the original GNNs.

Datasets
MUTAG BBBP Graph-SST2 Graph-Twitter BA-Shape

Original
73.3¬±5.8 84.6¬±3.4 89.7¬±0.5 67.5¬±1.9 91.9¬±1.7

GCN
ProtGNN
76.7¬±6.4 89.4¬±4.1 89.9¬±2.4 68.9¬±5.9 95.7¬±1.4

ProtGNN+
73.3¬±2.9 88.0¬±4.6 89.0¬±3.0 66.1¬±6.5 94.3¬±3.7

Original
93.3¬±2.9 86.2¬±1.1 92.2¬±0.3 66.2¬±1.3 92.9¬±0.5

GIN
ProtGNN
90.7¬±3.2 86.5¬±1.6 92.0¬±0.2 75.2¬±2.8 95.2¬±1.3

ProtGNN+
91.7¬±2.9 85.9¬±4.0 92.3¬±0.4 76.5 ¬±3.4 95.5 ¬±2.4

Original
75.0¬±5.0 83.0¬±2.6 88.1¬±0.8 69.6¬±6.5 92.9¬±1.2

GAT
ProtGNN
78.3¬±4.2 85.9¬±2.5 89.1¬±1.2 64.8¬±4.0 93.4¬±3.4

ProtGNN+
81.7¬±2.9 85.5¬±0.8 88.7¬±0.9 66.4¬±3.3 93.2¬±2.0

1.0
0.8
0.6
0.4
0.2
0.0 0.0 0.2 0.4 0.6 0.8 1.0
Figure 4: Visualization on BBBP dataset of the graph (dots) and prototype (stars) embeddings using the t-SNE method. Different colors indicate different classes.
Evaluations on ProtGNN/ProtGNN+
Comparison with Baselines In Table 1, we compare the classiÔ¨Åcation accuracy of ProtGNN/ProtGNN+ with the original GNNs. We apply 3 independent runs on random data splitting and report the means and standard deviations. In the following sections, we use GCN as the default backbone model. As we can see, ProtGNN and ProtGNN+ achieve comparable classiÔ¨Åcation performance with the corresponding original GNN models, which also empirically veriÔ¨Åes Theorem 1.
Reasoning Process of Our Network In Figure 3, we perform case studies on MUTAG and Graph-SST2 to qualitatively evaluate the performance of our proposed method. We visualize the prototypes and show the reasoning process of our ProtGNN+ in reaching a classiÔ¨Åcation decision on input graphs. In particular, given an input graph x, the network Ô¨Ånds the likelihood to be in each class by comparing it with prototypes from each class. The conditional subgraph sampling module Ô¨Ånds the most similar subgraphs in x. These similarity scores are calculated, weighted, and summed together to give a Ô¨Ånal score for x belonging to each class. For example, Figure 3(a) shows the reasoning process of ProtGNN+ in deciding whether the input molecular graph is mutagenic. Based on chemical domain knowledge (Debnath et al. 1991), carbon rings and N O2 groups tend to be mutagenic. In the Prototype column of the mutagenic class, we can observe that the prototypes can capture the structures of N O2 and carbon rings well. Moreover, in the column of Similar Subgraphs, the conditional subgraph sampling mod-

Table 2: EfÔ¨Åciency studies of different methods on BBBP

Methods GCN ProtGNN ProtGNN+ ProtGNN+*

Time 177.9 s 506.3 s 632.7 s

> 2 hrs

ule can effectively identify the most similar subgraphs. For instance, in the Ô¨Årst row of the mutagenic class, the N O2 group and part of the carbon ring can be identiÔ¨Åed, which is quite similar to the prototype on the right.
Compared with biochemistry datasets, examples on text data could be more understandable since no special domain knowledge is required. In Figure 3(b), the input graph ‚Äúcan take the grandkids or the grandparents and never worry about anyone being bored‚Äù is positive. Our method can effectively capture the key phrase/subgraph leading to positiveness, ‚Äúnever worry about bored‚Äù. Furthermore, we can observe that the similarity score between the input graph with the positive prototypes e.g., ‚Äúkind of entertainment love to have‚Äù is much larger than negative prototypes e.g., ‚Äúembarrassed by invention‚Äù.
Overall, our method provides interpretable evidence to support classiÔ¨Åcations. Such explanations participate in the actual model computation and is always faithful to the classiÔ¨Åcation decisions. More examples and case studies are reported in appendix.
t-SNE Visualization of Prototypes In Figure 4 we show the visualization on BBBP dataset of the graph and prototype embeddings using t-SNE method. We can observe that the prototypes can occupy the centers of graph embeddings, which veriÔ¨Åes the effectiveness of prototype learning.
EfÔ¨Åciency Studies Finally, we study the efÔ¨Åciency of our proposed methods. In Table 2, we show the time required to Ô¨Ånish training for each model. Here ProtGNN+* denotes using MCTS for subgraph sampling in the training of ProtGNN+. The time complexity of ProtGNN+* is extremely high due to the complexity of MCTS. The proposed conditional subgraph sampling module can effectively reduce the time cost of ProtGNN+. Although ProtGNN and ProtGNN+ have a larger time cost compared to GCN (largely due to prototype projection with MCTS), the time cost is still acceptable considering the provided built-in interpretability.

Conclusion
While extensive efforts have been made to explain GNNs from different angles, none of existing methods can provide built-in explanations for GNNs. In this paper, we propose ProtGNN/ProtGNN+ which provides a new perspective on the explanations of GNNs. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the prototype layer. For better interpretability and higher efÔ¨Åciency, a novel conditional subgraph sampling module is proposed to indicate the subgraphs most similar to prototypes. Extensive experimental results show that our method can provide a human-intelligible reasoning process with acceptable classiÔ¨Åcation accuracy and time-complexity.
References
Albert, R.; and Baraba¬¥si, A.-L. 2002. Statistical mechanics of complex networks. Reviews of modern physics, 74(1): 47.
Baldassarre, F.; and Azizpour, H. 2019. Explainability techniques for graph convolutional networks. ICML workshop.
Bian, T.; Xiao, X.; Xu, T.; Zhao, P.; Huang, W.; Rong, Y.; and Huang, J. 2020. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI, volume 34, 549‚Äì556.
Chen, C.; Li, O.; Tao, C.; Barnett, A. J.; Su, J.; and Rudin, C. 2018. This looks like that: deep learning for interpretable image recognition. NeurIPS.
Debnath, A. K.; Lopez de Compadre, R. L.; Debnath, G.; Shusterman, A. J.; and Hansch, C. 1991. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2): 786‚Äì 797.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Dong, L.; Wei, F.; Tan, C.; Tang, D.; Zhou, M.; and Xu, K. 2014. Adaptive recursive neural network for targetdependent twitter sentiment classiÔ¨Åcation. In ACL, 49‚Äì54.
Gardner, M.; Grus, J.; Neumann, M.; Tafjord, O.; Dasigi, P.; Liu, N.; Peters, M.; Schmitz, M.; and Zettlemoyer, L. 2018. Allennlp: A deep semantic natural language processing platform. arXiv preprint arXiv:1803.07640.
Gilbert, E. N. 1959. Random graphs. The Annals of Mathematical Statistics, 30(4): 1141‚Äì1144.
Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and Dahl, G. E. 2017. Neural message passing for quantum chemistry. In ICML, 1263‚Äì1272. PMLR.
He, Z.; Zhang, J.; Shi, X.-H.; Hu, L.-L.; Kong, X.; Cai, Y.D.; and Chou, K.-C. 2010. Predicting drug-target interaction networks based on functional groups and biological features. PloS one, 5(3): e9603.
Huang, Q.; Yamada, M.; Tian, Y.; Singh, D.; Yin, D.; and Chang, Y. 2020. Graphlime: Local interpretable model explanations for graph neural networks. arXiv preprint arXiv:2001.06216.

Kipf, T. N.; and Welling, M. 2017. Semi-supervised classiÔ¨Åcation with graph convolutional networks. ICLR.
Kolodner, J. L. 1992. An introduction to case-based reasoning. ArtiÔ¨Åcial intelligence review, 6(1): 3‚Äì34.
Luo, D.; Cheng, W.; Xu, D.; Yu, W.; Zong, B.; Chen, H.; and Zhang, X. 2020. Parameterized explainer for graph neural network. NeurIPS.
Ming, Y.; Xu, P.; Qu, H.; and Ren, L. 2019. Interpretable and steerable sequence learning via prototypes. In SIGKDD.
Pope, P. E.; Kolouri, S.; Rostami, M.; Martin, C. E.; and Hoffmann, H. 2019. Explainability methods for graph convolutional neural networks. In CVPR, 10772‚Äì10781.
Rudin, C. 2018. Please stop explaining black box models for high stakes decisions. stat, 1050: 26.
Rymarczyk, D.; Struski, ≈Å.; Tabor, J.; and Zielin¬¥ski, B. 2021. ProtoPShare: Prototype Sharing for Interpretable Image ClassiÔ¨Åcation and Similarity Discovery. SIGKDD.
Schlichtkrull, M. S.; De Cao, N.; and Titov, I. 2020. Interpreting graph neural networks for nlp with differentiable edge masking. arXiv preprint arXiv:2010.00577.
Schmidt, R.; Montani, S.; Bellazzi, R.; Portinale, L.; and Gierl, L. 2001. Cased-based reasoning for medical knowledge-based systems. International Journal of Medical Informatics, 64(2-3): 355‚Äì367.
Schnake, T.; Eberle, O.; Lederer, J.; Nakajima, S.; Schu¬®tt, K. T.; Mu¬®ller, K.-R.; and Montavon, G. 2020. XAI for graphs: explaining graph neural network predictions by identifying relevant walks. arXiv e-prints, arXiv‚Äì2006.
Schwarzenberg, R.; Hu¬®bner, M.; Harbecke, D.; Alt, C.; and Hennig, L. 2019. Layerwise relevance visualization in convolutional text graph classiÔ¨Åers. arXiv preprint arXiv:1909.10911.
Silver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.; Huang, A.; Guez, A.; Hubert, T.; Baker, L.; Lai, M.; Bolton, A.; et al. 2017. Mastering the game of go without human knowledge. nature, 550(7676): 354‚Äì359.
Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D.; Ng, A. Y.; and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 1631‚Äì1642.
VelicÀákovic¬¥, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio, P.; and Bengio, Y. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903.
Vu, M.; and Thai, M. T. 2020. PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M. F.; and Lin, H., eds., NeurIPS, volume 33.
Wu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Geniesse, C.; Pappu, A. S.; Leswing, K.; and Pande, V. 2018. MoleculeNet: a benchmark for molecular machine learning. Chemical science, 9(2): 513‚Äì530.
Xu, K.; Hu, W.; Leskovec, J.; and Jegelka, S. 2019. How Powerful are Graph Neural Networks? In ICLR.

Yang, S.; Zhang, Z.; Zhou, J.; Wang, Y.; Sun, W.; Zhong, X.; Fang, Y.; Yu, Q.; and Qi, Y. 2020. Financial Risk Analysis for SMEs with Graph-based Supply Chain Mining. In IJCAI, 4661‚Äì4667.
Ying, R.; Bourgeois, D.; You, J.; Zitnik, M.; and Leskovec, J. 2019. Gnnexplainer: Generating explanations for graph neural networks. NeurIPS, 32: 9240.
Yuan, H.; Tang, J.; Hu, X.; and Ji, S. 2020a. Xgnn: Towards model-level explanations of graph neural networks. In SIGKDD, 430‚Äì438.
Yuan, H.; Yu, H.; Gui, S.; and Ji, S. 2020b. Explainability in graph neural networks: A taxonomic survey. arXiv preprint arXiv:2012.15445.
Yuan, H.; Yu, H.; Wang, J.; Li, K.; and Ji, S. 2021. On explainability of graph neural networks via subgraph explorations. ICML.
Zhang, Z.; Liu, Q.; Wang, H.; Lu, C.; and Lee, C.-K. 2021. Motif-based Graph Self-Supervised Learning for Molecular Property Prediction. NeurIPS, 34.

Dataset Statistics
In Table 3, we show the detailed statistics of Ô¨Åve datasets. These datasets include biological data, text data, and synthetic data. The Ô¨Årst four datasets are used for graph classiÔ¨Åcation tasks while BA-Shape is used for node classÔ¨Åcation.

Proof of Theorem 1

In this section, we provide a proof for Theorem 1 in the main

paper.

Theorem 1: Let c ‚ó¶ gp ‚ó¶ f be a ProtoGNN. The embedding

of the input graph is h. We assume that the number of proto-

types is the same for each class, which is denoted as m. For

each class k, the weight connection in the last layer c be-

tween a class k prototype and the class k logit is 1, and that

between a non-class k prototype and the class k logit is 0.

Let pkl denote the l-th prototype for class k and hkl the embedding of the pruned subgraph. ProtGNN and ProtGNN+

has the same graph encoder f .

We make the following assumptions: there exists some Œ¥

with 0 < Œ¥ < 1,

‚àö ‚Ä¢ for the correct class, we have ‚àöh ‚àí hkl 2 ‚â§ ( 1 + Œ¥ ‚àí
1) h ‚àí pkl 2 and h ‚àí pkl 2 ‚â§ 1 ‚àí Œ¥;

‚Ä¢

for the inc‚àöorrect classes, Œ∏ = min( 1 + Œ¥ ‚àí 1, 1

h ‚àí hkl 2

‚àí

‚àö1 2‚àíŒ¥

).

‚â§

Œ∏

h ‚àí pkl

‚àö 2‚àí ,

For one correctly classiÔ¨Åed input graph in ProtGNN, if the output logits between the top-2 classes are at least 2mlog((1+Œ¥)(2‚àíŒ¥)), then ProtGNN+ can classify the input graph correctly as well.

Proof: For any class k, let Lk(x, {pkl }m l=1) denotes the summed contributed scores for graph x belonging to class k in ProtGNN. According to Eq. (2) and the assumption:

m
Lk(x, {pkl }m l=1) = log(
l=1

h ‚àí pkl h ‚àí pkl

2 2 2 2

+ +

1

).

(15)

Let Lk(x, {pkl }m l=1) denotes the summed contributed scores in ProtGNN+:

m
Lk(x, {pkl }m l=1) = log(
l=1

hkl ‚àí pkl hkl ‚àí pkl

2 2 2 2

+ +

1

).

(16)

Then, the gap between the summend contributed scores denoted by ‚àÜk is:

‚àÜk = Lk(x, {pkl }m l=1) ‚àí Lk(x, {pkl }m l=1)

m
= log(
l=1

hkl ‚àí pkl h ‚àí pkl

2 2

+

1

2 2

+

1

¬∑

h ‚àí pkl

2 2

+

hkl ‚àí pkl

2 2

+

).

(17)

Correct class: We Ô¨Årst derive the lower bound of ‚àÜk for the correct class. Firstly, we have

hkl ‚àí pkl h ‚àí pkl

2 2

+

1

2 2

+

1

‚â•

1

1

h ‚àí pkl

2 2

+

1

‚â•

2‚àíŒ¥

(18)

Then, by the triangle inequality , we have hkl ‚àí pkl ‚â§ h ‚àí pkl + h ‚àí hkl . As a result, we have:

h ‚àí pkl

2 2

+

hkl ‚àí pkl

2 2

+

‚â• (

h ‚àí pkl

2 2

+

h ‚àí pkl 2 + h ‚àí hkl

2)2 +

‚â•

(1

h ‚àí pkl + Œ¥) h ‚àí

2 2

+

pkl

2 2

+

(19)

1 ‚â•
1+Œ¥

Combining the above two inequalities, ‚àÜk for the correct

class

is

mlog(

1 (1+Œ¥)(2‚àíŒ¥)

)

=

‚àímlog((1

+

Œ¥)(2

‚àí

Œ¥)).

Wrong class: Now we begin to derive an upper bound of

‚àÜk for incorrect classes. First,

hkl ‚àí pkl

2 2

+

1

h ‚àí pkl

2 2

+

1

‚â§

(

h ‚àí pkl 2 + h ‚àí hkl h ‚àí pkl‚àö22 + 1

)2 + 1

‚â§

(

h ‚àí pkl

2+(

1 + Œ¥ ‚àí 1) h ‚àí pkl

h ‚àí pkl

2 2

+

1

2)2 + 1

‚â§1+Œ¥

(20)

Then,

h ‚àí pkl

2 2

+

hkl ‚àí pkl

2 2

+

‚â§ (
‚â§(

h ‚àí pkl

2 2

+

h

‚àí pkl h‚àí

2‚àí pkl

2

h ‚àí‚àöhkl +

h ‚àí pkl 2 ‚àí h ‚àí hkl

2)2 +
)2
2

(21)

According to the assumption for incorrect classes, we have:

h ‚àí hkl

1

2

‚â§

(1

‚àí

‚àö 2

‚àí

) Œ¥

h ‚àí pkl

‚àö 2‚àí

(22)

1 ‚àö
2‚àíŒ¥

h ‚àí pkl

‚àö 2+

‚â§

h ‚àí pkl 2 ‚àí

h ‚àí hkl

2

(23)

Combining Eq. (21) and Eq. (23), we have:

h ‚àí pkl

2 2

+

hkl ‚àí pkl

2 2

+

‚àö ‚â§ ( 2 ‚àí Œ¥)2 = 2 ‚àí Œ¥

(24)

For incorrect classes, ‚àÜk ‚â• mlog((1 + Œ¥)(2 ‚àí Œ¥)).

Finally, suppose the summed contributed scores of the correct class is at least 2mlog((1 + Œ¥)(2 ‚àí Œ¥)) larger than any other classes in ProtGNN, the input graph will still be correctly classiÔ¨Åed by ProtGNN+.

Architecture of the Conditional Subgraph Sampling Module

In the conditional subgraph sampling module, we adopt deep neural networks to learn eij:

eij = œÉ(MLPŒ∏([zi; zj; pk])).

(25)

In Table 4, we show the details of architecture. In our ex-

periments, the node embedding size and prototype size are

set to 128. To make sure the selected adjacency matrix is

symmetric, we set E

as

E +E T 2

in experiments.

Table 3: Statistics of Ô¨Åve datasets

# of Edges (avg) # of Nodes (avg)
# of Graphs # of Classes

MUTAG
19.79 17.93 188
2

BBBP
25.95 24.06 2039
2

Datasets

Graph-SST2 Graph-Twitter

9.20 10.19 70042
2

20.10 21.10 6940
3

BA-Shape
2055 700
1 4

Figure 5: The reasoning process of ProtGNN+ in deciding whether the input molecular graph is penetrative.

Table 4: Architecture of the Conditional Subgraph Sampling Module

Layer Input Fully Connected + ReLU Fully Connected + ReLU Sigmoid

Size 128 + 128 + 128
64 8 1

More Case Studies
In Figure 5 and Figure 6, we show more case studies on BBBP and Graph-Twitter. Note that Graph-Twitter is a 3class dataset and we show the prototypes for negativeness, neutrality, and positiveness. The input graph in Figure 6 is positive. Our method can effectively capture the key phrase/subgraph leading to positiveness, ‚Äúamazing lady gaga I love‚Äù.
Hyper-parameters Analysis
In this section, we provide some analysis on hyperparameters in ProtGNN/ProtGNN+.
Choosing the Number of Prototypes per Class
We Ô¨Årst investigate how would the number of prototypes per class m inÔ¨Çuence the performance of ProtGNN using BBBP and Graph-Twitter. With the default setting of hyperparameters, we train ProtGNN with varying m. In Figure 7, we observe that the accuracy of ProtGNN Ô¨Årstly increase

dramatically as m increases. Then the increasing slope Ô¨Çattens after m exceeds 5.
Actually, there is one trade-off between accuracy and interpretability when choosing m. The accuracy increases when m increases. However, a large number of prototypes makes the model difÔ¨Åcult to train and comprehend. In experiments, we choose m = 5 since the increasing m only brings marginal improvement to the performance.
InÔ¨Çuence of Diversity Loss
We further show the effectiveness of the diversity loss (Eq. 6). In Figure 8, we plot the cosine similarity matrices of the learned prototypes on BBBP. The Ô¨Årst row are similarity matrices with the diversity loss while the second row without the diversity loss. We can observe that the cosine similarities among prototypes without diversity regularization are much larger than those with diversity loss. In some extreme cases, the similarities are close to 1, which means the learned prototypes are nearly the same. Therefore, the diversity loss can help ProtGNN learn more diverse and evenly distributed prototypes.
InÔ¨Çuence of the Cluster and Separation Loss
Here we want to show the inÔ¨Çuence of Œª1 and Œª2 which controls the weights of the cluster loss and separation loss respectively. In Figure 9, we can observe that the cluster and separation constraints play important roles in ProtGNN. When Œª1 = 0.10 and Œª2 = 0.05, ProtGNN achieves the best performance on BBBP dataset.

(a)
(b) Figure 6: The reasoning process of ProtGNN+ in deciding whether the sentiment of the text graph is positive, neutral, or negative.

Accuracy Accuracy

0.950 0.925 0.900 0.875 0.850 0.825 0.800 0.775 0.750 3

Original GNN ProtGNN

5

10

20

50

Number of prototypes per class

(a) BBBP

0.74 0.72 0.70 0.68 0.66 0.64 0.62 0.60 3

Original GNN ProtGNN

5

10

20

50

Number of prototypes per class

(b) Graph-Twitter

Figure 7: The inÔ¨Çuence of the number of prototypes per class m on performance. The accuracy of ProtGNN Ô¨Årstly increase dramatically as m increases. Then the increasing slope Ô¨Çattens after m exceeds 5.

Prototypes

1.0 5
0.8 4
0.6 3

2

0.4

1

0.2

1 2 Proto3types 4 5

(a) w/ Div Class 0

1.00 5
0.98 4
0.96 3
0.94 2
0.92 1
1 2 Proto3types 4 5
(c) w/o Div Class 0

Prototypes

Prototypes

1.0 5
0.8 4

3

0.6

2

0.4

1

0.2

1 2 Proto3types 4 5

(b) w/ Div Class 1

1.00 5
0.98 4
0.96 3
0.94 2
0.92 1
1 2 Proto3types 4 5
(d) w/o Div Class 1

Prototypes

Figure 8: The inÔ¨Çuence of the diversity regularization in Eq. (6). In the Ô¨Årst row, we show the cosine similarities among prototypes learned with the diversity loss. In the Second row, the diversity loss is removed from the learning objective. The cosine similarities among prototypes without diversity regularization are much larger than those with diversity loss. In some extreme cases, the similarities are close to 1, which means the learned prototypes are nearly the same.

$FFXUDF\









2









 
     1

Figure 9: Effects of the cluster and separation losses on BBBP. When Œª1 = 0.10 and Œª2 = 0.05, ProtGNN achieves the best performance on BBBP dataset.

