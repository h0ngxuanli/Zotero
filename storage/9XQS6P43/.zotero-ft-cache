
Skip to main content
Cornell University
We gratefully acknowledge support from
the Simons Foundation and member institutions.
arxiv logo > cs > arXiv:2009.11848

Help | Advanced Search
Search
Computer Science > Machine Learning
(cs)
[Submitted on 24 Sep 2020 ( v1 ), last revised 2 Mar 2021 (this version, v5)]
Title: How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks
Authors: Keyulu Xu , Mozhi Zhang , Jingling Li , Simon S. Du , Ken-ichi Kawarabayashi , Stefanie Jegelka
Download a PDF of the paper titled How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks, by Keyulu Xu and 5 other authors
Download PDF

    Abstract: We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) -- structured networks with MLP modules -- have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufficiently "diverse". Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings. 

Subjects: 	Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)
Cite as: 	arXiv:2009.11848 [cs.LG]
  	(or arXiv:2009.11848v5 [cs.LG] for this version)
  	https://doi.org/10.48550/arXiv.2009.11848
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Keyulu Xu [ view email ]
[v1] Thu, 24 Sep 2020 17:48:59 UTC (6,897 KB)
[v2] Thu, 1 Oct 2020 03:54:28 UTC (7,888 KB)
[v3] Tue, 24 Nov 2020 23:54:16 UTC (7,780 KB)
[v4] Sun, 21 Feb 2021 19:42:02 UTC (12,658 KB)
[v5] Tue, 2 Mar 2021 23:05:49 UTC (12,657 KB)
Full-text links:
Download:

    Download a PDF of the paper titled How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks, by Keyulu Xu and 5 other authors
    PDF
    Other formats 

( license )
Current browse context:
cs.LG
< prev   |   next >
new | recent | 2009
Change to browse by:
cs
cs.AI
cs.CV
stat
stat.ML
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

DBLP - CS Bibliography
listing | bibtex
Keyulu Xu
Jingling Li
Mozhi Zhang
Simon S. Du
Ken-ichi Kawarabayashi
â€¦
a export BibTeX citation Loading...
Bookmark
BibSonomy logo Mendeley logo Reddit logo ScienceWISE logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

